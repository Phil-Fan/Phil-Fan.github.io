# 02 | Linear Regression

## OLS - ä¼˜åŒ–è§†è§’

è€ƒè™‘ç»å…¸çš„çº¿æ€§å›å½’æ¨¡å‹ï¼š

$$
y = X \beta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I_n)
$$

å…¶ä¸­ï¼š

* $y \in \mathbb{R}^n$ï¼šå“åº”å˜é‡
* $X \in \mathbb{R}^{n \times p}$ï¼šæ»¡ç§©è®¾è®¡çŸ©é˜µï¼ˆåˆ—æ»¡ç§©ï¼‰
* $\beta \in \mathbb{R}^p$ï¼šæœªçŸ¥å›å½’ç³»æ•°
* $\varepsilon$ï¼šç‹¬ç«‹åŒåˆ†å¸ƒå™ªå£°ï¼Œå‡å€¼ 0ï¼Œæ–¹å·® $\sigma^2$


æ®‹å·®å¹³æ–¹å’Œï¼ˆResidual Sum of Squares, RSSï¼‰å®šä¹‰ä¸ºï¼š

$$
RSS = \sum_{i=1}^n \left( y_i - x_{i1}\beta_1 - \cdots - x_{ip}\beta_p \right)^2
$$

ä¹Ÿå¯ä»¥å†™æˆå‘é‡å½¢å¼ï¼š


$$
RSS = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
$$

æœ€å°äºŒä¹˜ä¼°è®¡ï¼ˆOrdinary Least Squares, OLSï¼‰å°±æ˜¯é€‰æ‹©ä½¿ RSS æœ€å°çš„ $\boldsymbol{\beta}$ï¼š

$$
\widehat{\beta} = \underset{\boldsymbol{\beta}}{\operatorname*{arg\,min}} \; (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
$$


- To estimate $\beta$, we set the derivative equal to 0
$$\frac{\partial \text{RSS}}{\partial \beta} = -2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \beta) = 0$$

$$
\widehat{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
$$

- $\mathbf{X}$ full rank $\iff \mathbf{X}^\top \mathbf{X}$ invertible

### æ€§è´¨

$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$




**æ— å**




æˆ‘ä»¬è®¡ç®— $\mathbb{E}[\hat{\beta}]$ï¼š

$$
\begin{aligned}
\mathbb{E}[\hat{\beta}] &= \mathbb{E}[(X^T X)^{-1} X^T y] \\
&= (X^T X)^{-1} X^T \mathbb{E}[y] \\
&= (X^T X)^{-1} X^T (X\beta) \\
&= (X^T X)^{-1} X^T X \beta \\
&= \beta
\end{aligned}
$$

---

**æ–¹å·®**

å°† $\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$ ä»£å…¥ï¼š

$$
\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top (\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}) \\
= \boldsymbol{\beta} + (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\varepsilon}
$$

æœ‰

$$
\begin{aligned}
\operatorname{Var}(\hat{\boldsymbol{\beta}})
&= \operatorname{Var} \left( (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\varepsilon} \right) \\
&= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \operatorname{Var}(\boldsymbol{\varepsilon}) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \\
&= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \\
&=\boxed{ \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} }\\
&= \widehat{\sigma}^2 (\mathbf{X}^\top \mathbf{X})^{-1} \quad \text{ï¼ˆå¯ç”¨æ®‹å·®å¹³æ–¹å’Œä¼°è®¡ï¼‰} \\
&= \frac{RSS}{n - p} (\mathbf{X}^\top \mathbf{X})^{-1} \\
&= \frac{1}{n - p} \sum_{i=1}^n \hat{\varepsilon}_i^2 (\mathbf{X}^\top \mathbf{X})^{-1}
\end{aligned}
$$


---

**UMVUE**

Lehmannâ€“ScheffÃ© å®šç†å‘Šè¯‰æˆ‘ä»¬ï¼š

> è‹¥æŸæ— åä¼°è®¡é‡æ˜¯å……åˆ†ç»Ÿè®¡é‡çš„å‡½æ•°ï¼Œåˆ™å®ƒæ˜¯ UMVUEã€‚

æˆ‘ä»¬æ¥éªŒè¯ï¼š

1ï¸âƒ£ $\hat{\beta}$ æ˜¯ $\beta$ çš„æ— åä¼°è®¡é‡ â†’ âœ…

å·²è¯

2ï¸âƒ£ $X^T y$ æ˜¯å……åˆ†ç»Ÿè®¡é‡ â†’ âœ…

ç”±**å› å­åˆ†è§£å®šç†**ï¼š

* $y \sim \mathcal{N}(X\beta, \sigma^2 I)$
* è”åˆå¯†åº¦å‡½æ•°å¯ä»¥å†™æˆå…³äº $X^T y$ çš„å‡½æ•°å’Œä¸å« $\beta$ çš„å‡½æ•°ä¹‹ç§¯
* æ‰€ä»¥ $X^T y$ æ˜¯ $\beta$ çš„å……åˆ†ç»Ÿè®¡é‡

è€Œ $\hat{\beta}$ æ˜¯ $X^T y$ çš„å‡½æ•° â‡’ å®ƒæ˜¯**å……åˆ†ç»Ÿè®¡é‡çš„å‡½æ•°**

âœ… æ»¡è¶³ Lehmannâ€“ScheffÃ© å®šç†æ¡ä»¶ â‡’ æ˜¯ UMVUEï¼

---

æˆ–è€…ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ Gauss-Markov å®šç†ï¼ˆéæ­£æ€æ¡ä»¶ä¸‹ï¼‰

!!! note "Gauss-Markov å®šç†"

    **åœ¨çº¿æ€§æ¨¡å‹ä¸­ï¼Œåœ¨æ‰€æœ‰çº¿æ€§æ— åä¼°è®¡é‡ä¸­ï¼ŒOLS æ˜¯æ–¹å·®æœ€å°çš„ã€‚**

    ---
    $$
    Ax = b + \epsilon
    $$

    å™ªå£°$\epsilon$æ»¡è¶³

    $$
    \begin{align*}
    \mathbb{E}(\epsilon) &= 0\\
    Cov(\epsilon) &= \mathbb{E}[\epsilon \epsilon^T] = \sigma^2 I
    \end{align*}
    $$

    > å†…å«çš„å‡è®¾ï¼šè¯¯å·®çš„å¹²æ‰°æºæ˜¯ç‹¬ç«‹çš„


    $$
    \hat{x}_{LS} = (A^T A)^{-1} A^T b
    $$

    **OLSæœ€å°äºŒä¹˜ä¼°è®¡æ˜¯$x$çš„æœ€å°æ–¹å·®æ— åä¼°è®¡**

    å³æ»¡è¶³

    $$
    \begin{align*}
    \mathbb{E}[\hat{x}_{LS}] &= \mathbb{E}\left[(A^T A)^{-1} A^T b\right] \\
    &= (A^T A)^{-1} A^T \mathbb{E}(Ax - \epsilon) \\
    &= (A^T A)^{-1} A^T A x \\
    &= x\\
    Var(\hat{x}_{LS}) &\leq Var(\tilde{x})
    \end{align*}
    $$

ä½†è¦æ³¨æ„ï¼š

* **Gauss-Markov å®šç† â†’ æœ€ä¼˜çº¿æ€§æ— åä¼°è®¡ï¼ˆBLUEï¼‰**
* **Lehmannâ€“ScheffÃ© å®šç†ï¼ˆ+ æ­£æ€æ€§ï¼‰â†’ æœ€å°æ–¹å·®æ— åä¼°è®¡ï¼ˆUMVUEï¼‰**



### Training Error & Test Error

$$
\begin{aligned}
\mathbb{E}[\mathrm{TestErr}] &= \mathbb{E}\|\mathbf{y}^*-\mathbf{X}\widehat{\beta}\|^2 \\
&= \mathbb{E}\|(\mathbf{y}^*-\mathbf{X}\beta)+(\mathbf{X}\beta-\mathbf{X}\widehat{\beta})\|^2 \\
&= \mathbb{E}\|\mathbf{y}^*-\mu\|^2 + \mathbb{E}\|\mathbf{X}(\widehat{\beta}-\beta)\|^2 \\
&= \mathbb{E}\|\mathbf{e}^*\|^2 + \mathrm{Trace}(\mathbf{X}^\mathsf{T}\mathbf{X}\,\mathrm{Cov}(\widehat{\beta})) \\
&= n\sigma^2 + p\sigma^2
\end{aligned}
$$

$$
\begin{aligned}
\mathbb{E}[\mathrm{TrainErr}] &= \mathbb{E}\|\mathbf{y}-\mathbf{\widehat{y}}\|^2 = \mathbb{E}\|(\mathbf{I}-\mathbf{H})\mathbf{y}\|^2 \\
&= \mathbb{E}\|(\mathbf{I}-\mathbf{H})\mathbf{e}\|^2 \\
&= \mathrm{Trace}\left((\mathbf{I}-\mathbf{H})^\mathsf{T}(\mathbf{I}-\mathbf{H})\,\mathrm{Cov}(\mathbf{e})\right) \\
&= (n-p)\sigma^2
\end{aligned}
$$

## OLS - ç»Ÿè®¡è§†è§’

!!! note "OLSå’ŒMLEåœ¨é«˜æ–¯å™ªå£°çš„æ¡ä»¶ä¸‹æ˜¯ç­‰ä»·çš„"

!!! note "è§‚æµ‹å‡ºæ¨¡å‹çš„å‡è®¾éå¸¸å…³é”®ï¼Œç»™äººåˆ¤å®šæ¨¡å‹å¥½åçš„ä¸€ä¸ªç›´è§‚çš„æ–¹æ³•"

é¦–å…ˆå®šä¹‰æ‹Ÿåˆè¯¯å·®:

$$
Az = b + e
$$

å…¶ä¸­å‡è®¾å™ªå£°$e$æœä»ç™½å™ªå£°é«˜æ–¯åˆ†å¸ƒ

> ä½¿ç”¨é«˜æ–¯å™ªå£°çš„å»ºæ¨¡å‡è®¾ï¼šæ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›æ˜¯æ¯”è¾ƒå¥½çš„ï¼Œæ²¡æœ‰outlierï¼ˆè¶…å‡º$3\sigma$çš„ç¦»ç¾¤å€¼ï¼‰ï¼Œæ¯”å¦‚ä¸Šè¯¾ä¸€æ¬¡ä¸æ¥ï¼Œä½œä¸šä¸€æ¬¡ä¸äº¤ï¼Œè€ƒè¯•è€ƒ100åˆ†çš„æ ·æœ¬
> 
> åœ¨è¿™ç§æ—¶å€™ä½¿ç”¨é«˜æ–¯å™ªå£°å»ºæ¨¡ï¼Œå¯ä»¥å¾—åˆ°ä¸€ä¸ªæ¯”è¾ƒå¥½çš„ç»“æœ


$$
e \sim N(e|0,\sigma^{2}I) \propto \exp\left[-\frac{1}{\sigma^{2}}\mathrm{e}^{\mathrm{H}}e\right]
$$

å› æ­¤æ¡ä»¶æ¦‚ç‡å¯ä»¥å†™ä½œ:

$$
p(b | Ax) = N(b|Ax,\sigma^{2}I)\\
= \frac{1}{z}\exp\left[-\frac{(b-Ax)^T(b-Ax)}{\sigma^2}\right]
$$

æ ¹æ®æå¤§ä¼¼ç„¶ä¼°è®¡,æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ä¸ª$z$ä½¿å¾—$p(b|Az)$æœ€å¤§:

$$
\begin{aligned}
\max\ \log p(b|Az) &\Leftrightarrow \max\  \log  \frac{1}{z}\exp\left[-\frac{(b-Ax)^T(b-Ax)}{\sigma^2}\right]\\
&= \max\ \log \frac{1}{z} -\frac{(b-Ax)^T(b-Ax)}{\sigma^2} \\
&= \min\ \frac{(b-Ax)^T(b-Ax)}{\sigma^2}\\
&= \min \ (b-Ax)^T(b-Ax)\\
&= \min \ \|Ax-b\|_2^2
\end{aligned}
$$


conditional pdf å¯¹b

likelihood function å¯¹z


## DLS - æœ€å°æ•°æ®äºŒä¹˜



å‡è®¾æ•°æ®çŸ©é˜µ$A$å­˜åœ¨è¯¯å·®ï¼ˆæ¯”å¦‚è®°å½•æ ·æœ¬æ•°æ®çš„æ—¶å€™å†™é”™äº†ï¼‰

$$
A = A_0 + E \\
E_{ij} \stackrel{\text{i.i.d.}}{\sim} N(0, \sigma^2)
$$




ä½¿ç”¨æ ¡æ­£é‡$\Delta A$æ¥è¡¨ç¤ºè¯¯å·®,å³è€ƒå¯Ÿä¸‹é¢çš„çº¦æŸä¼˜åŒ–é—®é¢˜ 

$$
\begin{align*}
\min \quad & ||\Delta A||^2_F\\
s.t. \quad &\left[ A + \Delta A \right] x = b 
\end{align*}
$$  

> underlying idea: æ¯ä¸ªæ•°æ®çš„è¯¯å·®ä¸ä¼šç‰¹åˆ«å¤§

!!! note "Frobenius èŒƒæ•° $(p=2)$æ˜¯çŸ©é˜µå…ƒç´ èŒƒæ•°çš„ä¸€ç§ï¼Œå¹³æ–¹å’Œçš„å¹³æ–¹æ ¹"

    $$
    \|A\|_F \stackrel{\text{def}}{=} \left( \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 \right)^{1/2} = \sqrt{\text{trace}(AA^H)}
    $$

å¯¹äºæœ‰çº¦æŸé—®é¢˜ï¼Œå†™å‡ºæ‹‰æ ¼æœ—æ—¥å‡½æ•°

$$
\begin{align*}
L(A, \lambda) &= \|A\|_F^2 + \lambda^H \left[(A + \Delta A)x - b\right]\\
&= Trace(AA^H) + \lambda^H \left[(A + \Delta A)x - b\right]
\end{align*}
$$

æ±‚å¯¼æ•°å¹¶ä»¤å¯¼æ•°ä¸º0

$$
\begin{align*}
\frac{\partial L(A, \lambda)}{\partial \Delta A} &= \Delta A^H + \lambda x^H = 0\\
\frac{\partial L(A, \lambda)}{\partial \lambda^H} &= (A + \Delta A)x - b = 0
\end{align*}
$$

å¯ä»¥è§£å‡º

$$
\Delta A = - \frac{(Ax-b)x^H}{x^H x}\quad \lambda = \frac{Ax-b}{x^H x}
$$


æŠŠ$\Delta A$å’Œ$\lambda$ä»£å…¥$L(A, \lambda)$ï¼Œå¾—åˆ°

$$
L(\Delta A, \lambda,x) = \frac{(Ax-b)^H (Ax-b)}{x^H x}
$$

å˜æˆäº†ä¸€ä¸ªæ— çº¦æŸçš„ä¼˜åŒ–é—®é¢˜

$$
\min_x J(x) =\frac{(Ax-b)^H (Ax-b)}{x^H x}
$$

- æ–¹æ³•1:ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ±‚è§£$x^{t+1} = x^t - \eta \nabla J(x^t)$
- æ–¹æ³•2:è¿™æ˜¯ä¸€ä¸ªåˆ†å¼ä¼˜åŒ–çš„é—®é¢˜(Fractional Programming)ï¼Œ2018 IEEE TSP




$$
\begin{align*}
\max_{x ,y} & \quad x^H y \\
\mathrm{s.t.} & \quad y = \frac{x}{(Ax-b)^H(Ax-b)}
\end{align*}
$$

$$
\min_{x, y} \|y\|_2^2 x^H A A^H x - 2 \mathrm{Re} \left\{ \|y\|_2^2 b^H A x \right\} + \|y\|_2^2 b^H b - 2 y^H x
$$

- Fix $x$, é‚£ä¹ˆ$y$ æœ‰é—­å¼è§£
- Fix $y$, é‚£ä¹ˆ$x$ æ˜¯å‡¸ä¼˜åŒ–é—®é¢˜




## TLS - æ€»ä½“æœ€å°äºŒä¹˜

**ä¼˜åŒ–é—®é¢˜**ï¼šçº æ­£æœ€å°$\Delta A$å’Œ$\Delta b$ï¼ŒåŒæ—¶å¯ä»¥æ»¡è¶³çº¦æŸ

### æ­¥éª¤

1. input $A$å’Œ$b$
2. å¢å¹¿çŸ©é˜µ $B = \begin{bmatrix} A & b \end{bmatrix}$
3. $B^HB = V \Sigma V^H$
4. æ‰¾$\lambda_{min}$å¯¹åº”çš„ç‰¹å¾å‘é‡$v_{min}$
5. $z^{\star} = v_{min} \times \frac{-1}{v_ {n+1}}$


### é—®é¢˜æ±‚è§£

$$
\begin{align*}
\min_{\Delta A, \Delta b,x} \quad & ||\Delta A||^2_F + ||\Delta b||^2\\
s.t. \quad &\left[ A + \Delta A \right] x = b + \Delta b
\end{align*}
$$

å†™æˆåˆ†å—çŸ©é˜µçš„å½¢å¼

$$
\begin{bmatrix}A & b\end{bmatrix}\begin{bmatrix} x \\ -1 \end{bmatrix} +\begin{bmatrix} \Delta A & \Delta b \end{bmatrix} \begin{bmatrix} x \\ -1 \end{bmatrix} = 0
$$

ä»¤

$$
B = \begin{bmatrix} A & b \end{bmatrix} \quad D = \begin{bmatrix} \Delta A & \Delta b \end{bmatrix} \quad z = \begin{bmatrix} x \\ -1 \end{bmatrix}
$$

æ‰€ä»¥åŸå§‹é—®é¢˜å¯ä»¥å†™æˆ

$$
\begin{align*}
\min_{\Delta A, \Delta b,x} \quad & \|\mathbf{D}\|_F^2 \\
\text{s.t.} \quad   &(\mathbf{B} + \mathbf{D})z = 0
\end{align*}
$$

å¯ä»¥çœ‹å‡ºï¼ŒTLSæ˜¯DLSåœ¨$b = 0$çš„ç‰¹æ®Šæƒ…å†µ

ä½¿ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•

$$
\begin{align*}
    \min_{z} \quad & \frac{(Bz-0)^H (Bz-0)}{z^H z} \\
    =\; & \min_{z} \frac{z^H B^H B z}{z^H z}
\end{align*}
$$

ä¸¤ä¸ªäºŒæ¬¡å‹ç›¸é™¤ï¼šRayleighå•†ï¼Œæœ‰é—­å¼è§£ï¼ˆåœ¨PCAå’ŒTLSä¸­éƒ½æœ‰åº”ç”¨ï¼‰

å¯¹$B^HB = V \Sigma V^H$è¿›è¡Œç‰¹å¾å€¼åˆ†è§£

é‚£ä¹ˆæœ€ä¼˜è§£$z^{\star} = \begin{bmatrix} x^{\star} \\ -1 \end{bmatrix} =  v_{min}$ï¼ˆæœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼‰

ä½†æ˜¯è¿™é‡Œå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼š$v_{min}$çš„æœ€åä¸€è¡Œä¸ä¸€å®šæ˜¯$-1$,æ‰€ä»¥éœ€è¦è¿›è¡Œå½’ä¸€åŒ–ï¼ŒæŠŠæœ€åä¸€è¡Œæ„é€ æˆ$-1$

$$
\frac{-1}{v_{n+1}} V_{min}= \begin{bmatrix} \frac{-v_1}{v_{n+1}} \\ \frac{-v_2}{v_{n+1}} \\ \vdots \\ \frac{-v_n}{v_{n+1}} \\ -1 \end{bmatrix} = \begin{bmatrix} x^{\star} \\ -1 \end{bmatrix}
$$



### å‡ ä½•å«ä¹‰

æ™®é€šLSæ˜¯è®©ç«–ç›´æ–¹å‘çš„è·ç¦»è¯¯å·®æœ€å°

è€ŒTLSæ˜¯è®©å‚ç›´æ–¹å‘ä¸Šçš„è·ç¦»è¯¯å·®æœ€å°;å³æ‰¾åˆ°ä¸€æ¡ç›´çº¿ï¼Œè®©æ‰€æœ‰ç‚¹åˆ°ç›´çº¿çš„è·ç¦»æœ€å°




$$
\begin{align*}
 \min_{z} \frac{z^H B^H B z}{z^H z}
    =\;&\frac{
        \begin{bmatrix}
            x \\ -1
        \end{bmatrix}^H
        \left(
            \begin{bmatrix}
                A & b
            \end{bmatrix}^H
            \begin{bmatrix}
                A & b
            \end{bmatrix}
        \right)
        \begin{bmatrix}
            x \\ -1
        \end{bmatrix}
    }{
        \begin{bmatrix}
            x \\ -1
        \end{bmatrix}^H
        \begin{bmatrix}
            x \\ -1
        \end{bmatrix}
    } \\
    =\; & \frac{ \|A_{\color{red}m\times n}x_{\color{red}n\times 1}-b_{\color{red}m\times 1}\|_2^2 }{ \|x_{\color{red}n\times 1}\|_2^2 + 1 }\\
    =\; &\frac{\sum_{i=1}^{m}(a_i^Tx-b_i)^2}{\|x\|^2+1} \quad \text{çŸ©é˜µçš„è¡Œè§†è§’}
\end{align*}
$$

!!! note "ç‚¹åˆ°ç›´çº¿è·ç¦»å…¬å¼"
    å‡è®¾ç‚¹ $P(x_1, y_1)$ åˆ°ç›´çº¿ $Ax + By + C = 0$ çš„è·ç¦»ä¸º $d$ï¼Œåˆ™è·ç¦»å…¬å¼ä¸ºï¼š

    $$
    d = \frac{|Ax_1 + By_1 + C|}{\sqrt{A^2 + B^2}}
    $$

å¯¹äºç›´çº¿$Ax -b = 0$ï¼Œå¦‚æœæˆ‘ä»¬æŠŠ$A$çœ‹ä½œæ˜¯æ¨ªåæ ‡å˜é‡ï¼Œ$b$çœ‹ä½œæ˜¯çºµåæ ‡å˜é‡ï¼Œé‚£ä¹ˆç‚¹$(a_1,b_1)$åˆ°ç›´çº¿$b = Ax$çš„è·ç¦»å°±æ˜¯

$$
d^2 = \frac{|Ax -b|^2}{x^2 + 1}
$$

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/202506242052169.png)


!!! note "å¼•ç†ï¼š TLSæ‹Ÿåˆç›´çº¿ä¸€å®šè¿‡$(\bar{x}, \bar{y})$"
    $$
    \bar{x} = \frac{\sum_{i=1}^{N} x_i}{n}\qquad
    \bar{y} = \frac{\sum_{i=1}^{N} y_i}{n}
    $$

    è®¾ç›´çº¿æ–¹ç¨‹ä¸º$ax+by+c =0$,å› è¿‡ç‚¹$(\bar{x},\bar{y})$ï¼Œæ‰€ä»¥æœ‰$a\bar{x}+b\bar{y} + c =0 \leftrightarrow c = -a\bar{x} - b \bar{y}$

    å¸¦å…¥åŸæ¥çš„æ–¹ç¨‹å¯å¾— $a(x-\bar{x}) + b (y - \bar{y}) = 0$

    ä¸ºäº†å‡å°‘å‚æ•°é‡ï¼Œä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§ï¼Œä»¤
    
    $$
    k = \frac{-a}{b}
    $$

    å¾—åˆ°
    
    $$
    k(x-\bar{x}) + (y - \bar{y}) = 0
    $$


    å‡è®¾æˆ‘ä»¬æœ‰ç‚¹é›†$(x_i,y_i)$

    é‚£ä¹ˆå³æœ‰

    $$
    \begin{bmatrix}
    x_1 - \bar{x}\\
    x_2 - \bar{x}\\
    \cdots\\
    x_n - \bar{x} 
    \end{bmatrix}
    k = \begin{bmatrix}y_1 - \bar{y}\\y_2 - \bar{y}\\\cdots\\y_n - \bar{y}\end{bmatrix}\\
    Ak = b
    $$


### æ±‚è§£æ¡ˆä¾‹ 

å‡è®¾æœ‰ç‚¹$(2,1),(2,4),(5,1)$

- $\bar{x} = 3,\bar{y} = 2$


$$
\begin{align*}
B &= \begin{bmatrix}2-3 & 1-2\\ 2-3 & 4-2\\ 5-3 & 1-2\end{bmatrix} = \begin{bmatrix}-1 & -1\\ -1 & 2\\ 2 & -1\end{bmatrix}\\
B^{H}B &= \begin{bmatrix}-1 & -1 & 2\\ -1 & 2 & -1\end{bmatrix} \begin{bmatrix}-1 & -1\\ -1 & 2\\ 2 & -1\end{bmatrix}= \begin{bmatrix}6 & -3\\ -3 & 6\end{bmatrix}\\
V_{\min} &= \begin{bmatrix}\frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\end{bmatrix}\\
z &= \begin{bmatrix}-1\\ -1\end{bmatrix}=\begin{bmatrix}k\\-1\end{bmatrix}\quad \text{è¿›è¡Œå½’ä¸€åŒ–}
\end{align*}
$$

$$
\begin{align*}
\therefore k &= -1\\
&-(x-3) = y-2\\
y &= -x +5
\end{align*}
$$









!!! example "Rayleighå•†çš„åº”ç”¨åœºæ™¯ â€”â€” æœ€å¤§ä¿¡å™ªæ¯”çš„æ¥æ”¶æ»¤æ³¢å™¨è®¾è®¡"

    $$
    r(t) = BS(t) +noise(t)
    $$

    > - $r(t)$æ˜¯æ¥æ”¶åˆ°çš„ä¿¡å·
    > - $S(t)$æ˜¯å‘å°„ä¿¡å·
    > - $noise(t)$æ˜¯å™ªå£°

    signal-to-noise ratio

    è®¾è®¡æ»¤æ³¢å™¨ï¼Œä½¿å¾—è¾“å‡ºä¿¡å™ªæ¯”SNRæœ€å¤§

    $$
    \underset{\text{filter output}}{x^H r(t)} = \underset{\text{signal}}{x^H B s(t)} + \underset{\text{noise}}{x^H n(t)}
    $$

    $$
    \mathrm{SNR} = \frac{\mathbb{E}\left[\,|x^H B s(t)|^2\,\right]}{\mathbb{E}\left[\,|x^H n(t)|^2\,\right]} = \frac{x^H B\, \mathbb{E}\left[\underset{å‘å°„ä¿¡å·åæ–¹å·®}{S(t)S^H(t)}\right] B^H x}{x^H\, \mathbb{E}\left[\underset{å™ªå£°åæ–¹å·®}{n(t)n^H(t)}\right] x}
    $$


    å¦‚æœå»ºæ¨¡å™ªå£°æ˜¯ç™½å™ªå£°ï¼Œå½¼æ­¤æ­£äº¤ï¼›ä¸”è®¤ä¸ºä¿¡å·ä¹Ÿæ˜¯å½¼æ­¤æ­£äº¤çš„

    å³

    - $E(s(t)s^H(t)) = \alpha I$
    - $E(n(t)n^H(t)) = \beta I$


    $$
    \mathrm{SNR} = \frac{\alpha x^H B B^H x}{\beta x^H x}
    $$

    > å¾—åˆ°äº†Rayleighå•†çš„è¡¨è¾¾å¼

    å¦‚æœè¦maximize SNRï¼Œé‚£ä¹ˆéœ€è¦ å¯¹$B B^H$è¿›è¡Œç‰¹å¾å€¼åˆ†è§£ï¼Œå–æœ€å¤§çš„ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡







## å¹¿ä¹‰çº¿æ€§å›å½’

### logistic

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240807233452.png)

çº¿æ€§å›å½’æœ‰ä¸€ä¸ªå¾ˆå¼ºçš„å‡è®¾ï¼Œå°±æ˜¯yæ˜¯è¿ç»­çš„ï¼›å¹¶ä¸”æœ‰æ›´åƒé‚»è¿‘æ•°çš„è¶‹åŠ¿(MSE å¯¹äºçº¿æ€§å›å½’ä¸æ˜¯ä¸€ä¸ªå¥½çš„function)

- one vs. Rest

logistic function:

- sigmoid function: $f(x) = \frac{1}{1+e^{-x}}$
CDF(ç´¯ç§¯åˆ†å¸ƒå‡½æ•°)ofthe standard logistic distribution   
ä½¿ç”¨sigmoidå‡½æ•°å°†çº¿æ€§å›å½’çš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡

!!! note "logistic Regreesionæ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡å‹"
    ä¸»è¦è€ƒè™‘çš„æ˜¯decision boundary
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240807234021.png)


ä¸ºä»€ä¹ˆloss functionè¦å–log

- ä¸ºäº†æ–¹ä¾¿æ±‚å¯¼
- å–logä½¿å¾—è¿ä¹˜å˜æˆè¿åŠ ï¼Œä¸ä¼šä¸¢å¤±ä¿¡æ¯

Assumptions behind logistic regression

- $l(a) = -\sum_{i\in I} \log(1+e^{-y_i a^T x_i})$


pros:
- binomial distribution is a  good assumption for classification
- provide a probability
- low computation, easy to optimize
- support online learning:æ¢¯åº¦ä¸‹é™çš„æ¨¡å‹éƒ½æ”¯æŒåœ¨çº¿å­¦ä¹ 

cons:
- too simple:high bias & low variance


å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œåªå…³å¿ƒåˆ†ç±»æ­£ç¡®çš„ç±»çš„å€¼







## Penalty

A unified framework is to minimize the objective function

$$
\arg\min_{\beta} \frac{1}{2n}\|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|^2 + \sum_{j=1}^p P_{\lambda}(\beta_j)
$$

where $P_{\lambda}(\cdot)$ is a penalty function applied on the value of each parameter, and $\lambda$ is a tuning parameter.

- Lasso: $P_{\lambda}(\beta) = \lambda|\beta|$
- Ridge: $P_{\lambda}(\beta) = \lambda\beta^2$
- Best subset: $P_{\lambda}(\beta) = \lambda\mathbf{1}\{\beta \neq 0\}$
- Elastic net: $P_{\lambda}(\beta) = \lambda_1|\beta| + \lambda_2\beta^2$


### Lasso - l1

| æ ¸å¿ƒå†…å®¹            | è§£é‡Š                                          |
| --------------- | ------------------------------------------- |
| Oracle Property | åŒæ—¶å®ç°å˜é‡é€‰æ‹©ä¸€è‡´æ€§ + æœ€ä¼˜ä¼°è®¡ç²¾åº¦                        |
| Lasso çš„é—®é¢˜       | æœ‰åå·®ï¼Œä¸èƒ½åŒæ—¶å®ç°ä¸¤è€…                                |
| ç†è®ºä¸Šæ¡ä»¶           | ä¸ºäº†é€‰å˜é‡ï¼Œ$\lambda$ è¦å¤Ÿå¤§ï¼›ä½†ä¸ºä¼°è®¡ç²¾åº¦ï¼Œ$\lambda$ åˆè¦è¶‹äº 0 |
| è§£å†³æ–¹æ³•            | æ”¹ç”¨æ— åæƒ©ç½šå‡½æ•°ï¼ˆå¦‚ SCADï¼‰ï¼Œæˆ–è€…æ¥å—ä¸€å®šæŠ˜ä¸­                   |


æ±‚è§£ä¸‹é¢çš„ä¼˜åŒ–é—®é¢˜

$$
\begin{aligned}
& \text{minimize } \sum_{i=1}^{n} \left(y_i - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2 \\
& \text{subject to } \sum_{j=1}^{p} |\beta_j| \leq s
\end{aligned}
$$

- Each value of $\lambda$ corresponds to an unique value of $s$.


#### Lasso å›å½’åœ¨æ­£äº¤è®¾è®¡ä¸‹çš„æ¨å¯¼ä¸åŸç†


**å‡è®¾ï¼š**

* è®¾è®¡çŸ©é˜µæ»¡è¶³ $\mathbf{X}^\top \mathbf{X} = \mathbf{I}_p$ï¼ˆå³åˆ—å‘é‡æ­£äº¤ï¼Œå•ä½èŒƒæ•°ï¼‰
* ç›®æ ‡æ˜¯æ±‚è§£ Lasso å›å½’é—®é¢˜ï¼š

$$
\widehat{\boldsymbol{\beta}}^{\text{lasso}} = \arg\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X} \boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|_1
$$

æ­¥éª¤ 1ï¼šæ’å…¥ OLS è§£

å› ä¸º OLS è§£ä¸ºï¼š

$$
\widehat{\boldsymbol{\beta}}^{\text{ols}} = \mathbf{X}^\top \mathbf{y}
$$

æˆ‘ä»¬å°†å…¶æ’å…¥ç›®æ ‡å‡½æ•°ï¼š

$$
\begin{align*}
\|\mathbf{y} - \mathbf{X} \boldsymbol{\beta}\|^2 &= \|\mathbf{y} - \mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}} + \mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}} - \mathbf{X} \boldsymbol{\beta}\|^2\\
&= \|\mathbf{y} - \mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}}\|^2 + \|\mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}} - \mathbf{X} \boldsymbol{\beta}\|^2 + 2 \underbrace{(\mathbf{y} - \mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}})^\top (\mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}} - \mathbf{X} \boldsymbol{\beta})}_{=0}
\end{align*}
$$

å…¶ä¸­æœ€åä¸€é¡¹ä¸º 0 æ˜¯å› ä¸ºï¼š

* æ®‹å·® $\mathbf{r} = \mathbf{y} - \mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}}$ å‚ç›´äº $\operatorname{Col}(\mathbf{X})$
* è€Œ $\mathbf{X}(\widehat{\boldsymbol{\beta}}^{\text{ols}} - \boldsymbol{\beta}) \in \operatorname{Col}(\mathbf{X})$

---

æ­¥éª¤ 2ï¼šç›®æ ‡å‡½æ•°åŒ–ç®€

å› ä¸ºç¬¬ä¸€é¡¹ä¸ $\boldsymbol{\beta}$ æ— å…³ï¼Œæˆ‘ä»¬åªéœ€æœ€å°åŒ–ç¬¬äºŒé¡¹ + æ­£åˆ™é¡¹ï¼š

$$
\min_{\boldsymbol{\beta}} \|\mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}} - \mathbf{X} \boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|_1\\
\leftrightarrow \min_{\boldsymbol{\beta}} \|\widehat{\boldsymbol{\beta}}^{\text{ols}} - \boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|_1 \quad (\because\mathbf{X}^\top \mathbf{X} = \mathbf{I})
$$

**å˜é‡ç‹¬ç«‹æ±‚è§£**

ç›®æ ‡å‡½æ•°å¯åˆ†è§£ä¸ºæ¯ä¸ªå‚æ•°çš„ç‹¬ç«‹ä¼˜åŒ–ï¼š

$$
\widehat{\beta}_j^{\text{lasso}} = \arg\min_{x} (x - a)^2 + \lambda |x|, \quad a = \widehat{\beta}_j^{\text{ols}}
$$

è¿™å°±æ˜¯ç»å…¸çš„ **Soft Thresholding é—®é¢˜**ï¼Œè§£ä¸ºï¼š

$$
\boxed{
\widehat{\beta}_j^{\text{lasso}} = \operatorname{sign}(a) \cdot \max(|a| - \lambda/2, 0)
}
$$

å³ï¼š

* å¦‚æœ $|a| \leq \lambda/2$ï¼Œè§£ä¸º 0
* å¦åˆ™ï¼Œåœ¨æ–¹å‘ä¸Šç¼©å‡ $\lambda/2$

Soft Thresholding = å˜é‡é€‰æ‹©æœºåˆ¶

* Ridge å›å½’ä½¿ç”¨ $\ell_2$ æƒ©ç½šï¼šç³»æ•°æ°¸è¿œä¸ä¼šå˜ä¸º 0ï¼Œåªæ˜¯å˜å°
* Lasso ä½¿ç”¨ $\ell_1$ æƒ©ç½šï¼šä¼šç›´æ¥æŠŠå°çš„ç³»æ•°å‹æˆ 0
* æ‰€ä»¥ Lasso èƒ½å®ç° **å˜é‡é€‰æ‹©ï¼ˆsparsityï¼‰**


| é¡¹ç›®             | è§£é‡Š   |
| ---------------- | ------------------------ |
| æ­£äº¤è®¾è®¡         | $\mathbf{X}^\top \mathbf{X} = \mathbf{I}$ ç®€åŒ–é—®é¢˜            |
| æ‹†åˆ†è¯¯å·®é¡¹        | æ®‹å·®é¡¹å‚ç›´äºåˆ—ç©ºé—´ï¼Œäº¤å‰é¡¹ä¸º 0                               |
| å¯åˆ†è§£ç›®æ ‡        | å¯å¯¹æ¯ä¸ª $\beta_j$ ç‹¬ç«‹æ±‚è§£                                  |
| Soft Threshold è§£ | |
| ç¨€ç–æ€§æ¥æº        | ç³»æ•°å¯èƒ½ç›´æ¥ä¸º 0ï¼Œå®ç°é€‰æ‹©                                   |
| $\lambda$ è¶Šå¤§   | è¶Šå¤šçš„å‚æ•°ä¼šè¢«å‹æˆ 0                                         |



### Ridge - l2

| è§†è§’    | è§£é‡Š                                                                                                   |
| ----- | ---------------------------------------------------------------------------------------------------- |
| æœ€ä¼˜åŒ–è§†è§’ | Ridge è§£æ˜¯æœ€å°åŒ– $\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2 + \lambda \|\boldsymbol{\beta}\|^2$ çš„è§£ |
| è´å¶æ–¯è§†è§’ | Ridge è§£æ˜¯ $\boldsymbol{\beta} \sim \mathcal{N}(0, \frac{\sigma^2}{\lambda} \mathbf{I})$ ä¸‹çš„åéªŒå‡å€¼        |
|PCAè§†è§’||

#### ä¼˜åŒ–è§†è§’
æœ€ä¼˜åŒ–è§†è§’ï¼Œå³æ±‚è§£ä¸‹é¢çš„æœ€ä¼˜åŒ–é—®é¢˜

$$
(y - X\beta)^{\top}(y - X\beta) + \lambda\beta^{\top}\beta
$$

Take derivative with respect to $\beta$ and set to zero

$$
\begin{aligned}
\widehat{\beta}^{\mathrm{~ridge}}&= \boxed{(X^{\top}X + \lambda I)^{-1}X^{\top}y}\\&=(\mathbf{X}^\mathsf{T}\mathbf{X}+\lambda\mathbf{I})^{-1}(\mathbf{X}^\mathsf{T}\mathbf{X})(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y}\\&=(\mathbf{X}^\mathsf{T}\mathbf{X}+\lambda\mathbf{I})^{-1}(\mathbf{X}^\mathsf{T}\mathbf{X})\widehat{\boldsymbol{\beta}}^\mathsf{ols}\\&=\mathbf{Z}\widehat{\boldsymbol{\beta}}^{\mathrm{ols}}
\end{aligned}
$$


#### PCAè§†è§’

!!! note "SVD åˆ†è§£"

    $$
    \mathbf{X} = U D V^\top
    $$

    * $U$ï¼šæ­£äº¤åˆ—å‘é‡ï¼Œè¡¨ç¤ºåœ¨æ•°æ®ç©ºé—´ä¸­çš„æ–¹å‘ï¼ˆä¸»æˆåˆ†ï¼‰
    * $D$ï¼šå¥‡å¼‚å€¼ï¼ˆä¸åæ–¹å·®çŸ©é˜µç‰¹å¾å€¼ç›¸å…³ï¼‰
    * $V$ï¼šè¾“å…¥ç©ºé—´çš„æ­£äº¤åŸºï¼ˆå›å½’ç³»æ•°æ–¹å‘ï¼‰


å°†åæ–¹å·®çŸ©é˜µå†™æˆ PCA å½¢å¼ï¼š

$$
\frac{1}{n} \mathbf{X}^\top \mathbf{X} = V D^2 V^\top
$$

* è¯´æ˜åæ–¹å·®çš„ä¸»æ–¹å‘ï¼ˆç‰¹å¾å‘é‡ï¼‰å°±æ˜¯ $V$ï¼Œå¯¹åº”ç‰¹å¾å€¼ $d_j^2$
* ç¬¬ $j$ ä¸ªä¸»æˆåˆ†ä¸º $X v_j = d_j u_j$
* å¤§çš„å¥‡å¼‚å€¼æ–¹å‘ï¼šæ•°æ®æ–¹å·®å¤§ï¼Œä¿ç•™ä¿¡æ¯å¤š
* å°çš„å¥‡å¼‚å€¼æ–¹å‘ï¼šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè¦å¼ºçƒˆæƒ©ç½š

Ridge å›å½’å¯¹å“åº”å˜é‡çš„ä¼°è®¡ï¼š

$$
\mathbf{X} \hat{\boldsymbol{\beta}}^{\text{ridge}} = \sum_{j=1}^p u_j \cdot \frac{d_j^2}{d_j^2 + \lambda} \cdot u_j^\top \mathbf{y}
$$


1. æŠŠ $\mathbf{y}$ æŠ•å½±åˆ°æ¯ä¸ªä¸»æˆåˆ†æ–¹å‘ $u_j$
2. æŠ•å½±ç»“æœ $u_j^\top y$ è¢« **ç¼©å°** äº†ä¸€ä¸ªå› å­ $\frac{d_j^2}{d_j^2 + \lambda}$
3. $d_j^2$ å°çš„æ–¹å‘ï¼ˆä½æ–¹å·®ï¼‰è¢«æƒ©ç½šå¾—æ›´ä¸¥é‡ï¼Œé˜²æ­¢å¯¹å™ªå£°è¿‡æ‹Ÿåˆ


| ä¸»é¢˜     | å†…å®¹                                    |
| ------ | ------------------------------------- |
| æœ‰åæ€§    | Ridge æœ‰åï¼Œä½†å¯æ§åˆ¶åå·®                       |
| æ–¹å·®é™ä½   | Ridge æ˜¾è‘—å‡å°‘ä¼°è®¡æ–¹å·®                        |
| MSE æ›´ä¼˜ | åˆé€‚çš„ $\lambda$ å¯è®© MSE ä¼˜äº OLS           |
| å‡ ä½•ç†è§£   | Ridge åœ¨ PCA ç©ºé—´ä¸­å¯¹ä¸åŒæ–¹å‘æ–½åŠ ä¸åŒå¼ºåº¦çš„ shrinkage |
| å®ç”¨ä»·å€¼   | å°¤å…¶åœ¨é«˜ç»´/å…±çº¿æ€§ä¸¥é‡æ—¶è¡¨ç°æ›´å¥½                      |




#### è´å¶æ–¯è§†è§’

ğŸ“Œ å…ˆéªŒå‡è®¾

æˆ‘ä»¬å°†å›å½’ç³»æ•° $\boldsymbol{\beta}$ è§†ä¸ºä¸€ä¸ªéšæœºå˜é‡ï¼Œèµ‹äºˆå¦‚ä¸‹å…ˆéªŒåˆ†å¸ƒï¼š

$$
\boldsymbol{\beta} \sim \mathcal{N}\left(0, \frac{\sigma^2}{\lambda} \mathbf{I} \right)
$$

è¿™æ˜¯ä¸€ä¸ªé›¶å‡å€¼ã€é«˜æ–¯å…ˆéªŒï¼Œå¯¹æ¯ä¸ªå‚æ•°éƒ½åšäº† $\ell_2$ èŒƒæ•°çš„æƒ©ç½šã€‚

ğŸ¯ ä¼¼ç„¶å‡½æ•°ï¼ˆæ¥è‡ªçº¿æ€§æ¨¡å‹ï¼‰

$$
\mathbf{y} \mid \boldsymbol{\beta} \sim \mathcal{N}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I})
$$

ğŸ§  åéªŒåˆ†å¸ƒ

åˆ©ç”¨è´å¶æ–¯å®šç†ï¼ˆé«˜æ–¯ + é«˜æ–¯ â‡’ é«˜æ–¯ï¼‰ï¼Œå¾—åˆ°åéªŒåˆ†å¸ƒä¸ºï¼š

$$
\boldsymbol{\beta} \mid \mathbf{y} \sim \mathcal{N}\left( \underbrace{(\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \mathbf{y}}_{\text{ridge è§£}}, \; \text{åæ–¹å·®çŸ©é˜µ} \right)
$$

å…¶ä¸­åéªŒ **å‡å€¼** æ­£æ˜¯ Ridge å›å½’çš„è§£æè§£ï¼š

$$
\boxed{
\mathbb{E}[\boldsymbol{\beta} \mid \mathbf{y}] = (\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \mathbf{y}
}
$$


#### Tikhonovæ­£åˆ™åŒ–


å¯¹äºOLSé—®é¢˜ï¼Œæˆ‘ä»¬æ±‚è§£

$$
\min_x \|Ax-b\|_2^2
$$

$$
x_{LS} = (A^T A)^{-1} A^T b
$$

ä½†æ˜¯å¦‚æœ$A$æ˜¯ç—…æ€çš„ï¼Œé‚£ä¹ˆ$(A^T A)^{-1}$ä¼šå¾ˆå¤§ï¼Œå¯¼è‡´$x_{LS}$ä¸ç¨³å®š


å¾ˆç›´è§‚çš„æƒ³æ³•æ˜¯è®©$A^{H}A$å˜å¾—å¥½ä¸€äº›ï¼Œå³


$$
\hat{x} = (A^{H}A + \lambda I)^{-1}A^{H}b
$$


(Bayesian Linear Regression)


Tikhonovè¯æ˜æ±‚ä¸‹é¢çš„ä¼˜åŒ–é—®é¢˜å’Œä¸Šé¢çš„ç­‰ä»·

$$
\min_x J(x) = \|Ax-b\|_2^2 + \lambda \|x\|_2^2, \quad \lambda \geq 0
$$


!!! note "è¯æ˜ä¸€ä¸‹"

    $$
    J(x)=||Ax-b||_{2}^{2}+\lambda||x||_{2}^{2}
    $$

    æ±‚è§£å…±è½­æ¢¯åº¦

    $$
    \frac{\partial J(x)}{\partial x^{*}}=A^{H}Ax-A^{H}b+\lambda x=0\\
    (A^{H}A+\lambda I)x=A^{H}b
    $$


    è§£å¾—

    $$
    \hat{x}_{Tik}=(A^{H}A+\lambda I)^{-1}Ab
    $$




- è§£å†³è¿‡æ‹Ÿåˆ
- è§£å†³ç—…æ€é—®é¢˜ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§


- ä»£ä»·å‡½æ•°å¯¹åº”çš„æ˜¯likelihood
- æ­£åˆ™é¡¹å¯¹åº”çš„æ˜¯prior




#### bias


**Ridge å›å½’æ˜¯æœ‰åä¼°è®¡**

$$
\mathbb{E}[\hat{\boldsymbol{\beta}}^{\text{ridge}}] = Z \boldsymbol{\beta}, \quad Z = (\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \mathbf{X}
$$

* å› ä¸º $Z \neq I$ï¼Œæ‰€ä»¥ ridge ä¼°è®¡æ˜¯ **æœ‰åçš„**
* éšç€æ­£åˆ™åŒ–å‚æ•° $\lambda$ å¢å¤§ï¼Œ**biasÂ² å¢åŠ **
* è¿™æ˜¯åå·®-æ–¹å·®æƒè¡¡çš„ä¸€éƒ¨åˆ†



#### variance

$$
\begin{align*}
\operatorname{Var}\left(\widehat{\boldsymbol{\beta}}^{\text{ ridge}}\right) &= \operatorname{Var}\left(\mathbf{Z}\widehat{\boldsymbol{\beta}}^{\mathrm{ols}}\right) \\
 &= {\color{red}Z}\operatorname{Var}\left(\widehat{\boldsymbol{\beta}}^{\mathrm{ols}}\right) {\color{red}Z^T}\\
&= {\color{red}(\mathbf{X}^\mathsf{T}\mathbf{X}+\lambda\mathbf{I})^{-1}(\mathbf{X}^\mathsf{T}\mathbf{X})}\sigma^2(X^TX)^{-1}{\color{red}(\mathbf{X}^\mathsf{T}\mathbf{X})(\mathbf{X}^\mathsf{T}\mathbf{X}+\lambda\mathbf{I})^{-1}}\\
&=\sigma^{2}\left(\mathbf{X}^{\top} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{\top} \mathbf{X}\left(\mathbf{X}^{\top} \mathbf{X}+\lambda \mathbf{I}\right)^{-1}
\end{align*}
$$

!!! note "æ€»ä½“æ–¹å·®æ˜¯ä¸€ä¸ªå…³äºæ­£åˆ™åŒ–å¼ºåº¦ $\lambda$ çš„**å•è°ƒé€’å‡å‡½æ•°**"

    $$
    \text{Total Variance} = \operatorname{Tr}\left( \operatorname{Var}\left(\hat{\boldsymbol{\beta}}^{\text{ridge}} \right) \right)
    = \sigma^2 \cdot \operatorname{Tr} \left[ \left( X^T X + \lambda I \right)^{-1} X^T X \left( X^T X + \lambda I \right)^{-1} \right]
    $$

    è®° $\mathbf{S} = X^T X$ï¼Œå®ƒæ˜¯å¯¹ç§°æ­£å®šçš„

    æˆ‘ä»¬å¯ä»¥å¯¹å®ƒåš**ç‰¹å¾å€¼åˆ†è§£**ï¼ˆå› ä¸ºå®ƒå¯¹ç§°ï¼‰ï¼š

    $$
    \mathbf{S} = Q \Lambda Q^\top, \quad \text{å…¶ä¸­ } \Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p), \lambda_i > 0
    $$

    äºæ˜¯æ•´ä¸ªæ–¹å·®çŸ©é˜µå¯ä»¥åŒ–ç®€ä¸ºï¼š

    $$
    \operatorname{Var}(\hat{\boldsymbol{\beta}}^{\text{ridge}})
    = \sigma^2 Q \cdot \text{diag} \left( \frac{\lambda_i}{(\lambda_i + \lambda)^2} \right) \cdot Q^\top
    $$

    æ‰€ä»¥å…¶ trace ä¸ºï¼š

    $$
    \text{Total Variance} = \sigma^2 \sum_{i=1}^p \frac{\lambda_i}{(\lambda_i + \lambda)^2}
    $$

    * æ€»ä½“æ–¹å·®æ˜¯ä¸€ä¸ªå…³äºæ­£åˆ™åŒ–å¼ºåº¦ $\lambda$ çš„**å•è°ƒé€’å‡å‡½æ•°**
    * æ¢å¥è¯è¯´ï¼Œ**æ­£åˆ™åŒ–è¶Šå¼º â‡’ ç³»æ•°æ³¢åŠ¨è¶Šå°**


#### è‡ªç”±åº¦


* Ridge å›å½’è™½ç„¶ä¼°è®¡ $\widehat{\boldsymbol{\beta}}^{\text{ridge}} \in \mathbb{R}^p$ï¼Œä½†ç”±äº **Shrinkage**ï¼Œä¸ç­‰ä»·äºä½¿ç”¨æ‰€æœ‰ $p$ ä¸ªå˜é‡çš„å…¨éƒ¨è‡ªç”±åº¦ã€‚
* è‡ªç”±åº¦éšç€ $\lambda$ çš„å˜åŒ–è€Œå˜åŒ–ï¼š

  * $\lambda \to 0$: Ridge é€€åŒ–ä¸º OLSï¼Œ$\text{df} = p$
  * $\lambda \to \infty$: æ‰€æœ‰å‚æ•°è¢«å‹ç¼©åˆ° 0ï¼Œ$\text{df} \to 0$
  * æ‰€ä»¥ï¼š

    $$
    0 \leq \text{df}(\lambda) \leq p
    $$

!!! note "dof"
    $$
    \text{df}(\hat{f}) = \frac{1}{\sigma^2} \sum_{i=1}^n \operatorname{Cov}(\hat{y}_i, y_i) = \frac{1}{\sigma^2} \operatorname{Trace} \left( \operatorname{Cov}(\hat{\mathbf{y}}, \mathbf{y}) \right)
    $$

$$
\widehat{\mathbf{y}} = \mathbf{S} \mathbf{y}, \quad \text{å…¶ä¸­} \quad \mathbf{S} = \mathbf{X}(\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top
$$

$$
\text{df}(\lambda) = \operatorname{Trace}(\mathbf{S}) = \operatorname{Trace} \left( \mathbf{X}(\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \right)
$$

* è‹¥å¯¹ $\mathbf{X}$ åšå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼š

  $$
  \mathbf{X} = UDV^\top, \quad \text{å…¶ä¸­} \ D = \operatorname{diag}(d_1, \dots, d_p)
  $$

* åˆ™è‡ªç”±åº¦å¯å†™ä¸ºï¼š

$$
\boxed{
\text{df}(\lambda) = \sum_{j=1}^{p} \frac{d_j^2}{d_j^2 + \lambda}
}
$$

* æ¯ä¸ªä¸»æˆåˆ†æ–¹å‘ $j$ çš„è‡ªç”±åº¦è´¡çŒ®æ˜¯ä¸€ä¸ª shrinkage å› å­ï¼š

  $$
  \frac{d_j^2}{d_j^2 + \lambda}
  $$
* æ–¹å·®å°çš„æ–¹å‘ï¼ˆ$d_j$ å°ï¼‰ä¼šè¢«ä¸¥é‡ shrinkï¼Œè‡ªç”±åº¦è´¡çŒ®ä¹Ÿå°‘
* è¿™æ˜¯ Ridge æ¯” OLS æ›´ç¨³å¥ä½†æœ‰åçš„åŸå› 





### elastic
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/202506200340914.png)

lassoä¸ridgeå¯¹æ¯”
- Ridge is $\ell_{2}$ penalty
- Lasso is $\ell_{1}$ penalty
- Best subset is $\ell_{0}$ penalty
- Bridge penalty is $\ell_{q}$ normal

$q = 4$  
$q = 2$  
$q = 1$  
$q = 0.5$  
$q = 0.1$

$\sum_{j}|\beta_{j}|^{q}$ for given values of $q$.

Elastic-net is a hybrid of $\ell_{1}$ and $\ell_{2}$:

$\lambda_{1}\|\beta\|_{1} + \lambda_{2}\|\beta\|_{2}^{2}$


## LDA

[ç†è§£ä¸»æˆåˆ†åˆ†æï¼ˆ1ï¼‰â€”â€”æœ€å¤§æ–¹å·®æŠ•å½±ä¸æ•°æ®é‡å»º - Fenrier Lab](https://seanwangjs.github.io/2017/12/21/principal-components-analysis.html)

[ç®€å•ç†è§£çº¿æ€§åˆ¤åˆ«åˆ†æ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/66088884)

[LDAçº¿æ€§åˆ¤åˆ«åˆ†æâ€”â€”æŠ•å½±çš„ç–‘é—®è§£ç­”\_ldaæŠ•å½±-CSDNåšå®¢](https://blog.csdn.net/qq_41398808/article/details/100065314)

æœ€å°åŒ–ç±»å†…æ–¹å·®

$$
\begin{align*} &\quad \min\limits_w \left[\sum\limits_{x\in X_0}(w^Tx-w^T\mu_0)^2+\sum\limits_{x\in X_1}(w^Tx-w^T\mu_1)^2\right]\\ &=\min\limits_w w^T \left[\sum\limits_{x\in X_0}(x-\mu_0)(x-\mu_0)^T+\sum\limits_{x\in X_1}(x-\mu_1)(x-\mu_1)^T\right]w \\ &=\min\limits_w w^TS_ww \\ \end{align*}
$$

æœ€å¤§åŒ–ç±»é—´æ–¹å·®


$$
\begin{align*} &\quad \max\limits_w \left[(w^T\mu_0-\frac{w^T\mu_0+w^T\mu_1}{2})^2+(w^T\mu_1-\frac{w^T\mu_0+w^T\mu_1}{2})^2\right]\\ &=\max\limits_w \frac{1}{2}w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw\\ &=\max\limits_w \frac{1}{2}w^TS_bw \\ \end{align*}
$$

å› ä¸ºè‡ªå˜é‡åªæœ‰$w$ï¼Œä¸ä¸€å®šäºŒè€…éƒ½èƒ½åŒæ—¶è¾¾åˆ°æœ€ä¼˜ï¼Œæ‰€ä»¥æ•´åˆåˆ°ä¸€èµ·å–ä¸‹å¼çš„æœ€å¤§å€¼ï¼š

$$
J = \displaystyle \frac{w^TS_bw}{w^TS_ww}
$$

[LDAâ€”â€”çº¿æ€§åˆ¤åˆ«åˆ†æåŸºæœ¬æ¨å¯¼ä¸å®éªŒ-CSDNåšå®¢](https://blog.csdn.net/qq_37189298/article/details/108656649)

[äºŒåˆ†ç±»çº¿æ€§åˆ¤åˆ«åˆ†æï¼Œçœ‹æ‡‚è¿™ç¯‡å°±å¤Ÿäº† - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/488134514)