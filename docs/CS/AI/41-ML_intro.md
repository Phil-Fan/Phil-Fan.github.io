# æœºå™¨å­¦ä¹ å¯¼è®º

!!! note "è¯¾ç¨‹ä¿¡æ¯"
    === "æœºå™¨å­¦ä¹ "
        - è¯¾ç¨‹æ—¶é—´ï¼š2024å¹´ç§‹å†¬
        - è¯¾ç¨‹æ•™å¸ˆï¼šèµµæ´²
        - è€ƒæ ¸å†…å®¹ï¼š2æ¬¡ä¹¦é¢ä½œä¸š+2æ¬¡ç¼–ç¨‹ä½œä¸š[Kaggle](https://www.kaggle.com/)ï¼ˆ45%ï¼‰+15æ¬¡éšæœºç­¾åˆ°ï¼ˆ15%ï¼‰+1æ¬¡æœŸæœ«æ‘¸åº•è€ƒè¯•+1æ¬¡æœŸæœ«è€ƒè¯•ï¼›ï¼ˆ40%ï¼‰
        - è¯¾æœ¬ï¼šè¥¿ç“œä¹¦
    === "å®ç”¨çš„æœºå™¨å­¦ä¹ "


[äººå·¥æ™ºèƒ½åŸºç¡€ - é¹¤ç¿”ä¸‡é‡Œçš„ç¬”è®°æœ¬ (tonycrane.cc)](https://note.tonycrane.cc/cs/ai/basic/)

[02ï¼šè´å¶æ–¯å®šç† - å°è§’é¾™çš„å­¦ä¹ è®°å½• (zhang-each.github.io)](https://zhang-each.github.io/My-CS-Notebook/ML/ç»Ÿè®¡æœºå™¨å­¦ä¹ 02ï¼šè´å¶æ–¯å®šç†/)

[å‘½é¢˜é€»è¾‘ - Jerry's Blog (wxxcl.tech)](https://blog.wxxcl.tech/course/aid/çŸ¥è¯†è¡¨è¾¾ä¸æ¨ç†/å‘½é¢˜é€»è¾‘/)

[ç¬”è®°](https://github.com/mura1n/Machine-Learning-in-Practice-Crash-Course-Notes)

## å¯¼è®º

!!! note "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
    ä»¥æ•°æ®ä½œä¸ºç»éªŒçš„è½½ä½“ï¼Œåˆ©ç”¨ç»éªŒæ•°æ®ä¸æ–­æé«˜æ€§èƒ½çš„è®¡ç®—æœºç³»ç»Ÿ/ç¨‹åº/ç®—æ³•

    æœ€ç†æƒ³çš„æœºå™¨å­¦ä¹ æŠ€æœ¯æ˜¯å­¦ä¹ åˆ° **æ¦‚å¿µ** ï¼ˆâ¼ˆç±»å­¦ä¹ ï¼Œå¯ç†è§£çš„ï¼‰

- supervised learning ï¼šåˆ†ç±»ä»»åŠ¡ï¼ˆç¦»æ•£ï¼‰ï¼Œå›å½’ä»»åŠ¡ï¼ˆè¿ç»­ï¼‰ï¼›å­¦ä¹ ä¸€ä¸ªæ˜ å°„å‡½æ•°$x\rightarrow \mathbf{y}$
- unsupervised learning ï¼šæ‰¾åˆ°æ ‡ç­¾æˆ–è€…æ¨¡å¼ï¼Œèšç±»ã€é™ç»´
- reinforcement learningï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆç›¸å½“äºæ˜¯ç›‘ç£å­¦ä¹ ï¼‰
  ![image-20240611173113321](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/image-20240611173113321.png)



PAC æ¨¡å‹

$$
P(|f(x)-y \le \epsilon|) \ge 1-\delta
$$

é¢„æµ‹è¯¯å·®å¾ˆå°çš„æ¦‚ç‡å¤§äº1-Î´

iid ä¿è¯äº†ç»Ÿè®¡æ„ä¹‰ä¸Šå¯ä»¥ä½¿ç”¨æœºå™¨å­¦ä¹ 

è€Œ$\epsilon$ è¡¨ç¤ºäº†æ³›åŒ–èƒ½åŠ›

æ¦‚ç‡è¿‘ä¼¼æ­£ç¡®ï¼šä»¥å¾ˆé«˜çš„æ¦‚ç‡å¾—åˆ°ä¸€ä¸ªå¾ˆæ¥è¿‘çœŸå®å€¼çš„ç»“æœ

??? note "Fundamental Concepts in Machine Learning"

    === "**Sample, Instance, Example**"
        - Sample, instance, and example refer to the same concept, which is a single data point used for training or testing a machine learning model.
        - instance don't contain the label
        - example: instace + label

    === "**Feature, Representation, Predictor**"
        - A feature is an attribute or aspect of the data used to describe a data point.
        - Representation refers to the process of converting data into a form that a computer can process, such as a vector or a matrix.
        - A predictor is a model or function used to predict the target variable.
    
    === "**Label, Target, Class, Pattern Class**"
    
        - A label is the true category or value of the data, used in supervised learning.
        - A target is the variable that the model is intended to predict.
        - A pattern class is a category or grouping of data.
        - A class is a group of data points that belong to the same pattern.
    
    === "**Training Data**"
    
        - Training data is the dataset used to train the model.
        - $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$ represent individual data points in the training data, where $x_i$ is the feature and $y_i$ is the label.
    
    === "**Model, Classifier, Regressor**"
    
        - A model is a mathematical structure used to describe data or predict the target.
        - A classifier is a model used for categorizing data, with a discrete output representing the category.
        - A regressor is a model used for regression analysis, with a continuous output representing the numerical value.
    
    === "**Test Data**"
    
        - Test data is the dataset used to evaluate the performance of the model.
        - $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$ represent individual data points in the test data, where $x_i$ is the feature and $y_i$ is the label.
    
    === "éªŒè¯é›†"
        è®­ç»ƒé›†ä¸Šç”¨æ¥è°ƒå‚æ•°çš„é›†åˆ

|æœºå™¨å­¦ä¹ æœ¯è¯­|ç–¾ç—…è¯Šæ–­ä¾‹å­|
|---|---|
|æ•°æ®é›†ï¼Œç‰¹å¾ï¼Œæ ‡è®°|æŸç–¾ç—…æ‚£è€…â¼ˆç¾¤|
|å‡è®¾ç©ºé—´|æ‰€æœ‰å¯èƒ½çš„è¯|
|ç‰ˆæœ¬ç©ºé—´ï¼ˆè·Ÿè®­ç»ƒé›†ä¸€è‡´çš„â€œå‡è®¾é›†åˆâ€ï¼‰|èƒ½æ²»å¥½çš„è¯|
|å½’çº³åæ‰§|åæ‰§ï¼šä¸­è¯è¥¿è¯ï¼Œå‰¯ä½œç”¨å¤§å°ï¼Œè´¹ç”¨é«˜ä½|
|æ²¡æœ‰å…è´¹åˆé¤|æ²¡æœ‰ç‰¹æ•ˆè¯ï¼Œä¸‡èƒ½è¯|



**inductive bias** | å½’çº³åå¥½: æœºå™¨å­¦ä¹ ç®—æ³•å¯¹äºæŸäº›å‡è®¾çš„å€¾å‘æ€§ï¼Œå­˜åœ¨å¤šæ¡æ›²çº¿ç¬¦åˆæ•°æ®æ—¶å€™ï¼Œç®—æ³•çš„å€¾å‘æ€§å«åšinductive bias

**Occam's Razor** | å¥¥å¡å§†å‰ƒåˆ€åŸç†ï¼šåœ¨æ‰€æœ‰å¯èƒ½çš„è§£é‡Šä¸­ï¼Œæœ€ç®€å•çš„è§£é‡Šæœ€æœ‰å¯èƒ½æ˜¯æ­£ç¡®çš„ï¼ˆå¤§é“è‡³ç®€ï¼‰

!!! tip "ç®—æ³•çš„ä¼˜è¶Šæ€§æ¥è‡ªäºç®—æ³•çš„assumptionå’Œæ•°æ®çš„åŒ¹é…ç¨‹åº¦"

**No Free Lunch Theorem**:
ä¸€ä¸ªç®—æ³•Aåœ¨æŸä¸ªé—®é¢˜ä¸Šè¡¨ç°æ¯”Bå¥½ï¼Œæ¯”å­˜åœ¨å¦ä¸€ä¸ªé—®é¢˜ï¼ŒBæ¯”Aå¥½
è„±ç¦»å…·ä½“é—®é¢˜ï¼Œç©ºæ³›è°ˆè®ºâ€œä»€ä¹ˆå­¦ä¹ ç®—æ³•æ›´å¥½â€æ¯«æ— æ„ä¹‰

è„±ç¦»æ•°æ®åˆ†å¸ƒå’Œè¾“å‡ºå»è°ˆå­¦ä¹ ç®—æ³•ï¼Œæ˜¯æ²¡æœ‰æ„ä¹‰çš„

### ä»€ä¹ˆæ—¶å€™ä½¿ç”¨æœºå™¨å­¦ä¹ 

**there should be some patterns in the data**

- we know the patterns,but don't know how to use
- ML can discover the pattern themselves

æœºå™¨å­¦ä¹ æ˜¯å¤§èƒ†å‡è®¾å’Œå°å¿ƒæ±‚è¯çš„æŠ˜è¡·




### pipeline

pipelineï¼Œä¸­æ–‡æ„ä¸ºç®¡çº¿ï¼Œæ„ä¹‰ç­‰åŒäºæµæ°´çº¿ã€‚åœŸå‘³ä¸€ç‚¹ ä½ æŠŠå®ƒ ç¿»è¯‘æˆ **ä¸€æ¡é¾™æœåŠ¡**ï¼›ä¸“ä¸šä¸€ç‚¹ï¼Œå«å®ƒ **ç»¼åˆè§£å†³æ–¹æ¡ˆ**<br>

![image-20240614191015217](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/image-20240614191015217.png)

- **å®šä¹‰é—®é¢˜**:æ˜¯æœ‰ç›‘ç£è¿˜æ˜¯æ— ç›‘ç£ï¼Ÿæ˜¯åˆ†ç±»è¿˜æ˜¯å›å½’ï¼Ÿ
- æ”¶é›†æ•°æ®ï¼š
- æ•°æ®é¢„å¤„ç† transform data & get featuresï¼šæ‰¾åˆ°xå’Œy
- åˆ›å»ºæ¨¡å‹ï¼ˆå…·ä½“åˆ°æ¨¡å‹ä¹Ÿæœ‰ç›¸åº”çš„Pipeline,æ¯”å¦‚æ¨¡å‹çš„å…·ä½“æ„æˆéƒ¨åˆ†ï¼šæ¯”å¦‚GCN+Attention+MLPçš„æ··åˆæ¨¡å‹ï¼‰
- è¯„ä¼°æ¨¡å‹ç»“æœ
- æ¨¡å‹è°ƒå‚

æ˜¯ä¸€ä¸ª**è¿­ä»£**çš„è¿‡ç¨‹

### generalization æ³›åŒ–
æœºå™¨å­¦ä¹ æœ€é‡è¦çš„èƒ½åŠ›å°±æ˜¯generalizationï¼Œæœ€é‡è¦çš„å°±æ˜¯è¦å­¦ä¹ åˆ°ä¸€äº›æ¦‚å¿µ

## æ€§èƒ½
$ç»éªŒæ€§èƒ½E \approx æ³›åŒ–æ€§èƒ½E^*$

iidå‡è®¾ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ `identical and independently distributed`

### å®šä¹‰
"**Training Error and Test Error**"
- Training error is the error alculated on the training data.
- Test error is the error calculated on the test data.

use test error to evaluate the quality of model
<img src="https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/image-20240614191634130.png" alt="image-20240614191634130" style="zoom: 50%;" />

!!! tip "ä¸åŒçš„ç®—æ³•å°±æ˜¯åœ¨ç”¨ä¸åŒçš„æ–¹å¼å»è¾¾åˆ°å¹³è¡¡ç‚¹"


overfitting è¿‡æ‹Ÿåˆ

æ›´å¤æ‚çš„æ¨¡å‹ï¼šæ›´å°çš„training error

- ä¼˜åŒ–ç›®æ ‡ï¼ŒåŠ å…¥æ­£åˆ™åŒ–é¡¹ï¼Œä½¿å¾—æ¨¡å‹æ›´ç®€å•
- early stopping





æ¬ æ‹Ÿåˆï¼šå¯¹è®­ç»ƒæ ·æœ¬çš„ä¸€èˆ¬æ€§è´¨æ²¡æœ‰å­¦å¥½
- å†³ç­–æ ‘ï¼šæ‹“å±•åˆ†æ”¯
- ç¥ç»ç½‘ç»œï¼šå¢åŠ è®­ç»ƒè½®æ•°
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240912105851.png)


### è¯„ä¼°æ–¹æ³•
ç†è§£æ–¹æ³•ï¼šé¢˜åº“å‡ºå°æµ‹é¢˜ç›®


#### **hold-out**ï¼š
- ç›´æ¥å°†æ•°æ®é›†åˆ’åˆ†ä¸ºä¸¤ä¸ªäº’æ–¥é›†åˆâ€”â€”è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚åœ¨åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶ï¼Œè¦å°½å¯èƒ½ä¿æŒæ•°æ®åˆ†å¸ƒçš„ä¸€è‡´æ€§ã€‚
- ä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼ˆstratified samplingï¼‰æ–¹æ³•ï¼Œä»¥ä¿æŒç±»åˆ«æ¯”ä¾‹ä¸€è‡´ã€‚
- ä¸€èˆ¬è¿›è¡Œè‹¥å¹²æ¬¡éšæœºåˆ’åˆ†ï¼Œé‡å¤å®éªŒå¹¶å–å¹³å‡å€¼ã€‚
- è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ ·æœ¬æ¯”ä¾‹é€šå¸¸ä¸º2:1æˆ–4:1ï¼Œæ•ˆæœè¿˜ä¸é”™ã€‚
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240912151812.png)
#### **äº¤å‰éªŒè¯æ³•**

1. åˆ†å±‚é‡‡æ ·åˆ’åˆ†æ•°æ®é›†ï¼šå°†æ•°æ®é›†åˆ†å±‚é‡‡æ ·åˆ’åˆ†ä¸ºKä¸ªå¤§å°ç›¸ä¼¼çš„äº’æ–¥å­é›†ã€‚
2. è®­ç»ƒå’Œæµ‹è¯•ï¼šæ¯æ¬¡ä½¿ç”¨K-1ä¸ªå­é›†çš„å¹¶é›†ä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä¸‹çš„ä¸€ä¸ªå­é›†ä½œä¸ºæµ‹è¯•é›†ã€‚
3. é‡å¤å®éªŒï¼šé‡å¤ä¸Šè¿°è¿‡ç¨‹Kæ¬¡ï¼Œæ¯æ¬¡éƒ½ä½¿ç”¨ä¸åŒçš„å­é›†ä½œä¸ºæµ‹è¯•é›†ã€‚
4. è®¡ç®—å¹³å‡å€¼ï¼šæœ€ç»ˆè¿”å›Kä¸ªæµ‹è¯•ç»“æœçš„å¹³å‡å€¼
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240912152056.png)

LOO leave-one-outï¼šç•™ä¸€æ³•ï¼Œæœ€æ¥è¿‘äºç†æƒ³æƒ…å†µï¼Œå¼€é”€å¤ªå¤§ï¼ŒNFL

!!! tip "çŒœæµ‹è¿›æ•™å®¤æ€§åˆ«é—®é¢˜"
    ç”·ç”Ÿå¤šçŒœç”·ç”Ÿï¼Œå¥³ç”Ÿå¤šçŒœå¥³ç”Ÿ

#### è‡ªåŠ©æ³• bootstrap

åŒ…å¤–ä¼°è®¡ï¼Œæœ‰36.8%ä¸å‡ºç°
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240912152608.png)

- æ”¹å˜äº†æ•°æ®çš„åˆ†å¸ƒ

#### **è°ƒå‚æ•°**

- æ¨¡å‹å‚æ•°ï¼šç®—æ³•è®¡ç®—
- è¶…å‚æ•°ï¼šç”¨æˆ·æä¾›,è½®æ•°ï¼Œ


### æ€§èƒ½åº¦é‡
ä½œä¸ºè®¾è®¡è‡ªå·±åº¦é‡çš„ä¸€ç§å¯å‘

#### å›å½’

- MSE:
  
$$
MSE(f, \boldsymbol{\theta})=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}, \boldsymbol{\theta}\right)\right)^{2}
$$

- MAE:

$$
MAE(f, \boldsymbol{\theta})=\frac{1}{n}\left|y_{i}-f\left(x_{i}, \boldsymbol{\theta}\right)\right|
$$







memory consumption

platform required for running 

CPU vs GPU

server,workstation

#### æ··æ·†çŸ©é˜µ

å…ˆä»‹ç»ä¸€ä¸‹æ··æ·†çŸ©é˜µï¼ˆT/F: é¢„æµ‹æ˜¯å¦æ­£ç¡®ï¼ŒP/Nï¼šé¢„æµ‹æ˜¯æ­£ç±»è¿˜æ˜¯è´Ÿç±»ï¼‰
- TPï¼šé¢„æµ‹ä¸ºæ­£ç±»ï¼Œå®é™…ä¸ºæ­£ç±»ï¼Œé¢„æµ‹æ­£ç¡®ã€‚
- FPï¼šé¢„æµ‹ä¸ºæ­£ç±»ï¼Œå®é™…ä¸ºè´Ÿç±»ï¼Œé¢„æµ‹é”™è¯¯ã€‚
- FNï¼šé¢„æµ‹ä¸ºè´Ÿç±»ï¼Œå®é™…ä¸ºæ­£ç±»ï¼Œé¢„æµ‹é”™è¯¯ã€‚
- TNï¼šé¢„æµ‹ä¸ºè´Ÿç±»ï¼Œå®é™…ä¸ºè´Ÿç±»ï¼Œé¢„æµ‹æ­£ç¡®ã€‚



**å‡†ç¡®ç‡ | Accuracy**

æ­£ç±»å’Œè´Ÿç±»ä¸­é¢„æµ‹æ­£ç¡®çš„æ•°é‡å æ€»æ•°é‡çš„å æ¯”ã€‚

$Accuracy=\frac{T P+T N}{T P+F P+F N+T N}$

=== "å­˜åœ¨é—®é¢˜1"
	å‡†ç¡®ç‡ä¸å¯å¯¼ï¼Œæ— æ³•ä½œä¸ºcost functionå»åšè®­ç»ƒï¼Œåªèƒ½ç”¨ä½œè¯„ä¼°ã€‚
=== "å­˜åœ¨é—®é¢˜2"
	æ­£ç±»å’Œè´Ÿç±»é¢„æµ‹æ­£ç¡®çš„é‡è¦æ€§ä¸ä¸€æ ·ï¼Œæ¯”å¦‚å¯¹äºç™Œç—‡æ£€æµ‹æ¥è¯´ï¼Œå¯èƒ½è´Ÿç±»(æ²¡æœ‰æ‚£ç™Œç—‡) é¢„æµ‹æ­£ç¡®çš„æ•°é‡éå¸¸å¤§ï¼Œå°±å¯¼è‡´Accuracyçš„åˆ†å­éå¸¸å¤§ï¼Œå¾—åˆ°çš„Accuracyå°±éå¸¸å¤§ï¼Œä½†æ˜¯å¯èƒ½æ­£ç±»(æ‚£ç™Œç—‡) é¢„æµ‹æ­£ç¡®çš„æ•°é‡éå¸¸å°ï¼Œå°±å¯¼è‡´è™½ç„¶æ¨¡å‹çš„å‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†æ ¹æœ¬æ£€æµ‹ä¸å‡ºç™Œç—‡ã€‚

è§£å†³é—®é¢˜çš„æ–¹æ¡ˆï¼šé‡‡ç”¨ç²¾ç¡®ç‡æˆ–è€…å¬å›ç‡

!!! tip "æŸ¥å‡†ç‡å’ŒæŸ¥å…¨ç‡"
    - æŸ¥å‡†ç‡ï¼šå³ä½ è®¤ä¸ºæ˜¯Trueçš„æ ·æœ¬ä¸­ï¼Œåˆ°åº•æœ‰å¤šå°‘ä¸ªæ ·æœ¬æ˜¯çœŸä¸ºTrueã€‚
    - æŸ¥å…¨ç‡ï¼šå³åœ¨é¢„æµ‹æ ·æœ¬ä¸­å±äºTrueçš„æ ·æœ¬ï¼Œä½ çœŸçš„åˆ¤æ–­ä¸ºTrueçš„æœ‰å‡ ä¸ªã€‚

- çœŸé˜³æ€§ç‡ï¼ˆTrue Positive Rateï¼ŒTPRï¼‰é€šå¸¸ä¹Ÿè¢«ç§°ä¸ºæ•æ„Ÿæ€§ï¼ˆSensitivityï¼‰æˆ–å¬å›ç‡ï¼ˆRecallï¼‰ã€‚å®ƒæ˜¯æŒ‡åˆ†ç±»å™¨æ­£ç¡®è¯†åˆ«æ­£ä¾‹çš„èƒ½åŠ›ã€‚çœŸé˜³æ€§ç‡å¯ä»¥ç†è§£ä¸ºæ‰€æœ‰é˜³æ€§ç¾¤ä½“ä¸­è¢«æ£€æµ‹å‡ºæ¥çš„æ¯”ç‡(1-æ¼è¯Šç‡)ï¼Œå› æ­¤TPRè¶Šæ¥è¿‘1è¶Šå¥½ã€‚å®ƒçš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š$precision=\frac{T P}{TP+FP}$


- å¬å›ç‡ï¼ˆæŸ¥å…¨ç‡ï¼‰ï¼šå®é™…ä¸ºæ­£ç±»çš„æ ·æœ¬ä¸­ï¼Œè¢«é¢„æµ‹æ­£ç¡®çš„æ­£ç±»çš„æ¯”ä¾‹ã€‚$recall=\frac{T P}{T P+F N}$



- å‡é˜³æ€§ç‡ (False Positive Rate, FPR)
å‡é˜³æ€§ç‡ï¼ˆFalse Positive Rateï¼ŒFPRï¼‰æ˜¯æŒ‡åœ¨æ‰€æœ‰å®é™…ä¸ºè´Ÿä¾‹çš„æ ·æœ¬ä¸­ï¼Œæ¨¡å‹é”™è¯¯åœ°é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬æ¯”ä¾‹ã€‚å‡é˜³æ€§ç‡å¯ä»¥ç†è§£ä¸ºæ‰€æœ‰é˜´æ€§ç¾¤ä½“ä¸­è¢«æ£€æµ‹å‡ºæ¥é˜³æ€§çš„æ¯”ç‡(è¯¯è¯Šç‡)ï¼Œå› æ­¤FPRè¶Šæ¥è¿‘0è¶Šå¥½ã€‚å®ƒçš„è®¡ç®—å…¬å¼å¦‚ä¸‹ FP = \frac{FP}{FP+TN}

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240909154344.png)


F1 åº¦é‡ï¼š

$F 1=\frac{2 \times \text { precision } \times \text { recall }}{\text { precision }+\text { recall }} = ã€\frac{2\times TP}{æ€»æ•° + TP -TN}$

!!! tip "F1æ˜¯På’ŒRçš„è°ƒå’Œå¹³å‡"

å¯¹æŸ¥å‡†ç‡ã€æŸ¥å…¨ç‡æœ‰ä¸åŒçš„åå¥½
$F_\beta = \frac{(1+\beta^2)\times P\times R}{(\beta^2 \times P) + R}$
$\beta = 1$:æ ‡å‡†F1

$\beta > 1$:åé‡æŸ¥å…¨ç‡ï¼ˆé€ƒçŠ¯ä¿¡æ¯æ£€ç´¢ï¼‰

$\beta < 1$:åé‡æŸ¥å‡†ç‡ï¼Œå•†å“æ¨èç³»ç»Ÿ

#### ROC & AUC
ç»˜åˆ¶æ–¹æ³•ï¼šç»™å®š$m^+$ä¸ªæ­£æ ·æœ¬å’Œ$m^-$ä¸ªè´Ÿæ ·æœ¬ï¼Œå¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œè®¡ç®—å…¶é¢„æµ‹æ¦‚ç‡ï¼Œç„¶åæŒ‰ç…§æ¦‚ç‡ä»å¤§åˆ°å°æ’åºï¼Œç„¶åé€ä¸ªæ ·æœ¬è®¡ç®—TP rateå’ŒFP rateï¼Œç„¶åç»˜åˆ¶ROCæ›²çº¿ã€‚

é¢„æµ‹å‡†ç¡®ï¼Œå¢åŠ yå€¼ï¼›é¢„æµ‹é”™è¯¯ï¼Œå¢åŠ xå€¼ï¼›

---

**AUC**

AUCï¼ˆROCæ›²çº¿ä¸‹é¢ç§¯ï¼‰æ˜¯ROCæ›²çº¿ä¸‹çš„é¢ç§¯ï¼Œç”¨äºè¡¡é‡åˆ†ç±»å™¨æ€§èƒ½ã€‚AUCå€¼è¶Šæ¥è¿‘1ï¼Œè¡¨ç¤ºåˆ†ç±»å™¨æ€§èƒ½è¶Šå¥½ï¼›åä¹‹ï¼ŒAUCå€¼è¶Šæ¥è¿‘0ï¼Œè¡¨ç¤ºåˆ†ç±»å™¨æ€§èƒ½è¶Šå·®ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å¸¸å¸¸é€šè¿‡è®¡ç®—AUCå€¼æ¥è¯„ä¼°åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚

ç†è®ºä¸Šï¼Œå®Œç¾çš„åˆ†ç±»å™¨çš„AUCå€¼ä¸º1ï¼Œè€Œéšæœºåˆ†ç±»å™¨çš„AUCå€¼ä¸º0.5ã€‚è¿™æ˜¯å› ä¸ºå®Œç¾çš„åˆ†ç±»å™¨å°†æ‰€æœ‰çš„æ­£ä¾‹å’Œè´Ÿä¾‹å®Œå…¨æ­£ç¡®åœ°åˆ†ç±»ï¼Œè€Œéšæœºåˆ†ç±»å™¨å°†æ­£ä¾‹å’Œè´Ÿä¾‹çš„åˆ†ç±»ç»“æœéšæœºåˆ†å¸ƒåœ¨ROCæ›²çº¿ä¸Šã€‚

ç»¼ä¸Šï¼ŒROCæ›²çº¿å’ŒAUCå€¼æ˜¯ç”¨äºè¯„ä¼°äºŒåˆ†ç±»æ¨¡å‹æ€§èƒ½çš„ä¸¤ä¸ªé‡è¦æŒ‡æ ‡ã€‚é€šè¿‡ROCæ›²çº¿ï¼Œæˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°äº†è§£åˆ†ç±»å™¨åœ¨ä¸åŒé˜ˆå€¼ä¸‹çš„æ€§èƒ½ï¼›è€Œé€šè¿‡AUCå€¼ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹åˆ†ç±»å™¨çš„æ•´ä½“æ€§èƒ½è¿›è¡Œé‡åŒ–è¯„ä¼°ã€‚

### æ€§èƒ½è¯„ä»·

- æµ‹è¯•æ€§èƒ½ä¸ç­‰äºæ³›åŒ–æ€§èƒ½
- æµ‹è¯•æ€§èƒ½éšç€æµ‹è¯•é›†çš„å˜åŒ–è€Œå˜åŒ–

å‡è®¾æ£€éªŒï¼š
æœ‰å¤šå°‘æŠŠæ¡åœ¨ç»Ÿè®¡æ„ä¹‰ä¸Šè¯´è¿™ä¸ªæ¨¡å‹æ˜¯å¥½çš„




### bias and variance decomposition
åå·® & æ–¹å·®

- bias: æœ€å¥½çš„æ¨¡å‹å’Œground truthä¹‹é—´çš„å·®è·;æ¨¡å‹çš„ä¸Šé™; training error
- variance: æœ€ä¼˜çš„æ¨¡å‹å’Œæœ€å·®çš„æ¨¡å‹ä¹‹é—´çš„å·®è·ï¼›æ¨¡å‹çš„ä¸‹é™; the difference between training error and test error

prediction error = bias + variance + noise

- high bias, low variance: underfitting
- low bias, high variance: overfitting
- low bias, low variance: good model


æ”¹è¿›ç­–ç•¥

underfitting:
- add more features
- use more complex model
- descrease regularization

overfitting:
- decrease model complexity
- decrease number of features
- add more regularization
- add more data
!!! note "train val test"
    60% 20% 20%
    - training set: train the model
    - validation set: tune the hyperparameters
    - test set: evaluate the model



## è´å¶æ–¯å†³ç­–

### é—®é¢˜ä¸å®šä¹‰

$x$ sample

$y$ state of the nature

$P(y|x)$ given $x$â€‹â€‹,what is the probability of the state of the nature





æ¡ä»¶æ¦‚ç‡ï¼š
$$
P(A|B) = \frac{P(A,B)}{P(B)}
$$




**å…ˆéªŒæ¦‚ç‡ | `prior`**: $P(A)$â€‹the probability A being True. this is the knowledge

åæ˜ äº†æˆ‘ä»¬å…³æ³¨çš„æ ‡ç­¾åœ¨è‡ªç„¶ç•Œä¸­(æ— äººä¸ºå¹²é¢„çš„æƒ…å†µä¸‹)çš„æ•°é‡åˆ†å¸ƒæƒ…å†µï¼ˆåœ¨æŸä¸ªç‰¹å¾ä¸‹ä¹Ÿå¯ä»¥ï¼‰ã€‚ä¾‹å¦‚ï¼šæˆ‘ä»¬å¯ä»¥çœ‹åœ¨ lightness ç‰¹å¾æ¡ä»¶ä¸‹See basså’Œsalmonè¿™ä¸¤ä¸ªæ ‡ç­¾çš„æ•°é‡æƒ…å†µã€‚æ›´ç®€å•çš„æ¥è®²ï¼Œå…ˆéªŒå°±æ˜¯åœ¨æˆ‘ä»¬ä¸çŸ¥æƒ…çš„æƒ…å†µä¸‹çŒœæµ‹çš„æ ‡ç­¾ç§ç±»ã€‚ï¼ˆæˆ‘ä»¬å€¾å‘äºçŒœæµ‹ç‚¸å¼¹ä¸ä¼šçˆ†ç‚¸ï¼Œå› ä¸ºå…¶å…ˆéªŒæ¦‚ç‡å¾ˆå°ï¼‰

- å¦‚æœæ²¡æœ‰å…ˆéªŒæ¦‚ç‡çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šè®¤ä¸ºsalmonå’Œsea bassçš„æ•æ‰æ¦‚ç‡æ˜¯ç›¸ç­‰çš„ã€‚è¿™ç§å‡è®¾å¹¶ä¸é€‚ç”¨äºä»»ä½•æƒ…å†µã€‚$P(y_{1} )=P(y_{2} )$

- å¦‚æœæ˜¯äºŒåˆ†ç±»é—®é¢˜$P(y_{1} )+P(y_{2} )=1$
- å¦‚æœåªæ ¹æ®å…ˆéªŒä¿¡æ¯è¿›è¡Œå†³ç­–
    - å³å¦‚æœy1çš„æ¦‚ç‡å¤§äºy2åˆ™è®¤ä¸ºæ˜¯y1ï¼Œå¦åˆ™å°±æ˜¯y2.
    - å…ˆéªŒæ¦‚ç‡ä¸ä¸€å®šå‡†ç¡®ï¼Œæ²¡æœ‰ç”¨åˆ°äº‹ç‰©æœ¬èº«çš„featureï¼Œçº¯åœ¨çŒœæµ‹ã€‚

**ä¼¼ç„¶æ€§ | `likelihood`**: $P(B|A)$â€‹the probability of B being true,given A is true

!!! note "æ¦‚ç‡å’Œä¼¼ç„¶"
	**æ¦‚ç‡ï¼š** åœ¨ä¸€ä»¶äº‹çš„ç»“æœæœªçŸ¥çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡äº‹ä»¶è‡ªèº«çš„æ€§è´¨ä¼°è®¡äº‹ä»¶å„ä¸ªç»“æœçš„å¯èƒ½æ€§çš„å¤§å°ï¼Œå°±æ˜¯äº‹ä»¶å„ä¸ªç»“æœå‘ç”Ÿçš„æ¦‚ç‡ã€‚ï¼ˆæŠ›ç¡¬å¸ï¼šç¡¬å¸æœ‰ä¸¤é¢ï¼Œæ‰€ä»¥ä¸¤é¢åˆ†åˆ«æœä¸Šçš„æ¦‚ç‡éƒ½æ˜¯ç™¾åˆ†ä¹‹äº”åï¼Œæ¦‚ç‡åªæœ‰åœ¨äº‹ä»¶å‘ç”Ÿå‰æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºå½“ç¡¬å¸æŠ›å‡ºåï¼Œç»“æœå°±å·²ç»ç¡®å®šäº†ã€‚ï¼‰<br>
	**ä¼¼ç„¶ï¼š** åŸºäºäº‹ä»¶å·²ç»ç¡®å®šçš„ç»“æœæ¥æ¨æµ‹äº§ç”Ÿè¿™ä¸ªç»“æœçš„å¯èƒ½ç¯å¢ƒï¼ˆç¯å¢ƒä¸­çš„æŸäº›å‚æ•°ï¼‰ã€‚ï¼ˆæŠ›ç¡¬å¸ï¼šç›´æ¥æŠ›ç¡¬å¸10000æ¬¡ï¼Œå…¶ä¸­8000æ¬¡æ­£é¢æœä¸Šï¼Œ2000æ¬¡åé¢æœä¸Šï¼Œæˆ‘ä»¬ä¼šè®¤ä¸ºç¡¬å¸çš„æ„é€ æ¯”è¾ƒç‰¹æ®Šï¼Œè¿›è€Œæ¨æµ‹è¯¥ç¡¬å¸çš„å…·ä½“å‚æ•°ï¼‰



**åéªŒæ¦‚ç‡ | `posterior`**: $P(A|B)$â€‹

è´å¶æ–¯å®šç†
$$
P(A|B) = P(A)\frac{P(B|A)}{P(B)}
$$

$$
[åéªŒæ¦‚ç‡] = [å…ˆéªŒæ¦‚ç‡]\times[åéªŒæ¦‚ç‡]
$$

[ã€å®˜æ–¹åŒè¯­ã€‘è´å¶æ–¯å®šç†ï¼Œä½¿æ¦‚ç‡è®ºç›´è§‰åŒ–_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1R7411a76r)

> rationality is not about knowing facts, it's about recognizing which facts are relevant
>
> æ–°è¯æ®ä¸èƒ½ç›´æ¥å†³å®šçœ‹æ³•ï¼Œè€Œæ˜¯åº”è¯¥æ›´æ–°ä½ çš„è§‚ç‚¹








æœ€å¤§ä¼¼ç„¶æ¦‚ç‡å†³ç­– MLE

$$
P(y_i|x) = \frac{P(x|y_i) P(y_i)}{P(x)}
$$


**Bayes Decision Rule**

Decide $y_1$, if $P(y_1|x)> P(y_2|x)$,otherwise $y_2$

> å› ä¸º$P(x)$ä¸ç±»åˆ«$y_i$æ— å…³ï¼Œæ‰€ä»¥å¯ä»¥çœç•¥

**æœ€å°åŒ–é”™è¯¯æ¦‚ç‡**

æœ€å°é”™è¯¯å…¶å®å’Œæœ€å¤§åéªŒæ¦‚ç‡æ˜¯ç­‰ä»·çš„ï¼Œå› ä¸ºæœ€å°é”™è¯¯å°±æ˜¯æœ€å¤§åŒ–åéªŒæ¦‚ç‡ã€‚ï¼ˆä½¿ç”¨äºŒåˆ†ç±»æ¥ç†è§£ï¼‰

[ä¾‹å­](https://blog.csdn.net/Harry_Jack/article/details/111242672)



### Bayesian Risk | è´å¶æ–¯é£é™©

> å¹¶ä¸æ˜¯æ‰€æœ‰çš„é”™è¯¯ä»£ä»·éƒ½æ˜¯ç›¸åŒçš„

å¦‚æœå°†ä¸¤ä¸ªç±»åˆ«äº’ç›¸è¯†åˆ«é”™è¯¯çš„é£é™©ç›¸å½“çš„å‰æä¸‹ï¼Œç±»åˆ«åªéœ€è¦æ¯”è¾ƒä¸¤è€…çš„åéªŒæ¦‚ç‡å³å¯ï¼Œå°±ä¸ä¹‹å‰ç»˜åˆ¶ç›´æ–¹å›¾ç±»ä¼¼ã€‚å¦‚æœä¸¤è€…ä¸ç›¸ç­‰ï¼Œåˆ™éœ€è¦ä¾æ®é£é™©å‡½æ•°æ¯”è¾ƒå¤§å°è¿›è¡Œç±»åˆ«åˆ¤å®šã€‚

$$
\begin{align*}
E_{ij} = E(\hat{y_i}|y_j)\\
R(\hat{y_1}|x) = E_{11} P(y_1|x)+E_{12}P(y_2|x) = E_{12}P(y_2|x)\\
R(\hat{y_2}|x) = E_{21} P(y_1|x)+E_{22}P(y_2|x) = E_{21}P(y_1|x)\\
\end{align*}
$$

å†³ç­–æ–¹æ³•ï¼šé€‰é£é™©æœ€å°çš„ï¼Œdecide $y_1$ if $R(\hat{y_1}|x) < R(\hat{y_2}|x)$

- äºŒåˆ†ç±»ä¸­ï¼šå½“ likelihood ratio è¶…è¿‡æŸä¸ªä¸xæ— å…³çš„é˜ˆå€¼æ—¶å€™ï¼Œå°±åšå†³ç­–

$$
\begin{align*}
E_{21}P(x|y_1)P(y_1) > E_{12}P(x|y_2)P(y_2)\\
\frac{P(x|y_1)}{P(x|y_2)} > \frac{E_{12}P(y_2)}{E_{21}P(y_1)}\\
\end{align*}
$$

> å¦‚æœ$E_{12} = E_{21}$â€‹ï¼Œæœ€å°åŒ–é£é™©å‡½æ•°Riskå°±æ˜¯æœ€å¤§åŒ–åéªŒæ¦‚ç‡P(yi|x)
>
> ä½ ä¼šè§‰å¾—è¦è®©ç­‰å¼å·¦è¾¹çš„Rè¶Šå°ï¼Œç­‰å¼å³è¾¹çš„Pä¸æ˜¯ä¹Ÿè¯¥è¶Šå°å—ï¼Ÿä¸ºä»€ä¹ˆè¦æœ€å¤§åŒ–å‘¢ï¼Ÿä»”ç»†çœ‹ä¸‹è¡¨ä½ å°±ä¼šå‘ç°Rä¸­çš„$\hat{y}$çš„ä¸‹æ ‡å’ŒPä¸­çš„$y$çš„ä¸‹æ ‡æ˜¯ä¸ä¸€æ ·çš„ï¼Œä½ è¦$\hat{y_1}$çš„Riskå€¼è¶Šå°ï¼Œ$\hat{y_2}$å¯¹åº”çš„På€¼å°±è¶Šå°ï¼Œ$y_1$å¯¹åº”çš„På€¼å°±åº”è¯¥è¶Šå¤§ï¼Œæ‰€ä»¥ç¡®å®æ˜¯given xæ¡ä»¶ä¸‹$y_1$çš„åéªŒæ¦‚ç‡è¶Šå¤§

- å¤šåˆ†ç±»çš„æƒ…å†µï¼šseek a decision rule that minimizes the probability of error or maximizes the accuracy;

$$
\begin{align*}
    E(\hat{y_i}|y_j) = \begin{cases}
    0 & if\ i=j\\
    1 & if\ i\ne j
    \end{cases}
\end{align*}
$$



### å›é¡¾

è´å¶æ–¯çš„æ¡†æ¶
- çŸ¥é“å…ˆéªŒæ¦‚ç‡P(yi)ï¼ŒçŸ¥é“ä¼¼ç„¶P(x|yi)ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªæœ€ä¼˜çš„åˆ†ç±»å™¨ã€‚
- ç°å®ç”Ÿæ´»ä¸­ï¼Œå¾ˆéš¾è·å–åˆ°å‡†ç¡®çš„ä¼¼ç„¶çš„ä¿¡æ¯ï¼ˆç‰¹å¾ç»´åº¦å¤ªé«˜æˆ–è€…ç‰¹å¾å¹¶ä¸å……åˆ†ï¼‰ã€‚
- å¸¸ç”¨çš„åšæ³•ï¼šåˆ©ç”¨è®­ç»ƒæ•°æ®å»ä¼°è®¡å‡ºå…ˆéªŒæ¦‚ç‡å’Œä¼¼ç„¶ï¼Œå†å»åšè´å¶æ–¯å†³ç­–ã€‚

classifier assigns a feature when:

$$
g_i(x) > g_j(x), \forall j \neq i
$$



![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240630201905.png)
ä¸­é—´çš„èŠ‚ç‚¹æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨

- åˆ¤åˆ«å‡½æ•°æ˜¯å…ˆéªŒ
- åˆ¤åˆ«å‡½æ•°æ˜¯åéªŒï¼šè´å¶æ–¯å†³ç­–
- ä¼¼ç„¶å‡½æ•°ï¼šæå¤§ä¼¼ç„¶ä¼°è®¡
- æœŸæœ›é£é™©æœ€å°åŒ–ï¼šè´å¶æ–¯é£é™©


### æå¤§ä¼¼ç„¶æ³• maximum likehood


**Decision Regions and surfaces**

- learning çš„è¿‡ç¨‹å…¶å®å°±æ˜¯å°†feature space åˆ†æˆä¸åŒçš„ decision regions
- ç°å®ç”Ÿæ´»ä¸­ï¼Œç”±äºåªèƒ½ä»æœ‰é™çš„æ ·æœ¬ä¸­å­¦ä¹ ï¼Œæ‰€ä»¥åªèƒ½å¾—åˆ°likelihoodå’Œå…ˆéªŒçš„ä¼°è®¡å€¼

<img src="https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/image-20240630210634705.png" alt="image-20240630210634705" style="zoom:33%;" />

**ç¦»æ•£å½¢å¼æå¤§ä¼¼ç„¶ä¼°è®¡**

- å…ˆéªŒæ¦‚ç‡ï¼šå°†é¢‘ç‡ä¼°è®¡ä¸ºæ¦‚ç‡$P\left(y_{k}\right)=\frac{N_{y_{k}}}{N}$

- ä¼¼ç„¶ï¼šåœ¨ç±»åˆ«ä¸º$y_k$çš„æ ·æœ¬ä¸­ç‰¹å¾ä¸º$x_i$æ ·æœ¬çš„å æ¯”ã€‚$P\left(x_{i} \mid y_{k}\right)=\frac{\left|x_{i k}\right|}{N_{y_{k}}}$
  

**è¿ç»­å½¢å¼æå¤§ä¼¼ç„¶ä¼°è®¡**

- discretize: the range into binsï¼Œå¯¹äºä½“é‡æ¥è¯´ï¼Œå¯ä»¥50-100kgï¼Œ100-150kgï¼Œ150-200kgã€‚å†å°†æ•°æ®æ®µç¦»æ•£æˆä¸åŒçš„ç±»åˆ«å³å¯ã€‚
- two-way split: æš´åŠ›åˆ†ä¸ºä¸¤ä¸ªæ®µï¼Œè®¾ç½®ä¸€ä¸ªä¸­é—´å€¼ï¼Œå°äºä¸­é—´å€¼çš„è®¾ä¸ºä¸€ç±»ï¼Œå¤§äºä¸­é—´å€¼çš„è®¾ä¸ºå¦ä¸€ç±»ã€‚
- Probability Density estimation: assume attribute follows a normal distribution or some other distribution



### æœ´ç´ è´å¶æ–¯

curse of dimensionality: feature space becomes sparse 

å‡è®¾ç‰¹å¾ä¹‹é—´æ˜¯ç‹¬ç«‹çš„

$$
\begin{align*}
    P(y|x_1,\dots,x_p) \propto  P(x_1,\dots,x_p | y) P(y)= P(y)P(x_1|y)P(x_2|y)\dots P(x_p|y)\\
\end{align*}
$$

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240630201709.png)

å¥½å¤„ï¼š
- robust to isolated noise points
- can handle missing values
- robust to irrelevant features
- å¯è§£é‡Šæ€§éå¸¸å¥½
- è®¡ç®—é‡éå¸¸å°
- åœ¨å®è·µä¸­è¡¨ç°å¥½çš„åŸå› ï¼šæ•°æ®ä¸­çš„ç‰¹å¾ä¹‹é—´çš„å…³ç³»å¾ˆå¼±ï¼›æˆ–è€…å°±ç®—ç­‰å¼ä¸¤ä¾§ä¸ç›¸ç­‰ï¼Œå…¶å¤§å°çš„ç›¸å¯¹å…³ç³»ä»ç„¶æ˜¯ä¸€è‡´çš„

é—®é¢˜ï¼š
- ä¸Šè¿°å‡è®¾åœ¨å®é™…ä¸­å¯èƒ½å¹¶ä¸æˆç«‹
- float point underflow
- 0 probability
  - laplace smoothing: $P(x_i| y_k) = \frac{|x_{ik}|+1}{N_{y_k}+K}$,Kæ˜¯labelçš„æ•°é‡

!!! note "æ”¹æˆå–$\ln$çš„åŸå› "
    æœ€é‡è¦çš„ä¸æ˜¯å€¼æœ¬èº«ï¼Œè€Œæ˜¯ç›¸å¯¹å¤§å°
    ä¸ºäº†é¿å…å‘ä¸Šæº¢å‡ºï¼Œå’Œå‘ä¸‹æº¢å‡ºï¼ˆæµ®ç‚¹æ•°é—®é¢˜ï¼‰ï¼Œtake a log

!!! example "ä¾‹é¢˜"
    é¦–å…ˆéœ€è¦å°†æ–‡æœ¬è¡¨ç¤ºæˆè¯å‘é‡ï¼Œå†ä»è¯å‘é‡ä¸­è®¡ç®—å¾—åˆ°æ¡ä»¶æ¦‚ç‡ $P(X|C)$å’Œå…ˆéªŒæ¦‚ç‡ $P(C)$
    ç„¶ååˆ©ç”¨æ¡ä»¶æ¦‚ç‡ $P(X|C)$ä¸å…ˆéªŒæ¦‚ç‡ $P(C)$è®¡ç®—åéªŒæ¦‚ç‡ $P(C_0|X)$ã€ $P (C_1|X)$
    æœ€ç»ˆæ¯”è¾ƒ $P(C_0|X)$ã€ $P (C_1|X)$å¤§å°å¾—åˆ° $X$ å±äº $C_0$ ç±»è¿˜æ˜¯ $C_1$ ç±»





## linear Regression
y æ˜¯ä¸€ä¸ªè¿ç»­çš„å€¼ï¼›
åŒºåˆ«äºclassificationï¼Œyæ˜¯ä¸€ä¸ªç¦»æ•£çš„å€¼

### Polynomial Curve Fitting

$f(x,\omega) = \omega_0 + \omega_1x + \omega_2x^2 + \omega_3x^3 + \dots + \omega_Mx^M = \sum_{j=0}^{M}\omega_jx^j$

loss function: MSE: 


$MSE(\omega) = \frac{1}{N}\sum_{i=1}^{N}(y_i - f(x_i,\omega))^2$

æ¨¡å‹æ˜¯å·²çŸ¥çš„ï¼Œ$\omega$æœªçŸ¥ï¼Œé€šè¿‡æœ€å°åŒ–MSEæ¥æ±‚è§£$\omega$

- æœ€å°äºŒä¹˜æ³•

åªèƒ½ç”¨äºçº¿æ€§å›å½’

!!! note "è¿‡ç¨‹"
    using matrix notation for convenience: $X = [1,x,x^2,x^3,...,x^n], y = [y_1,y_2,...,y_n]^T$

    $Loss(\omega) = (y - X^T\omega)^T(y - X^T\omega)$

    æ¢¯åº¦ï¼š $\nabla_{\omega}Loss(\omega) = -2X(y - X^T\omega)$

    ä»¤æ¢¯åº¦ä¸º0ï¼Œæ±‚è§£$\omega$ï¼Œå¾—åˆ°$\omega = (X^TX)^{-1}X^Ty$

!!! tips "å‡ ä½•ç†è§£"

    

- Gradient Descent | æ¢¯åº¦ä¸‹é™æ³•
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240806020009.png)
æ­¥é•¿çš„å¤§å°ï¼šå­¦ä¹ ç‡
Time complexity: $O(ndt)$,tæ˜¯è¿­ä»£æ¬¡æ•°,dæ˜¯ç‰¹å¾çš„æ•°é‡,næ˜¯æ ·æœ¬çš„æ•°é‡


stochastic gradient descent | éšæœºæ¢¯åº¦ä¸‹é™æ³•
- randomly select b(batch size) samples from the training set
- Time complexity: $O(bdt)$


- Quasi Newton Method | æ‹Ÿç‰›é¡¿æ³•



### overfitting | è¿‡æ‹Ÿåˆ
åœ¨æµ‹è¯•é›†ä¸Šæ•ˆæœå¥½çš„æ¨¡å‹å°±æ˜¯å¥½çš„æ¨¡å‹

åœ¨æµ‹è¯•é›†ä¸Šæ•ˆæœå·®çš„æ¨¡å‹å°±æ˜¯å·®æ¨¡å‹

æŠŠtraining dataä¸­çš„noiseä¹Ÿå­¦ä¹ åˆ°äº†

- Ridge Regression | å²­å›å½’

Loss(\omega*)

regularization: ä¸€äº›å…ˆéªŒçš„å‡è®¾ï¼Œæ¯”å¦‚$\omega$æ˜¯ç¨€ç–çš„ï¼Œæˆ–è€…$\omega$æ˜¯å¹³æ»‘çš„ï¼›é¿å…å­¦ä¹ åˆ°

è¶…å‚æ•°ï¼š
- $\lambda$ï¼šæ§åˆ¶æ­£åˆ™åŒ–çš„å¼ºåº¦: $\lambda$è¶Šå¤§ï¼Œæ­£åˆ™åŒ–çš„å¼ºåº¦è¶Šå¤§ï¼Œæ¨¡å‹è¶Šç®€å•ï¼Œtraining error è¶Šå¤§ï¼›å¦‚ä¸‹å›¾ï¼Œå·¦ä¾§å«åšè¿‡æ‹Ÿåˆï¼Œå³ä¾§å«åšæ¬ æ‹Ÿåˆ![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240806021738.png)
- $\alpha$ï¼šæ§åˆ¶å­¦ä¹ ç‡





### Logistic Regression
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240807233452.png)

çº¿æ€§å›å½’æœ‰ä¸€ä¸ªå¾ˆå¼ºçš„å‡è®¾ï¼Œå°±æ˜¯yæ˜¯è¿ç»­çš„ï¼›å¹¶ä¸”æœ‰æ›´åƒé‚»è¿‘æ•°çš„è¶‹åŠ¿(MSE å¯¹äºçº¿æ€§å›å½’ä¸æ˜¯ä¸€ä¸ªå¥½çš„function)

- one vs. Rest

logistic function:

- sigmoid function: $f(x) = \frac{1}{1+e^{-x}}$
CDF(ç´¯ç§¯åˆ†å¸ƒå‡½æ•°)ofthe standard logistic distribution   
ä½¿ç”¨sigmoidå‡½æ•°å°†çº¿æ€§å›å½’çš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡

!!! note "logistic Regreesionæ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡å‹"
    ä¸»è¦è€ƒè™‘çš„æ˜¯decision boundary
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240807234021.png)


ä¸ºä»€ä¹ˆloss functionè¦å–log
- ä¸ºäº†æ–¹ä¾¿æ±‚å¯¼
- å–logä½¿å¾—è¿ä¹˜å˜æˆè¿åŠ ï¼Œä¸ä¼šä¸¢å¤±ä¿¡æ¯

Assumptions behind logistic regression
- l(a) = -\sum_{i\in I} \log(1+e^{-y_i a^T x_i})


pros:
- binomial distribution is a  good assumption for classification
- provide a probability
- low computation, easy to optimize
- support online learning:æ¢¯åº¦ä¸‹é™çš„æ¨¡å‹éƒ½æ”¯æŒåœ¨çº¿å­¦ä¹ 

cons:
- too simple:high bias & low variance


### Support Vector Machine
è®¡ç®—æ¯ä¸ªç‚¹åˆ°å†³ç­–è¾¹ç•Œçš„è·ç¦»$\gamma$

- Maximum Margin Classifier:æ•°æ®é›†æœ€å°çš„margin
ç›®çš„å°±æ˜¯è¦æ‰¾åˆ°ä¸€ä¸ªå†³ç­–è¾¹ç•Œï¼Œä½¿å¾—marginæœ€å¤§

ä¸ºä»€ä¹ˆä½¿ç”¨è¿™ç§æ–¹æ³•
- è£•åº¦æ›´é«˜ï¼Œå®¹é”™æ€§æ›´å¥½
- å¦‚æœmarginè¶Šå¤§ï¼Œå¯¹äºå™ªå£°çš„å®¹å¿åº¦è¶Šé«˜

### loss function


task



## å­¦ä¹ èµ„æº

[Machine Learning in Practice Crash Course | Jinming Hu (conanhujinming.github.io)](https://conanhujinming.github.io/post/ml_in_practice_crash_course/)

[å®ç”¨çš„æœºå™¨å­¦ä¹  ç¬¬ä¸€è¯¾ æœºå™¨å­¦ä¹ å¯¼è®º 2024summer_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1Gw4m1i7ys/?spm_id_from=333.788.recommend_more_video.0&vd_source=8b7a5460b512357b2cf80ce1cefc69f5)

[æœºå™¨å­¦ä¹ 2023-10-19ç¬¬6-8èŠ‚ (zju.edu.cn)](https://classroom.zju.edu.cn/livingpage?course_id=53449&sub_id=915451&tenant_code=112)

èµµæ´²è€å¸ˆ







[æœ‰ç›‘ç£çš„æœºå™¨å­¦ä¹ ï¼šå›å½’ä¸åˆ†ç±» | Coursera](https://www.coursera.org/learn/machine-learning?action=enroll)

[CS229å´æ©è¾¾æœºå™¨å­¦ä¹ ](https://www.bilibili.com/video/BV16J411t71N)

[CS229: Machine Learning (stanford.edu)](https://cs229.stanford.edu/)



æ·±åº¦å­¦ä¹ 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/)ï¼šdeep learning for CV

[å›¾çµç­ã€Šæœºå™¨å­¦ä¹ ã€‹è¯¾ç¨‹æ€»ç»“ - CC98è®ºå›](https://www.cc98.org/topic/5599897)

æˆ‘åœ¨å¿ƒçµå­¦MLç³»åˆ—doge

[å†æ¬¡å…¥é—¨deep learningä»¥åŠä¸€äº›å›å¿†ï¼ˆæ›´æ–°ç¬¬äºŒéƒ¨åˆ†ï¼‰ - CC98è®ºå›](https://www.cc98.org/topic/5207160)

[å†æ¬¡å…¥é—¨deep learningï¼Œè¿™æ¬¡ç›´æ¥ä¸Šé‡ç‚¹ï¼ˆå®Œç»“ç¯‡ï¼‰ - CC98è®ºå›](https://www.cc98.org/topic/5208795)



å­”é™¢ğŸ•ğŸ¦è¯¾

[äººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹ å¤ä¹ èµ„æ–™ - CC98è®ºå›](https://www.cc98.org/topic/5518130)

[2022-2023ç§‹å†¬äººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹ çº¿ä¸Šè€ƒè¯•å›å¿†å·ï¼ˆå’Œå¤§ä½¬ä»¬å‘é‡äº†ï¼‰ - CC98è®ºå›](https://www.cc98.org/topic/5508899)

[äººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹ å›å¿†å· - CC98è®ºå›](https://www.cc98.org/topic/5234359)


### ä¼šè®®è®ºæ–‡
- ICML (International Conference on Machine
Learning)
- NeurIPS (Neural Information Processing Systems)
- KDD (ACM SIGKDD Conference on Knowledge Discovery and Data Mining)
- AAAI (AAAI conference on Artificial Intelligence)

æœ¬äººå†œå­¦åšå£«ï¼Œç§‘ç ”æ¥è§¦çš„æœºå™¨å­¦ä¹ ï¼Œä¹‹å‰æœ‰è®¡ç®—æœºçš„å¯¼å¸ˆé¢†å…¥é—¨äº†ã€‚ä¸ªäººç›®å‰é‡åˆ°æœ€å¥½çš„æ•™ç¨‹æ˜¯å´æ©è¾¾çš„è§†é¢‘è¯¾ç¨‹ï¼Œå› ä¸ºä»–å……åˆ†è€ƒè™‘åˆ°äº†å­¦ç”Ÿçš„æ°´å¹³ï¼ŒæŠŠéœ€è¦çš„æ•°å­¦çŸ¥è¯†ä¹Ÿè®²äº†ï¼Œå…ˆçœ‹äº†å´æ©è¾¾æ—©æœŸçš„æœºå™¨å­¦ä¹ ï¼ˆåå‘ä¼ æ’­çš„é‚£èŠ‚è®²çš„ä¸æ˜¯å¾ˆå¥½ï¼‰ï¼Œç„¶åè¿‘ä¸¤å¹´çš„æ·±åº¦å­¦ä¹ ï¼Œåœ¨çœ‹äº†bç«™åŒ—å¤§çš„tensorflowç¬”è®°è¯¾ç¨‹ï¼Œè§‰å¾—è‡³å°‘çŸ¥é“è¯¥æ€ä¹ˆåšæœºå™¨å­¦ä¹ ï¼ˆåŒ…æ‹¬æ·±åº¦å­¦ä¹ ï¼‰äº†ã€‚ ä¸è¿‡ä½œä¸ºä¸€ä¸ªéè®¡ç®—æœºä¸“ä¸šçš„å­¦ç”Ÿï¼Œä¸ªäººè§‰å¾—æ‰€æœ‰çš„æ•™ç¨‹éƒ½å¿½è§†äº†ä¸€ä¸ªæœ€åŸºç¡€ä½†æ˜¯ä¹Ÿæ˜¯æœ€é‡è¦çš„ä¸œè¥¿â€”â€”**ç‰¹å¾å·¥ç¨‹**ï¼ŒæŒ‡çš„ä¸æ˜¯ç‰¹å¾é€‰æ‹©ï¼ˆæ— ç›‘ç£å­¦ä¹ çš„é™ç»´ï¼‰ï¼Œè€Œæ˜¯ç‰¹å¾è¡¨å¾ï¼ˆfeature representï¼‰ï¼Œæ·±åº¦å­¦ä¹ é‡Œé¢å«embeddingï¼ˆè‡ªå·±çœ‹äº†åŠŸèƒ½åç†è§£çš„ï¼‰ï¼Œå°±æ˜¯æˆ‘ä»¬åº”è¯¥æ€æ ·å»è¡¨å¾é—®é¢˜ï¼Œå°†é—®é¢˜çš„ä¿¡æ¯è¡¨ç¤ºä¸ºæ•°æ®ç»™è®¡ç®—æœºè¿›è¡Œå­¦ä¹ ã€‚ä¹‹å‰çœ‹äº†ä»€ä¹ˆæœ‰ç›‘ç£å­¦ä¹ å•Šï¼Œæ— ç›‘ç£å­¦ä¹ å•Šï¼Œå¯¹ç‰¹å¾å°±æ˜¯å‘Šè¯‰ä½ æ ·æœ¬æˆ–å‘é‡ç©ºé—´ï¼Œå®Œå…¨ä¸çŸ¥é“æœºå™¨å­¦ä¹ å»åšä»€ä¹ˆã€‚åªåˆ°æœ‰ä¸ªè€å¸ˆè®©æˆ‘åœ¨åšäº†ç‰¹å¾æå–ï¼Œç„¶åé™ç»´ï¼Œç„¶ååˆ†ç±»æˆ–é¢„æµ‹çš„æ—¶å€™æˆ‘æ‰æ˜ç™½æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªä»€ä¹ˆæ ·çš„è¿‡ç¨‹ã€‚



æˆ‘æ˜¯å…¥é—¨çœ‹çš„å’±ä»¬å­¦æ ¡çš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ï¼Œå¯¹æœºå™¨å­¦ä¹ å¤§æ¦‚æœ‰ä¸ªäº†è§£ï¼Œæ²¡å¤ªå…³å¿ƒæ•°å­¦ã€‚ è¯´å®è¯è¿™äº›ç®—æ³•ï¼ˆmlé‡Œä¸åŒ…å«dlçš„é‚£äº›ï¼‰æˆ‘ç§‘ç ”ä¸Šç”¨åˆ°çš„æ¯”è¾ƒå°‘ï¼Œåæ¥éšç€ç§‘ç ”çš„æ·±å…¥ä¼šå»æ€è€ƒè¿™äº›ç®—æ³•åé¢çš„æ•°å­¦åŸç†ï¼Œå°±å»å‚è€ƒæèˆªçš„æœºå™¨å­¦ä¹ ï¼Œè¥¿ç“œä¹¦ã€‚æ›´åŠ é«˜å±‹å»ºç“´ä¸€ç‚¹çš„æ•™æå°±æ˜¯PRMLäº†ã€‚ æˆ‘æ¯”è¾ƒæ¨èUCBçš„CS188ï¼Œä»æ•´ä¸ªäººå·¥æ™ºèƒ½çš„è§’åº¦è®²é—®é¢˜ï¼Œæœºå™¨å­¦ä¹ æ˜¯å…¶ä¸­çš„ä¸€ä¸ªéƒ¨åˆ†ã€‚ç¼–ç¨‹é¡¹ç›®æœ‰è¶£è¿è´¯ã€‚ç”¨çš„æ•™æä¹Ÿæ˜¯ç»å…¸ï¼Œä¸€äº›æ€æƒ³ç°åœ¨ä¹Ÿä¸è¿‡æ—¶ã€‚



pipeline

å®Œæˆå®ç”¨æœºå™¨å­¦ä¹ çš„æ‰€æœ‰ä½œä¸š

å®Œæˆèµµæ´²è€å¸ˆçš„æ‰€æœ‰ä½œä¸š+å¤§ä½œä¸š

