# GPT

- W_E GPT3 50267个token，每个token具有12288维度，共6亿左右参数，随机初始化
## Interpretability

词汇存在高维向量当中，向量的方向可以编码不同的含义

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/202507040930043.png)

transformer大部分的参数在MLP层当中（约占用2/3的参数，GPT3 - 12亿）

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/202507040940946.png)

第一个线性层可以使用行视角，视作嵌入空间中的方向

ReLU 类似于与门，只有最终结果为正数时，才会输出

第二个线性层，可以使用列视角，如果某个列向量学习到了“篮球”的概念，同时对应的向量又被激活


![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/202507040938039.png)


在$N$维空间当中，如果使用正交基表示一个概念，那么最多只能表示$N$个概念

johnson-lindenstrauss lemma 告诉我们，如果使用非正交基，那么可以表示更多的概念，尤其是在高维空间当中。能表示的概念数量与维数$n$成指数分布

这也说明，某个概念并不是单纯由一个单元激活，而是由多个单元激活（superposition）

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=113215035936825&bvid=BV1aTxMehEjK&cid=26046694390&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height=450px></iframe>

### 拓展阅读
芝加哥大学victor veitch 的论文

Anthropic Transformer circuit

[Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)

[Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html)

- RLHF
- scaling law







