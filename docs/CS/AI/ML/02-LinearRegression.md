# 02 | Linear Regression

## OLS

è€ƒè™‘ç»å…¸çš„çº¿æ€§å›å½’æ¨¡å‹ï¼š

$$
y = X \beta + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I_n)
$$

å…¶ä¸­ï¼š

* $y \in \mathbb{R}^n$ï¼šå“åº”å˜é‡
* $X \in \mathbb{R}^{n \times p}$ï¼šæ»¡ç§©è®¾è®¡çŸ©é˜µï¼ˆåˆ—æ»¡ç§©ï¼‰
* $\beta \in \mathbb{R}^p$ï¼šæœªçŸ¥å›å½’ç³»æ•°
* $\varepsilon$ï¼šç‹¬ç«‹åŒåˆ†å¸ƒå™ªå£°ï¼Œå‡å€¼ 0ï¼Œæ–¹å·® $\sigma^2$


æ®‹å·®å¹³æ–¹å’Œï¼ˆResidual Sum of Squares, RSSï¼‰å®šä¹‰ä¸ºï¼š

$$
RSS = \sum_{i=1}^n \left( y_i - x_{i1}\beta_1 - \cdots - x_{ip}\beta_p \right)^2
$$

ä¹Ÿå¯ä»¥å†™æˆå‘é‡å½¢å¼ï¼š

$$
RSS = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
$$

æœ€å°äºŒä¹˜ä¼°è®¡ï¼ˆOrdinary Least Squares, OLSï¼‰å°±æ˜¯é€‰æ‹©ä½¿ RSS æœ€å°çš„ $\boldsymbol{\beta}$ï¼š

$$
\widehat{\beta} = \underset{\boldsymbol{\beta}}{\operatorname*{arg\,min}} \; (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\mathsf{T}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})
$$


- To estimate $\beta$, we set the derivative equal to 0
$$\frac{\partial \text{RSS}}{\partial \beta} = -2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \beta) = 0$$

$$
\widehat{\beta} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}
$$

- $\mathbf{X}$ full rank $\iff \mathbf{X}^\top \mathbf{X}$ invertible

### æ€§è´¨

$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$




**æ— å**




æˆ‘ä»¬è®¡ç®— $\mathbb{E}[\hat{\beta}]$ï¼š

$$
\begin{aligned}
\mathbb{E}[\hat{\beta}] &= \mathbb{E}[(X^T X)^{-1} X^T y] \\
&= (X^T X)^{-1} X^T \mathbb{E}[y] \\
&= (X^T X)^{-1} X^T (X\beta) \\
&= (X^T X)^{-1} X^T X \beta \\
&= \beta
\end{aligned}
$$

---

**æ–¹å·®**

å°† $\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}$ ä»£å…¥ï¼š

$$
\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top (\mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon}) \\
= \boldsymbol{\beta} + (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\varepsilon}
$$

æœ‰

$$
\begin{aligned}
\operatorname{Var}(\hat{\boldsymbol{\beta}})
&= \operatorname{Var} \left( (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \boldsymbol{\varepsilon} \right) \\
&= (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \operatorname{Var}(\boldsymbol{\varepsilon}) \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \\
&= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{X} (\mathbf{X}^\top \mathbf{X})^{-1} \\
&=\boxed{ \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1} }\\
&= \widehat{\sigma}^2 (\mathbf{X}^\top \mathbf{X})^{-1} \quad \text{ï¼ˆå¯ç”¨æ®‹å·®å¹³æ–¹å’Œä¼°è®¡ï¼‰} \\
&= \frac{RSS}{n - p} (\mathbf{X}^\top \mathbf{X})^{-1} \\
&= \frac{1}{n - p} \sum_{i=1}^n \hat{\varepsilon}_i^2 (\mathbf{X}^\top \mathbf{X})^{-1}
\end{aligned}
$$


---

**UMVUE**

Lehmannâ€“ScheffÃ© å®šç†å‘Šè¯‰æˆ‘ä»¬ï¼š

> è‹¥æŸæ— åä¼°è®¡é‡æ˜¯å……åˆ†ç»Ÿè®¡é‡çš„å‡½æ•°ï¼Œåˆ™å®ƒæ˜¯ UMVUEã€‚

æˆ‘ä»¬æ¥éªŒè¯ï¼š

1ï¸âƒ£ $\hat{\beta}$ æ˜¯ $\beta$ çš„æ— åä¼°è®¡é‡ â†’ âœ…

å·²è¯

2ï¸âƒ£ $X^T y$ æ˜¯å……åˆ†ç»Ÿè®¡é‡ â†’ âœ…

ç”±**å› å­åˆ†è§£å®šç†**ï¼š

* $y \sim \mathcal{N}(X\beta, \sigma^2 I)$
* è”åˆå¯†åº¦å‡½æ•°å¯ä»¥å†™æˆå…³äº $X^T y$ çš„å‡½æ•°å’Œä¸å« $\beta$ çš„å‡½æ•°ä¹‹ç§¯
* æ‰€ä»¥ $X^T y$ æ˜¯ $\beta$ çš„å……åˆ†ç»Ÿè®¡é‡

è€Œ $\hat{\beta}$ æ˜¯ $X^T y$ çš„å‡½æ•° â‡’ å®ƒæ˜¯**å……åˆ†ç»Ÿè®¡é‡çš„å‡½æ•°**

âœ… æ»¡è¶³ Lehmannâ€“ScheffÃ© å®šç†æ¡ä»¶ â‡’ æ˜¯ UMVUEï¼

---

æˆ–è€…ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ Gauss-Markov å®šç†ï¼ˆéæ­£æ€æ¡ä»¶ä¸‹ï¼‰

> åœ¨çº¿æ€§æ¨¡å‹ä¸­ï¼Œåœ¨æ‰€æœ‰çº¿æ€§æ— åä¼°è®¡é‡ä¸­ï¼ŒOLS æ˜¯æ–¹å·®æœ€å°çš„ã€‚

ä½†è¦æ³¨æ„ï¼š

* **Gauss-Markov å®šç† â†’ æœ€ä¼˜çº¿æ€§æ— åä¼°è®¡ï¼ˆBLUEï¼‰**
* **Lehmannâ€“ScheffÃ© å®šç†ï¼ˆ+ æ­£æ€æ€§ï¼‰â†’ æœ€å°æ–¹å·®æ— åä¼°è®¡ï¼ˆUMVUEï¼‰**



### Training Error & Test Error

$$
\begin{aligned}
\mathbb{E}[\mathrm{TestErr}] &= \mathbb{E}\|\mathbf{y}^*-\mathbf{X}\widehat{\beta}\|^2 \\
&= \mathbb{E}\|(\mathbf{y}^*-\mathbf{X}\beta)+(\mathbf{X}\beta-\mathbf{X}\widehat{\beta})\|^2 \\
&= \mathbb{E}\|\mathbf{y}^*-\mu\|^2 + \mathbb{E}\|\mathbf{X}(\widehat{\beta}-\beta)\|^2 \\
&= \mathbb{E}\|\mathbf{e}^*\|^2 + \mathrm{Trace}(\mathbf{X}^\mathsf{T}\mathbf{X}\,\mathrm{Cov}(\widehat{\beta})) \\
&= n\sigma^2 + p\sigma^2
\end{aligned}
$$

$$
\begin{aligned}
\mathbb{E}[\mathrm{TrainErr}] &= \mathbb{E}\|\mathbf{y}-\mathbf{\widehat{y}}\|^2 = \mathbb{E}\|(\mathbf{I}-\mathbf{H})\mathbf{y}\|^2 \\
&= \mathbb{E}\|(\mathbf{I}-\mathbf{H})\mathbf{e}\|^2 \\
&= \mathrm{Trace}\left((\mathbf{I}-\mathbf{H})^\mathsf{T}(\mathbf{I}-\mathbf{H})\,\mathrm{Cov}(\mathbf{e})\right) \\
&= (n-p)\sigma^2
\end{aligned}
$$

## å¹¿ä¹‰çº¿æ€§å›å½’

### logistic

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240807233452.png)

çº¿æ€§å›å½’æœ‰ä¸€ä¸ªå¾ˆå¼ºçš„å‡è®¾ï¼Œå°±æ˜¯yæ˜¯è¿ç»­çš„ï¼›å¹¶ä¸”æœ‰æ›´åƒé‚»è¿‘æ•°çš„è¶‹åŠ¿(MSE å¯¹äºçº¿æ€§å›å½’ä¸æ˜¯ä¸€ä¸ªå¥½çš„function)

- one vs. Rest

logistic function:

- sigmoid function: $f(x) = \frac{1}{1+e^{-x}}$
CDF(ç´¯ç§¯åˆ†å¸ƒå‡½æ•°)ofthe standard logistic distribution   
ä½¿ç”¨sigmoidå‡½æ•°å°†çº¿æ€§å›å½’çš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡

!!! note "logistic Regreesionæ˜¯ä¸€ä¸ªçº¿æ€§æ¨¡å‹"
    ä¸»è¦è€ƒè™‘çš„æ˜¯decision boundary
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240807234021.png)


ä¸ºä»€ä¹ˆloss functionè¦å–log
- ä¸ºäº†æ–¹ä¾¿æ±‚å¯¼
- å–logä½¿å¾—è¿ä¹˜å˜æˆè¿åŠ ï¼Œä¸ä¼šä¸¢å¤±ä¿¡æ¯

Assumptions behind logistic regression
- l(a) = -\sum_{i\in I} \log(1+e^{-y_i a^T x_i})


pros:
- binomial distribution is a  good assumption for classification
- provide a probability
- low computation, easy to optimize
- support online learning:æ¢¯åº¦ä¸‹é™çš„æ¨¡å‹éƒ½æ”¯æŒåœ¨çº¿å­¦ä¹ 

cons:
- too simple:high bias & low variance


å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œåªå…³å¿ƒåˆ†ç±»æ­£ç¡®çš„ç±»çš„å€¼

## ç»Ÿè®¡è§†è§’

!!! note "å› æ­¤ï¼Œåœ¨é«˜æ–¯å™ªå£°çš„å‡è®¾ä¸‹ï¼Œæœ€å°åŒ–å‡æ–¹è¯¯å·®ç­‰ä»·äºå¯¹çº¿æ€§æ¨¡å‹çš„æå¤§ä¼¼ç„¶ä¼°è®¡ã€‚"


## Penalty

A unified framework is to minimize the objective function

$$
\arg\min_{\beta} \frac{1}{2n}\|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|^2 + \sum_{j=1}^p P_{\lambda}(\beta_j)
$$

where $P_{\lambda}(\cdot)$ is a penalty function applied on the value of each parameter, and $\lambda$ is a tuning parameter.

- Lasso: $P_{\lambda}(\beta) = \lambda|\beta|$
- Ridge: $P_{\lambda}(\beta) = \lambda\beta^2$
- Best subset: $P_{\lambda}(\beta) = \lambda\mathbf{1}\{\beta \neq 0\}$
- Elastic net: $P_{\lambda}(\beta) = \lambda_1|\beta| + \lambda_2\beta^2$


### Lasso - l1

| æ ¸å¿ƒå†…å®¹            | è§£é‡Š                                          |
| --------------- | ------------------------------------------- |
| Oracle Property | åŒæ—¶å®ç°å˜é‡é€‰æ‹©ä¸€è‡´æ€§ + æœ€ä¼˜ä¼°è®¡ç²¾åº¦                        |
| Lasso çš„é—®é¢˜       | æœ‰åå·®ï¼Œä¸èƒ½åŒæ—¶å®ç°ä¸¤è€…                                |
| ç†è®ºä¸Šæ¡ä»¶           | ä¸ºäº†é€‰å˜é‡ï¼Œ$\lambda$ è¦å¤Ÿå¤§ï¼›ä½†ä¸ºä¼°è®¡ç²¾åº¦ï¼Œ$\lambda$ åˆè¦è¶‹äº 0 |
| è§£å†³æ–¹æ³•            | æ”¹ç”¨æ— åæƒ©ç½šå‡½æ•°ï¼ˆå¦‚ SCADï¼‰ï¼Œæˆ–è€…æ¥å—ä¸€å®šæŠ˜ä¸­                   |


æ±‚è§£ä¸‹é¢çš„ä¼˜åŒ–é—®é¢˜

$$
\begin{aligned}
& \text{minimize } \sum_{i=1}^{n} \left(y_i - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2 \\
& \text{subject to } \sum_{j=1}^{p} |\beta_j| \leq s
\end{aligned}
$$

- Each value of $\lambda$ corresponds to an unique value of $s$.


#### Lasso å›å½’åœ¨æ­£äº¤è®¾è®¡ä¸‹çš„æ¨å¯¼ä¸åŸç†


**å‡è®¾ï¼š**

* è®¾è®¡çŸ©é˜µæ»¡è¶³ $\mathbf{X}^\top \mathbf{X} = \mathbf{I}_p$ï¼ˆå³åˆ—å‘é‡æ­£äº¤ï¼Œå•ä½èŒƒæ•°ï¼‰
* ç›®æ ‡æ˜¯æ±‚è§£ Lasso å›å½’é—®é¢˜ï¼š

$$
\widehat{\boldsymbol{\beta}}^{\text{lasso}} = \arg\min_{\boldsymbol{\beta}} \|\mathbf{y} - \mathbf{X} \boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|_1
$$

æ­¥éª¤ 1ï¼šæ’å…¥ OLS è§£

å› ä¸º OLS è§£ä¸ºï¼š

$$
\widehat{\boldsymbol{\beta}}^{\text{ols}} = \mathbf{X}^\top \mathbf{y}
$$

æˆ‘ä»¬å°†å…¶æ’å…¥ç›®æ ‡å‡½æ•°ï¼š

$$
\begin{align*}
\|\mathbf{y} - \mathbf{X} \boldsymbol{\beta}\|^2 &= \|\mathbf{y} - \mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}} + \mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}} - \mathbf{X} \boldsymbol{\beta}\|^2\\
&= \|\mathbf{y} - \mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}}\|^2 + \|\mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}} - \mathbf{X} \boldsymbol{\beta}\|^2 + 2 \underbrace{(\mathbf{y} - \mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}})^\top (\mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}} - \mathbf{X} \boldsymbol{\beta})}_{=0}
\end{align*}
$$

å…¶ä¸­æœ€åä¸€é¡¹ä¸º 0 æ˜¯å› ä¸ºï¼š

* æ®‹å·® $\mathbf{r} = \mathbf{y} - \mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}}$ å‚ç›´äº $\operatorname{Col}(\mathbf{X})$
* è€Œ $\mathbf{X}(\widehat{\boldsymbol{\beta}}^{\text{ols}} - \boldsymbol{\beta}) \in \operatorname{Col}(\mathbf{X})$

---

æ­¥éª¤ 2ï¼šç›®æ ‡å‡½æ•°åŒ–ç®€

å› ä¸ºç¬¬ä¸€é¡¹ä¸ $\boldsymbol{\beta}$ æ— å…³ï¼Œæˆ‘ä»¬åªéœ€æœ€å°åŒ–ç¬¬äºŒé¡¹ + æ­£åˆ™é¡¹ï¼š

$$
\min_{\boldsymbol{\beta}} \|\mathbf{X} \widehat{\boldsymbol{\beta}}^{\text{ols}} - \mathbf{X} \boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|_1\\
\leftrightarrow \min_{\boldsymbol{\beta}} \|\widehat{\boldsymbol{\beta}}^{\text{ols}} - \boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|_1 \quad (\because\mathbf{X}^\top \mathbf{X} = \mathbf{I})
$$

**å˜é‡ç‹¬ç«‹æ±‚è§£**

ç›®æ ‡å‡½æ•°å¯åˆ†è§£ä¸ºæ¯ä¸ªå‚æ•°çš„ç‹¬ç«‹ä¼˜åŒ–ï¼š

$$
\widehat{\beta}_j^{\text{lasso}} = \arg\min_{x} (x - a)^2 + \lambda |x|, \quad a = \widehat{\beta}_j^{\text{ols}}
$$

è¿™å°±æ˜¯ç»å…¸çš„ **Soft Thresholding é—®é¢˜**ï¼Œè§£ä¸ºï¼š

$$
\boxed{
\widehat{\beta}_j^{\text{lasso}} = \operatorname{sign}(a) \cdot \max(|a| - \lambda/2, 0)
}
$$

å³ï¼š

* å¦‚æœ $|a| \leq \lambda/2$ï¼Œè§£ä¸º 0
* å¦åˆ™ï¼Œåœ¨æ–¹å‘ä¸Šç¼©å‡ $\lambda/2$

Soft Thresholding = å˜é‡é€‰æ‹©æœºåˆ¶

* Ridge å›å½’ä½¿ç”¨ $\ell_2$ æƒ©ç½šï¼šç³»æ•°æ°¸è¿œä¸ä¼šå˜ä¸º 0ï¼Œåªæ˜¯å˜å°
* Lasso ä½¿ç”¨ $\ell_1$ æƒ©ç½šï¼šä¼šç›´æ¥æŠŠå°çš„ç³»æ•°å‹æˆ 0
* æ‰€ä»¥ Lasso èƒ½å®ç° **å˜é‡é€‰æ‹©ï¼ˆsparsityï¼‰**


| é¡¹ç›®             | è§£é‡Š   |
| ---------------- | ------------------------ |
| æ­£äº¤è®¾è®¡         | $\mathbf{X}^\top \mathbf{X} = \mathbf{I}$ ç®€åŒ–é—®é¢˜            |
| æ‹†åˆ†è¯¯å·®é¡¹        | æ®‹å·®é¡¹å‚ç›´äºåˆ—ç©ºé—´ï¼Œäº¤å‰é¡¹ä¸º 0                               |
| å¯åˆ†è§£ç›®æ ‡        | å¯å¯¹æ¯ä¸ª $\beta_j$ ç‹¬ç«‹æ±‚è§£                                  |
| Soft Threshold è§£ | |
| ç¨€ç–æ€§æ¥æº        | ç³»æ•°å¯èƒ½ç›´æ¥ä¸º 0ï¼Œå®ç°é€‰æ‹©                                   |
| $\lambda$ è¶Šå¤§   | è¶Šå¤šçš„å‚æ•°ä¼šè¢«å‹æˆ 0                                         |



### Ridge - l2

| è§†è§’    | è§£é‡Š                                                                                                   |
| ----- | ---------------------------------------------------------------------------------------------------- |
| æœ€ä¼˜åŒ–è§†è§’ | Ridge è§£æ˜¯æœ€å°åŒ– $\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta} \|^2 + \lambda \|\boldsymbol{\beta}\|^2$ çš„è§£ |
| è´å¶æ–¯è§†è§’ | Ridge è§£æ˜¯ $\boldsymbol{\beta} \sim \mathcal{N}(0, \frac{\sigma^2}{\lambda} \mathbf{I})$ ä¸‹çš„åéªŒå‡å€¼        |
|PCAè§†è§’||

#### ä¼˜åŒ–è§†è§’
æœ€ä¼˜åŒ–è§†è§’ï¼Œå³æ±‚è§£ä¸‹é¢çš„æœ€ä¼˜åŒ–é—®é¢˜

$$
(y - X\beta)^{\top}(y - X\beta) + \lambda\beta^{\top}\beta
$$

Take derivative with respect to $\beta$ and set to zero

$$
\begin{aligned}
\widehat{\beta}^{\mathrm{~ridge}}&= \boxed{(X^{\top}X + \lambda I)^{-1}X^{\top}y}\\&=(\mathbf{X}^\mathsf{T}\mathbf{X}+\lambda\mathbf{I})^{-1}(\mathbf{X}^\mathsf{T}\mathbf{X})(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y}\\&=(\mathbf{X}^\mathsf{T}\mathbf{X}+\lambda\mathbf{I})^{-1}(\mathbf{X}^\mathsf{T}\mathbf{X})\widehat{\boldsymbol{\beta}}^\mathsf{ols}\\&=\mathbf{Z}\widehat{\boldsymbol{\beta}}^{\mathrm{ols}}
\end{aligned}
$$


#### PCAè§†è§’

!!! note "SVD åˆ†è§£"

    $$
    \mathbf{X} = U D V^\top
    $$

    * $U$ï¼šæ­£äº¤åˆ—å‘é‡ï¼Œè¡¨ç¤ºåœ¨æ•°æ®ç©ºé—´ä¸­çš„æ–¹å‘ï¼ˆä¸»æˆåˆ†ï¼‰
    * $D$ï¼šå¥‡å¼‚å€¼ï¼ˆä¸åæ–¹å·®çŸ©é˜µç‰¹å¾å€¼ç›¸å…³ï¼‰
    * $V$ï¼šè¾“å…¥ç©ºé—´çš„æ­£äº¤åŸºï¼ˆå›å½’ç³»æ•°æ–¹å‘ï¼‰


å°†åæ–¹å·®çŸ©é˜µå†™æˆ PCA å½¢å¼ï¼š

$$
\frac{1}{n} \mathbf{X}^\top \mathbf{X} = V D^2 V^\top
$$

* è¯´æ˜åæ–¹å·®çš„ä¸»æ–¹å‘ï¼ˆç‰¹å¾å‘é‡ï¼‰å°±æ˜¯ $V$ï¼Œå¯¹åº”ç‰¹å¾å€¼ $d_j^2$
* ç¬¬ $j$ ä¸ªä¸»æˆåˆ†ä¸º $X v_j = d_j u_j$
* å¤§çš„å¥‡å¼‚å€¼æ–¹å‘ï¼šæ•°æ®æ–¹å·®å¤§ï¼Œä¿ç•™ä¿¡æ¯å¤š
* å°çš„å¥‡å¼‚å€¼æ–¹å‘ï¼šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè¦å¼ºçƒˆæƒ©ç½š

Ridge å›å½’å¯¹å“åº”å˜é‡çš„ä¼°è®¡ï¼š

$$
\mathbf{X} \hat{\boldsymbol{\beta}}^{\text{ridge}} = \sum_{j=1}^p u_j \cdot \frac{d_j^2}{d_j^2 + \lambda} \cdot u_j^\top \mathbf{y}
$$


1. æŠŠ $\mathbf{y}$ æŠ•å½±åˆ°æ¯ä¸ªä¸»æˆåˆ†æ–¹å‘ $u_j$
2. æŠ•å½±ç»“æœ $u_j^\top y$ è¢« **ç¼©å°** äº†ä¸€ä¸ªå› å­ $\frac{d_j^2}{d_j^2 + \lambda}$
3. $d_j^2$ å°çš„æ–¹å‘ï¼ˆä½æ–¹å·®ï¼‰è¢«æƒ©ç½šå¾—æ›´ä¸¥é‡ï¼Œé˜²æ­¢å¯¹å™ªå£°è¿‡æ‹Ÿåˆ


| ä¸»é¢˜     | å†…å®¹                                    |
| ------ | ------------------------------------- |
| æœ‰åæ€§    | Ridge æœ‰åï¼Œä½†å¯æ§åˆ¶åå·®                       |
| æ–¹å·®é™ä½   | Ridge æ˜¾è‘—å‡å°‘ä¼°è®¡æ–¹å·®                        |
| MSE æ›´ä¼˜ | åˆé€‚çš„ $\lambda$ å¯è®© MSE ä¼˜äº OLS           |
| å‡ ä½•ç†è§£   | Ridge åœ¨ PCA ç©ºé—´ä¸­å¯¹ä¸åŒæ–¹å‘æ–½åŠ ä¸åŒå¼ºåº¦çš„ shrinkage |
| å®ç”¨ä»·å€¼   | å°¤å…¶åœ¨é«˜ç»´/å…±çº¿æ€§ä¸¥é‡æ—¶è¡¨ç°æ›´å¥½                      |




#### è´å¶æ–¯è§†è§’

ğŸ“Œ å…ˆéªŒå‡è®¾

æˆ‘ä»¬å°†å›å½’ç³»æ•° $\boldsymbol{\beta}$ è§†ä¸ºä¸€ä¸ªéšæœºå˜é‡ï¼Œèµ‹äºˆå¦‚ä¸‹å…ˆéªŒåˆ†å¸ƒï¼š

$$
\boldsymbol{\beta} \sim \mathcal{N}\left(0, \frac{\sigma^2}{\lambda} \mathbf{I} \right)
$$

è¿™æ˜¯ä¸€ä¸ªé›¶å‡å€¼ã€é«˜æ–¯å…ˆéªŒï¼Œå¯¹æ¯ä¸ªå‚æ•°éƒ½åšäº† $\ell_2$ èŒƒæ•°çš„æƒ©ç½šã€‚

ğŸ¯ ä¼¼ç„¶å‡½æ•°ï¼ˆæ¥è‡ªçº¿æ€§æ¨¡å‹ï¼‰

$$
\mathbf{y} \mid \boldsymbol{\beta} \sim \mathcal{N}(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{I})
$$

ğŸ§  åéªŒåˆ†å¸ƒ

åˆ©ç”¨è´å¶æ–¯å®šç†ï¼ˆé«˜æ–¯ + é«˜æ–¯ â‡’ é«˜æ–¯ï¼‰ï¼Œå¾—åˆ°åéªŒåˆ†å¸ƒä¸ºï¼š

$$
\boldsymbol{\beta} \mid \mathbf{y} \sim \mathcal{N}\left( \underbrace{(\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \mathbf{y}}_{\text{ridge è§£}}, \; \text{åæ–¹å·®çŸ©é˜µ} \right)
$$

å…¶ä¸­åéªŒ **å‡å€¼** æ­£æ˜¯ Ridge å›å½’çš„è§£æè§£ï¼š

$$
\boxed{
\mathbb{E}[\boldsymbol{\beta} \mid \mathbf{y}] = (\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \mathbf{y}
}
$$






#### bias


**Ridge å›å½’æ˜¯æœ‰åä¼°è®¡**

$$
\mathbb{E}[\hat{\boldsymbol{\beta}}^{\text{ridge}}] = Z \boldsymbol{\beta}, \quad Z = (\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \mathbf{X}
$$

* å› ä¸º $Z \neq I$ï¼Œæ‰€ä»¥ ridge ä¼°è®¡æ˜¯ **æœ‰åçš„**
* éšç€æ­£åˆ™åŒ–å‚æ•° $\lambda$ å¢å¤§ï¼Œ**biasÂ² å¢åŠ **
* è¿™æ˜¯åå·®-æ–¹å·®æƒè¡¡çš„ä¸€éƒ¨åˆ†



#### variance

$$
\begin{align*}
\operatorname{Var}\left(\widehat{\boldsymbol{\beta}}^{\text{ ridge}}\right) &= \operatorname{Var}\left(\mathbf{Z}\widehat{\boldsymbol{\beta}}^{\mathrm{ols}}\right) \\
 &= {\color{red}Z}\operatorname{Var}\left(\widehat{\boldsymbol{\beta}}^{\mathrm{ols}}\right) {\color{red}Z^T}\\
&= {\color{red}(\mathbf{X}^\mathsf{T}\mathbf{X}+\lambda\mathbf{I})^{-1}(\mathbf{X}^\mathsf{T}\mathbf{X})}\sigma^2(X^TX)^{-1}{\color{red}(\mathbf{X}^\mathsf{T}\mathbf{X})(\mathbf{X}^\mathsf{T}\mathbf{X}+\lambda\mathbf{I})^{-1}}\\
&=\sigma^{2}\left(\mathbf{X}^{\top} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{\top} \mathbf{X}\left(\mathbf{X}^{\top} \mathbf{X}+\lambda \mathbf{I}\right)^{-1}
\end{align*}
$$

!!! note "æ€»ä½“æ–¹å·®æ˜¯ä¸€ä¸ªå…³äºæ­£åˆ™åŒ–å¼ºåº¦ $\lambda$ çš„**å•è°ƒé€’å‡å‡½æ•°**"

    $$
    \text{Total Variance} = \operatorname{Tr}\left( \operatorname{Var}\left(\hat{\boldsymbol{\beta}}^{\text{ridge}} \right) \right)
    = \sigma^2 \cdot \operatorname{Tr} \left[ \left( X^T X + \lambda I \right)^{-1} X^T X \left( X^T X + \lambda I \right)^{-1} \right]
    $$

    è®° $\mathbf{S} = X^T X$ï¼Œå®ƒæ˜¯å¯¹ç§°æ­£å®šçš„

    æˆ‘ä»¬å¯ä»¥å¯¹å®ƒåš**ç‰¹å¾å€¼åˆ†è§£**ï¼ˆå› ä¸ºå®ƒå¯¹ç§°ï¼‰ï¼š

    $$
    \mathbf{S} = Q \Lambda Q^\top, \quad \text{å…¶ä¸­ } \Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p), \lambda_i > 0
    $$

    äºæ˜¯æ•´ä¸ªæ–¹å·®çŸ©é˜µå¯ä»¥åŒ–ç®€ä¸ºï¼š

    $$
    \operatorname{Var}(\hat{\boldsymbol{\beta}}^{\text{ridge}})
    = \sigma^2 Q \cdot \text{diag} \left( \frac{\lambda_i}{(\lambda_i + \lambda)^2} \right) \cdot Q^\top
    $$

    æ‰€ä»¥å…¶ trace ä¸ºï¼š

    $$
    \text{Total Variance} = \sigma^2 \sum_{i=1}^p \frac{\lambda_i}{(\lambda_i + \lambda)^2}
    $$

    * æ€»ä½“æ–¹å·®æ˜¯ä¸€ä¸ªå…³äºæ­£åˆ™åŒ–å¼ºåº¦ $\lambda$ çš„**å•è°ƒé€’å‡å‡½æ•°**
    * æ¢å¥è¯è¯´ï¼Œ**æ­£åˆ™åŒ–è¶Šå¼º â‡’ ç³»æ•°æ³¢åŠ¨è¶Šå°**


#### è‡ªç”±åº¦


* Ridge å›å½’è™½ç„¶ä¼°è®¡ $\widehat{\boldsymbol{\beta}}^{\text{ridge}} \in \mathbb{R}^p$ï¼Œä½†ç”±äº **Shrinkage**ï¼Œä¸ç­‰ä»·äºä½¿ç”¨æ‰€æœ‰ $p$ ä¸ªå˜é‡çš„å…¨éƒ¨è‡ªç”±åº¦ã€‚
* è‡ªç”±åº¦éšç€ $\lambda$ çš„å˜åŒ–è€Œå˜åŒ–ï¼š

  * $\lambda \to 0$: Ridge é€€åŒ–ä¸º OLSï¼Œ$\text{df} = p$
  * $\lambda \to \infty$: æ‰€æœ‰å‚æ•°è¢«å‹ç¼©åˆ° 0ï¼Œ$\text{df} \to 0$
  * æ‰€ä»¥ï¼š

    $$
    0 \leq \text{df}(\lambda) \leq p
    $$

!!! note "dof"
    $$
    \text{df}(\hat{f}) = \frac{1}{\sigma^2} \sum_{i=1}^n \operatorname{Cov}(\hat{y}_i, y_i) = \frac{1}{\sigma^2} \operatorname{Trace} \left( \operatorname{Cov}(\hat{\mathbf{y}}, \mathbf{y}) \right)
    $$

$$
\widehat{\mathbf{y}} = \mathbf{S} \mathbf{y}, \quad \text{å…¶ä¸­} \quad \mathbf{S} = \mathbf{X}(\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top
$$

$$
\text{df}(\lambda) = \operatorname{Trace}(\mathbf{S}) = \operatorname{Trace} \left( \mathbf{X}(\mathbf{X}^\top \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top \right)
$$

* è‹¥å¯¹ $\mathbf{X}$ åšå¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼š

  $$
  \mathbf{X} = UDV^\top, \quad \text{å…¶ä¸­} \ D = \operatorname{diag}(d_1, \dots, d_p)
  $$

* åˆ™è‡ªç”±åº¦å¯å†™ä¸ºï¼š

$$
\boxed{
\text{df}(\lambda) = \sum_{j=1}^{p} \frac{d_j^2}{d_j^2 + \lambda}
}
$$

* æ¯ä¸ªä¸»æˆåˆ†æ–¹å‘ $j$ çš„è‡ªç”±åº¦è´¡çŒ®æ˜¯ä¸€ä¸ª shrinkage å› å­ï¼š

  $$
  \frac{d_j^2}{d_j^2 + \lambda}
  $$
* æ–¹å·®å°çš„æ–¹å‘ï¼ˆ$d_j$ å°ï¼‰ä¼šè¢«ä¸¥é‡ shrinkï¼Œè‡ªç”±åº¦è´¡çŒ®ä¹Ÿå°‘
* è¿™æ˜¯ Ridge æ¯” OLS æ›´ç¨³å¥ä½†æœ‰åçš„åŸå› 





### elastic
![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/202506200340914.png)

lassoä¸ridgeå¯¹æ¯”
- Ridge is $\ell_{2}$ penalty
- Lasso is $\ell_{1}$ penalty
- Best subset is $\ell_{0}$ penalty
- Bridge penalty is $\ell_{q}$ normal

$q = 4$  
$q = 2$  
$q = 1$  
$q = 0.5$  
$q = 0.1$

$\sum_{j}|\beta_{j}|^{q}$ for given values of $q$.

Elastic-net is a hybrid of $\ell_{1}$ and $\ell_{2}$:

$\lambda_{1}\|\beta\|_{1} + \lambda_{2}\|\beta\|_{2}^{2}$


## LDA

[ç†è§£ä¸»æˆåˆ†åˆ†æï¼ˆ1ï¼‰â€”â€”æœ€å¤§æ–¹å·®æŠ•å½±ä¸æ•°æ®é‡å»º - Fenrier Lab](https://seanwangjs.github.io/2017/12/21/principal-components-analysis.html)

[ç®€å•ç†è§£çº¿æ€§åˆ¤åˆ«åˆ†æ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/66088884)

[LDAçº¿æ€§åˆ¤åˆ«åˆ†æâ€”â€”æŠ•å½±çš„ç–‘é—®è§£ç­”\_ldaæŠ•å½±-CSDNåšå®¢](https://blog.csdn.net/qq_41398808/article/details/100065314)

æœ€å°åŒ–ç±»å†…æ–¹å·®

$$
\begin{align*} &\quad \min\limits_w \left[\sum\limits_{x\in X_0}(w^Tx-w^T\mu_0)^2+\sum\limits_{x\in X_1}(w^Tx-w^T\mu_1)^2\right]\\ &=\min\limits_w w^T \left[\sum\limits_{x\in X_0}(x-\mu_0)(x-\mu_0)^T+\sum\limits_{x\in X_1}(x-\mu_1)(x-\mu_1)^T\right]w \\ &=\min\limits_w w^TS_ww \\ \end{align*}
$$

æœ€å¤§åŒ–ç±»é—´æ–¹å·®


$$
\begin{align*} &\quad \max\limits_w \left[(w^T\mu_0-\frac{w^T\mu_0+w^T\mu_1}{2})^2+(w^T\mu_1-\frac{w^T\mu_0+w^T\mu_1}{2})^2\right]\\ &=\max\limits_w \frac{1}{2}w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw\\ &=\max\limits_w \frac{1}{2}w^TS_bw \\ \end{align*}
$$

å› ä¸ºè‡ªå˜é‡åªæœ‰$w$ï¼Œä¸ä¸€å®šäºŒè€…éƒ½èƒ½åŒæ—¶è¾¾åˆ°æœ€ä¼˜ï¼Œæ‰€ä»¥æ•´åˆåˆ°ä¸€èµ·å–ä¸‹å¼çš„æœ€å¤§å€¼ï¼š

$$
J = \displaystyle \frac{w^TS_bw}{w^TS_ww}
$$

[LDAâ€”â€”çº¿æ€§åˆ¤åˆ«åˆ†æåŸºæœ¬æ¨å¯¼ä¸å®éªŒ-CSDNåšå®¢](https://blog.csdn.net/qq_37189298/article/details/108656649)

[äºŒåˆ†ç±»çº¿æ€§åˆ¤åˆ«åˆ†æï¼Œçœ‹æ‡‚è¿™ç¯‡å°±å¤Ÿäº† - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/488134514)