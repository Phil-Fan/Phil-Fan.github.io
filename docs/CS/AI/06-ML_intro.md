# æœºå™¨å­¦ä¹ å¯¼è®º

[äººå·¥æ™ºèƒ½åŸºç¡€ - é¹¤ç¿”ä¸‡é‡Œçš„ç¬”è®°æœ¬ (tonycrane.cc)](https://note.tonycrane.cc/cs/ai/basic/)

[02ï¼šè´å¶æ–¯å®šç† - å°è§’é¾™çš„å­¦ä¹ è®°å½• (zhang-each.github.io)](https://zhang-each.github.io/My-CS-Notebook/ML/ç»Ÿè®¡æœºå™¨å­¦ä¹ 02ï¼šè´å¶æ–¯å®šç†/)

[å‘½é¢˜é€»è¾‘ - Jerry's Blog (wxxcl.tech)](https://blog.wxxcl.tech/course/aid/çŸ¥è¯†è¡¨è¾¾ä¸æ¨ç†/å‘½é¢˜é€»è¾‘/)

[ç¬”è®°](https://github.com/mura1n/Machine-Learning-in-Practice-Crash-Course-Notes)

## å¯¼è®º

!!! note "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
    è‡ªåŠ¨ä»æ•°æ®ä¸­å­¦ä¹ æé«˜ç³»ç»Ÿèƒ½åŠ›

- supervised learning ï¼šåˆ†ç±»ä»»åŠ¡ï¼ˆç¦»æ•£ï¼‰ï¼Œå›å½’ä»»åŠ¡ï¼ˆè¿ç»­ï¼‰ï¼›å­¦ä¹ ä¸€ä¸ªæ˜ å°„å‡½æ•°$x\rightarrow \mathbf{y}$
- unsupervised learning ï¼šæ‰¾åˆ°æ ‡ç­¾æˆ–è€…æ¨¡å¼ï¼Œèšç±»ã€é™ç»´
- reinforcement learningï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆç›¸å½“äºæ˜¯ç›‘ç£å­¦ä¹ ï¼‰
  ![image-20240611173113321](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/image-20240611173113321.png)

??? note "Fundamental Concepts in Machine Learning"

    === "**Sample, Instance, Example**"
        - Sample, instance, and example refer to the same concept, which is a single data point used for training or testing a machine learning model.

    === "**Feature, Representation, Predictor**"
        - A feature is an attribute or aspect of the data used to describe a data point.
        - Representation refers to the process of converting data into a form that a computer can process, such as a vector or a matrix.
        - A predictor is a model or function used to predict the target variable.
    
    === "**Label, Target, Class, Pattern Class**"
    
        - A label is the true category or value of the data, used in supervised learning.
        - A target is the variable that the model is intended to predict.
        - A pattern class is a category or grouping of data.
        - A class is a group of data points that belong to the same pattern.
    
    === "**Training Data**"
    
        - Training data is the dataset used to train the model.
        - $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$ represent individual data points in the training data, where $x_i$ is the feature and $y_i$ is the label.
    
    === "**Model, Classifier, Regressor**"
    
        - A model is a mathematical structure used to describe data or predict the target.
        - A classifier is a model used for categorizing data, with a discrete output representing the category.
        - A regressor is a model used for regression analysis, with a continuous output representing the numerical value.
    
    === "**Test Data**"
    
        - Test data is the dataset used to evaluate the performance of the model.
        - $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$ represent individual data points in the test data, where $x_i$ is the feature and $y_i$ is the label.
    
    === "**Training Error and Test Error**"
    
        - Training error is the error calculated on the training data.
        - Test error is the error calculated on the test data.
    
        use test error to evaluate the quality of model

<img src="https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/image-20240614191634130.png" alt="image-20240614191634130" style="zoom: 50%;" />

overfitting è¿‡æ‹Ÿåˆ

æ›´å¤æ‚çš„æ¨¡å‹ï¼šæ›´å°çš„training error





### ä»€ä¹ˆæ—¶å€™ä½¿ç”¨æœºå™¨å­¦ä¹ 

**there should be some patterns in the data**

- we know the patterns,but don't know how to use
- ML can discover the pattern themselves



### pipeline

pipelineï¼Œä¸­æ–‡æ„ä¸ºç®¡çº¿ï¼Œæ„ä¹‰ç­‰åŒäºæµæ°´çº¿ã€‚<br>
ä¸€ä¸ªç”ŸåŠ¨çš„å½¢å®¹<br>
Pipelineï¼Œä½  åœŸå‘³ä¸€ç‚¹ ä½ æŠŠå®ƒ ç¿»è¯‘æˆ **ä¸€æ¡é¾™æœåŠ¡**<br>
ä¸“ä¸šä¸€ç‚¹ï¼Œå« å®ƒ **ç»¼åˆè§£å†³æ–¹æ¡ˆ**ï¼Œå°±è¡Œã€‚<br>

![image-20240614191015217](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/image-20240614191015217.png)

- **å®šä¹‰é—®é¢˜**:æ˜¯æœ‰ç›‘ç£è¿˜æ˜¯æ— ç›‘ç£ï¼Ÿæ˜¯åˆ†ç±»è¿˜æ˜¯å›å½’ï¼Ÿ
- æ”¶é›†æ•°æ®ï¼š
- æ•°æ®é¢„å¤„ç† transform data & get featuresï¼šæ‰¾åˆ°xå’Œy
- åˆ›å»ºæ¨¡å‹ï¼ˆå…·ä½“åˆ°æ¨¡å‹ä¹Ÿæœ‰ç›¸åº”çš„Pipeline,æ¯”å¦‚æ¨¡å‹çš„å…·ä½“æ„æˆéƒ¨åˆ†ï¼šæ¯”å¦‚GCN+Attention+MLPçš„æ··åˆæ¨¡å‹ï¼‰
- è¯„ä¼°æ¨¡å‹ç»“æœ
- æ¨¡å‹è°ƒå‚

æ˜¯ä¸€ä¸ª**è¿­ä»£**çš„è¿‡ç¨‹



## metric

### åˆ†ç±»

å…ˆä»‹ç»ä¸€ä¸‹æ··æ·†çŸ©é˜µï¼ˆT/F: é¢„æµ‹æ˜¯å¦æ­£ç¡®ï¼ŒP/Nï¼šé¢„æµ‹æ˜¯æ­£ç±»è¿˜æ˜¯è´Ÿç±»ï¼‰
- TPï¼šé¢„æµ‹ä¸ºæ­£ç±»ï¼Œå®é™…ä¸ºæ­£ç±»ï¼Œé¢„æµ‹æ­£ç¡®ã€‚
- FPï¼šé¢„æµ‹ä¸ºæ­£ç±»ï¼Œå®é™…ä¸ºè´Ÿç±»ï¼Œé¢„æµ‹é”™è¯¯ã€‚
- FNï¼šé¢„æµ‹ä¸ºè´Ÿç±»ï¼Œå®é™…ä¸ºæ­£ç±»ï¼Œé¢„æµ‹é”™è¯¯ã€‚
- TNï¼šé¢„æµ‹ä¸ºè´Ÿç±»ï¼Œå®é™…ä¸ºè´Ÿç±»ï¼Œé¢„æµ‹æ­£ç¡®ã€‚



**å‡†ç¡®ç‡ | Accuracy**

æ­£ç±»å’Œè´Ÿç±»ä¸­é¢„æµ‹æ­£ç¡®çš„æ•°é‡å æ€»æ•°é‡çš„å æ¯”ã€‚$Accuracy=\frac{T P+T N}{T P+F P+F N+T N}$

=== "å­˜åœ¨é—®é¢˜1"
	å‡†ç¡®ç‡ä¸å¯å¯¼ï¼Œæ— æ³•ä½œä¸ºcost functionå»åšè®­ç»ƒï¼Œåªèƒ½ç”¨ä½œè¯„ä¼°ã€‚
=== "å­˜åœ¨é—®é¢˜2"
	æ­£ç±»å’Œè´Ÿç±»é¢„æµ‹æ­£ç¡®çš„é‡è¦æ€§ä¸ä¸€æ ·ï¼Œæ¯”å¦‚å¯¹äºç™Œç—‡æ£€æµ‹æ¥è¯´ï¼Œå¯èƒ½è´Ÿç±»(æ²¡æœ‰æ‚£ç™Œç—‡) é¢„æµ‹æ­£ç¡®çš„æ•°é‡éå¸¸å¤§ï¼Œå°±å¯¼è‡´Accuracyçš„åˆ†å­éå¸¸å¤§ï¼Œå¾—åˆ°çš„Accuracyå°±éå¸¸å¤§ï¼Œä½†æ˜¯å¯èƒ½æ­£ç±»(æ‚£ç™Œç—‡) é¢„æµ‹æ­£ç¡®çš„æ•°é‡éå¸¸å°ï¼Œå°±å¯¼è‡´è™½ç„¶æ¨¡å‹çš„å‡†ç¡®ç‡å¾ˆé«˜ï¼Œä½†æ ¹æœ¬æ£€æµ‹ä¸å‡ºç™Œç—‡ã€‚

è§£å†³é—®é¢˜çš„æ–¹æ¡ˆï¼šé‡‡ç”¨ç²¾ç¡®ç‡æˆ–è€…å¬å›ç‡
- ç²¾ç¡®ç‡ï¼šé¢„æµ‹ä¸ºæ­£ç±»çš„æ ·æœ¬ä¸­é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ã€‚$precision=\frac{T P}{T P+F P}$

- å¬å›ç‡ï¼šå®é™…ä¸ºæ­£ç±»çš„æ ·æœ¬ä¸­ï¼Œè¢«é¢„æµ‹æ­£ç¡®çš„æ­£ç±»çš„æ¯”ä¾‹ã€‚$recall=\frac{T P}{T P+F N}$



### å›å½’

- MSE:
  
$$
MSE(f, \boldsymbol{\theta})=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}, \boldsymbol{\theta}\right)\right)^{2}
$$

- MAE:

$$
MAE(f, \boldsymbol{\theta})=\frac{1}{n}\left|y_{i}-f\left(x_{i}, \boldsymbol{\theta}\right)\right|
$$







memory consumption

platform required for running 

CPU vs GPU

server,workstation



![image-20240614200543795](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/image-20240614200543795.png)

more closed to the realistic world



## è´å¶æ–¯å†³ç­–

### é—®é¢˜ä¸å®šä¹‰

$x$ sample

$y$ state of the nature

$P(y|x)$ given $x$â€‹â€‹,what is the probability of the state of the nature





æ¡ä»¶æ¦‚ç‡ï¼š
$$
P(A|B) = \frac{P(A,B)}{P(B)}
$$




**å…ˆéªŒæ¦‚ç‡ | `prior`**: $P(A)$â€‹the probability A being True. this is the knowledge

åæ˜ äº†æˆ‘ä»¬å…³æ³¨çš„æ ‡ç­¾åœ¨è‡ªç„¶ç•Œä¸­(æ— äººä¸ºå¹²é¢„çš„æƒ…å†µä¸‹)çš„æ•°é‡åˆ†å¸ƒæƒ…å†µï¼ˆåœ¨æŸä¸ªç‰¹å¾ä¸‹ä¹Ÿå¯ä»¥ï¼‰ã€‚ä¾‹å¦‚ï¼šæˆ‘ä»¬å¯ä»¥çœ‹åœ¨ lightness ç‰¹å¾æ¡ä»¶ä¸‹See basså’Œsalmonè¿™ä¸¤ä¸ªæ ‡ç­¾çš„æ•°é‡æƒ…å†µã€‚æ›´ç®€å•çš„æ¥è®²ï¼Œå…ˆéªŒå°±æ˜¯åœ¨æˆ‘ä»¬ä¸çŸ¥æƒ…çš„æƒ…å†µä¸‹çŒœæµ‹çš„æ ‡ç­¾ç§ç±»ã€‚ï¼ˆæˆ‘ä»¬å€¾å‘äºçŒœæµ‹ç‚¸å¼¹ä¸ä¼šçˆ†ç‚¸ï¼Œå› ä¸ºå…¶å…ˆéªŒæ¦‚ç‡å¾ˆå°ï¼‰

- å¦‚æœæ²¡æœ‰å…ˆéªŒæ¦‚ç‡çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šè®¤ä¸ºsalmonå’Œsea bassçš„æ•æ‰æ¦‚ç‡æ˜¯ç›¸ç­‰çš„ã€‚è¿™ç§å‡è®¾å¹¶ä¸é€‚ç”¨äºä»»ä½•æƒ…å†µã€‚$P(y_{1} )=P(y_{2} )$

- å¦‚æœæ˜¯äºŒåˆ†ç±»é—®é¢˜$P(y_{1} )+P(y_{2} )=1$
- å¦‚æœåªæ ¹æ®å…ˆéªŒä¿¡æ¯è¿›è¡Œå†³ç­–
    - å³å¦‚æœy1çš„æ¦‚ç‡å¤§äºy2åˆ™è®¤ä¸ºæ˜¯y1ï¼Œå¦åˆ™å°±æ˜¯y2.
    - å…ˆéªŒæ¦‚ç‡ä¸ä¸€å®šå‡†ç¡®ï¼Œæ²¡æœ‰ç”¨åˆ°äº‹ç‰©æœ¬èº«çš„featureï¼Œçº¯åœ¨çŒœæµ‹ã€‚

**ä¼¼ç„¶æ€§ | `likelihood`**: $P(B|A)$â€‹the probability of B being true,given A is true

!!! note "æ¦‚ç‡å’Œä¼¼ç„¶"
	**æ¦‚ç‡ï¼š** åœ¨ä¸€ä»¶äº‹çš„ç»“æœæœªçŸ¥çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡äº‹ä»¶è‡ªèº«çš„æ€§è´¨ä¼°è®¡äº‹ä»¶å„ä¸ªç»“æœçš„å¯èƒ½æ€§çš„å¤§å°ï¼Œå°±æ˜¯äº‹ä»¶å„ä¸ªç»“æœå‘ç”Ÿçš„æ¦‚ç‡ã€‚ï¼ˆæŠ›ç¡¬å¸ï¼šç¡¬å¸æœ‰ä¸¤é¢ï¼Œæ‰€ä»¥ä¸¤é¢åˆ†åˆ«æœä¸Šçš„æ¦‚ç‡éƒ½æ˜¯ç™¾åˆ†ä¹‹äº”åï¼Œæ¦‚ç‡åªæœ‰åœ¨äº‹ä»¶å‘ç”Ÿå‰æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºå½“ç¡¬å¸æŠ›å‡ºåï¼Œç»“æœå°±å·²ç»ç¡®å®šäº†ã€‚ï¼‰<br>
	**ä¼¼ç„¶ï¼š** åŸºäºäº‹ä»¶å·²ç»ç¡®å®šçš„ç»“æœæ¥æ¨æµ‹äº§ç”Ÿè¿™ä¸ªç»“æœçš„å¯èƒ½ç¯å¢ƒï¼ˆç¯å¢ƒä¸­çš„æŸäº›å‚æ•°ï¼‰ã€‚ï¼ˆæŠ›ç¡¬å¸ï¼šç›´æ¥æŠ›ç¡¬å¸10000æ¬¡ï¼Œå…¶ä¸­8000æ¬¡æ­£é¢æœä¸Šï¼Œ2000æ¬¡åé¢æœä¸Šï¼Œæˆ‘ä»¬ä¼šè®¤ä¸ºç¡¬å¸çš„æ„é€ æ¯”è¾ƒç‰¹æ®Šï¼Œè¿›è€Œæ¨æµ‹è¯¥ç¡¬å¸çš„å…·ä½“å‚æ•°ï¼‰



**åéªŒæ¦‚ç‡ | `posterior`**: $P(A|B)$â€‹

è´å¶æ–¯å®šç†
$$
P(A|B) = P(A)\frac{P(B|A)}{P(B)}
$$

$$
[åéªŒæ¦‚ç‡] = [å…ˆéªŒæ¦‚ç‡]\times[åéªŒæ¦‚ç‡]
$$

[ã€å®˜æ–¹åŒè¯­ã€‘è´å¶æ–¯å®šç†ï¼Œä½¿æ¦‚ç‡è®ºç›´è§‰åŒ–_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1R7411a76r)

> rationality is not about knowing facts, it's about recognizing which facts are relevant
>
> æ–°è¯æ®ä¸èƒ½ç›´æ¥å†³å®šçœ‹æ³•ï¼Œè€Œæ˜¯åº”è¯¥æ›´æ–°ä½ çš„è§‚ç‚¹








æœ€å¤§ä¼¼ç„¶æ¦‚ç‡å†³ç­– MLE

$$
P(y_i|x) = \frac{P(x|y_i) P(y_i)}{P(x)}
$$


**Bayes Decision Rule**

Decide $y_1$, if $P(y_1|x)> P(y_2|x)$,otherwise $y_2$

> å› ä¸º$P(x)$ä¸ç±»åˆ«$y_i$æ— å…³ï¼Œæ‰€ä»¥å¯ä»¥çœç•¥

**æœ€å°åŒ–é”™è¯¯æ¦‚ç‡**

æœ€å°é”™è¯¯å…¶å®å’Œæœ€å¤§åéªŒæ¦‚ç‡æ˜¯ç­‰ä»·çš„ï¼Œå› ä¸ºæœ€å°é”™è¯¯å°±æ˜¯æœ€å¤§åŒ–åéªŒæ¦‚ç‡ã€‚ï¼ˆä½¿ç”¨äºŒåˆ†ç±»æ¥ç†è§£ï¼‰

[ä¾‹å­](https://blog.csdn.net/Harry_Jack/article/details/111242672)



### Bayesian Risk | è´å¶æ–¯é£é™©

> å¹¶ä¸æ˜¯æ‰€æœ‰çš„é”™è¯¯ä»£ä»·éƒ½æ˜¯ç›¸åŒçš„

å¦‚æœå°†ä¸¤ä¸ªç±»åˆ«äº’ç›¸è¯†åˆ«é”™è¯¯çš„é£é™©ç›¸å½“çš„å‰æä¸‹ï¼Œç±»åˆ«åªéœ€è¦æ¯”è¾ƒä¸¤è€…çš„åéªŒæ¦‚ç‡å³å¯ï¼Œå°±ä¸ä¹‹å‰ç»˜åˆ¶ç›´æ–¹å›¾ç±»ä¼¼ã€‚å¦‚æœä¸¤è€…ä¸ç›¸ç­‰ï¼Œåˆ™éœ€è¦ä¾æ®é£é™©å‡½æ•°æ¯”è¾ƒå¤§å°è¿›è¡Œç±»åˆ«åˆ¤å®šã€‚

$$
\begin{align*}
E_{ij} = E(\hat{y_i}|y_j)\\
R(\hat{y_1}|x) = E_{11} P(y_1|x)+E_{12}P(y_2|x) = E_{12}P(y_2|x)\\
R(\hat{y_2}|x) = E_{21} P(y_1|x)+E_{22}P(y_2|x) = E_{21}P(y_1|x)\\
\end{align*}
$$

å†³ç­–æ–¹æ³•ï¼šé€‰é£é™©æœ€å°çš„ï¼Œdecide $y_1$ if $R(\hat{y_1}|x) < R(\hat{y_2}|x)$

- äºŒåˆ†ç±»ä¸­ï¼šå½“ likelihood ratio è¶…è¿‡æŸä¸ªä¸xæ— å…³çš„é˜ˆå€¼æ—¶å€™ï¼Œå°±åšå†³ç­–

$$
\begin{align*}
E_{21}P(x|y_1)P(y_1) > E_{12}P(x|y_2)P(y_2)\\
\frac{P(x|y_1)}{P(x|y_2)} > \frac{E_{12}P(y_2)}{E_{21}P(y_1)}\\
\end{align*}
$$

> å¦‚æœ$E_{12} = E_{21}$â€‹ï¼Œæœ€å°åŒ–é£é™©å‡½æ•°Riskå°±æ˜¯æœ€å¤§åŒ–åéªŒæ¦‚ç‡P(yi|x)
>
> ä½ ä¼šè§‰å¾—è¦è®©ç­‰å¼å·¦è¾¹çš„Rè¶Šå°ï¼Œç­‰å¼å³è¾¹çš„Pä¸æ˜¯ä¹Ÿè¯¥è¶Šå°å—ï¼Ÿä¸ºä»€ä¹ˆè¦æœ€å¤§åŒ–å‘¢ï¼Ÿä»”ç»†çœ‹ä¸‹è¡¨ä½ å°±ä¼šå‘ç°Rä¸­çš„$\hat{y}$çš„ä¸‹æ ‡å’ŒPä¸­çš„$y$çš„ä¸‹æ ‡æ˜¯ä¸ä¸€æ ·çš„ï¼Œä½ è¦$\hat{y_1}$çš„Riskå€¼è¶Šå°ï¼Œ$\hat{y_2}$å¯¹åº”çš„På€¼å°±è¶Šå°ï¼Œ$y_1$å¯¹åº”çš„På€¼å°±åº”è¯¥è¶Šå¤§ï¼Œæ‰€ä»¥ç¡®å®æ˜¯given xæ¡ä»¶ä¸‹$y_1$çš„åéªŒæ¦‚ç‡è¶Šå¤§

- å¤šåˆ†ç±»çš„æƒ…å†µï¼šseek a decision rule that minimizes the probability of error or maximizes the accuracy;

$$
\begin{align*}
    E(\hat{y_i}|y_j) = \begin{cases}
    0 & if\ i=j\\
    1 & if\ i\ne j
    \end{cases}
\end{align*}
$$



### å›é¡¾

è´å¶æ–¯çš„æ¡†æ¶
- çŸ¥é“å…ˆéªŒæ¦‚ç‡P(yi)ï¼ŒçŸ¥é“ä¼¼ç„¶P(x|yi)ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªæœ€ä¼˜çš„åˆ†ç±»å™¨ã€‚
- ç°å®ç”Ÿæ´»ä¸­ï¼Œå¾ˆéš¾è·å–åˆ°å‡†ç¡®çš„ä¼¼ç„¶çš„ä¿¡æ¯ï¼ˆç‰¹å¾ç»´åº¦å¤ªé«˜æˆ–è€…ç‰¹å¾å¹¶ä¸å……åˆ†ï¼‰ã€‚
- å¸¸ç”¨çš„åšæ³•ï¼šåˆ©ç”¨è®­ç»ƒæ•°æ®å»ä¼°è®¡å‡ºå…ˆéªŒæ¦‚ç‡å’Œä¼¼ç„¶ï¼Œå†å»åšè´å¶æ–¯å†³ç­–ã€‚

classifier assigns a feature when:

$$
g_i(x) > g_j(x), \forall j \neq i
$$



![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240630201905.png)
ä¸­é—´çš„èŠ‚ç‚¹æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨

- åˆ¤åˆ«å‡½æ•°æ˜¯å…ˆéªŒ
- åˆ¤åˆ«å‡½æ•°æ˜¯åéªŒï¼šè´å¶æ–¯å†³ç­–
- ä¼¼ç„¶å‡½æ•°ï¼šæå¤§ä¼¼ç„¶ä¼°è®¡
- æœŸæœ›é£é™©æœ€å°åŒ–ï¼šè´å¶æ–¯é£é™©





**Decision Regions and surfaces**

- learning çš„è¿‡ç¨‹å…¶å®å°±æ˜¯å°†feature space åˆ†æˆä¸åŒçš„ decision regions
- ç°å®ç”Ÿæ´»ä¸­ï¼Œç”±äºåªèƒ½ä»æœ‰é™çš„æ ·æœ¬ä¸­å­¦ä¹ ï¼Œæ‰€ä»¥åªèƒ½å¾—åˆ°likelihoodå’Œå…ˆéªŒçš„ä¼°è®¡å€¼

<img src="https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/image-20240630210634705.png" alt="image-20240630210634705" style="zoom:33%;" />

**ç¦»æ•£å½¢å¼æå¤§ä¼¼ç„¶ä¼°è®¡**

- å…ˆéªŒæ¦‚ç‡ï¼šå°†é¢‘ç‡ä¼°è®¡ä¸ºæ¦‚ç‡$P\left(y_{k}\right)=\frac{N_{y_{k}}}{N}$

- ä¼¼ç„¶ï¼šåœ¨ç±»åˆ«ä¸º$y_k$çš„æ ·æœ¬ä¸­ç‰¹å¾ä¸º$x_i$æ ·æœ¬çš„å æ¯”ã€‚$P\left(x_{i} \mid y_{k}\right)=\frac{\left|x_{i k}\right|}{N_{y_{k}}}$
  

**è¿ç»­å½¢å¼æå¤§ä¼¼ç„¶ä¼°è®¡**

- discretize: the range into binsï¼Œå¯¹äºä½“é‡æ¥è¯´ï¼Œå¯ä»¥50-100kgï¼Œ100-150kgï¼Œ150-200kgã€‚å†å°†æ•°æ®æ®µç¦»æ•£æˆä¸åŒçš„ç±»åˆ«å³å¯ã€‚
- two-way split: æš´åŠ›åˆ†ä¸ºä¸¤ä¸ªæ®µï¼Œè®¾ç½®ä¸€ä¸ªä¸­é—´å€¼ï¼Œå°äºä¸­é—´å€¼çš„è®¾ä¸ºä¸€ç±»ï¼Œå¤§äºä¸­é—´å€¼çš„è®¾ä¸ºå¦ä¸€ç±»ã€‚
- Probability Density estimation: assume attribute follows a normal distribution or some other distribution



### æœ´ç´ è´å¶æ–¯

curse of dimensionality: feature space becomes sparse 

å‡è®¾ç‰¹å¾ä¹‹é—´æ˜¯ç‹¬ç«‹çš„

$$
\begin{align*}
    P(y|x_1,\dots,x_p) \propto  P(x_1,\dots,x_p | y) P(y)= P(y)P(x_1|y)P(x_2|y)\dots P(x_p|y)\\
\end{align*}
$$

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/img/20240630201709.png)

å¥½å¤„ï¼š
- robust to isolated noise points
- can handle missing values
- robust to irrelevant features
- å¯è§£é‡Šæ€§éå¸¸å¥½
- è®¡ç®—é‡éå¸¸å°
- åœ¨å®è·µä¸­è¡¨ç°å¥½çš„åŸå› ï¼šæ•°æ®ä¸­çš„ç‰¹å¾ä¹‹é—´çš„å…³ç³»å¾ˆå¼±ï¼›æˆ–è€…å°±ç®—ç­‰å¼ä¸¤ä¾§ä¸ç›¸ç­‰ï¼Œå…¶å¤§å°çš„ç›¸å¯¹å…³ç³»ä»ç„¶æ˜¯ä¸€è‡´çš„

é—®é¢˜ï¼š
- ä¸Šè¿°å‡è®¾åœ¨å®é™…ä¸­å¯èƒ½å¹¶ä¸æˆç«‹
- float point underflow
- 0 probability
  - smoothing: $P(x_i| y_k) = \frac{|x_{ik}|+1}{N_{y_k}+K}$,Kæ˜¯labelçš„æ•°é‡

!!! note "æ”¹æˆå–$\ln$çš„åŸå› "
    æœ€é‡è¦çš„ä¸æ˜¯å€¼æœ¬èº«ï¼Œè€Œæ˜¯ç›¸å¯¹å¤§å°
    ä¸ºäº†é¿å…å‘ä¸Šæº¢å‡ºï¼Œå’Œå‘ä¸‹æº¢å‡ºï¼ˆæµ®ç‚¹æ•°é—®é¢˜ï¼‰ï¼Œtake a log





## generalization æ³›åŒ–





## å­¦ä¹ èµ„æº

[Machine Learning in Practice Crash Course | Jinming Hu (conanhujinming.github.io)](https://conanhujinming.github.io/post/ml_in_practice_crash_course/)

[å®ç”¨çš„æœºå™¨å­¦ä¹  ç¬¬ä¸€è¯¾ æœºå™¨å­¦ä¹ å¯¼è®º 2024summer_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1Gw4m1i7ys/?spm_id_from=333.788.recommend_more_video.0&vd_source=8b7a5460b512357b2cf80ce1cefc69f5)

[æœºå™¨å­¦ä¹ 2023-10-19ç¬¬6-8èŠ‚ (zju.edu.cn)](https://classroom.zju.edu.cn/livingpage?course_id=53449&sub_id=915451&tenant_code=112)

èµµæ´²è€å¸ˆ







[æœ‰ç›‘ç£çš„æœºå™¨å­¦ä¹ ï¼šå›å½’ä¸åˆ†ç±» | Coursera](https://www.coursera.org/learn/machine-learning?action=enroll)

[CS229å´æ©è¾¾æœºå™¨å­¦ä¹ ](https://www.bilibili.com/video/BV16J411t71N)

[CS229: Machine Learning (stanford.edu)](https://cs229.stanford.edu/)



æ·±åº¦å­¦ä¹ 

[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/)ï¼šdeep learning for CV

[å›¾çµç­ã€Šæœºå™¨å­¦ä¹ ã€‹è¯¾ç¨‹æ€»ç»“ - CC98è®ºå›](https://www.cc98.org/topic/5599897)

æˆ‘åœ¨å¿ƒçµå­¦MLç³»åˆ—doge

[å†æ¬¡å…¥é—¨deep learningä»¥åŠä¸€äº›å›å¿†ï¼ˆæ›´æ–°ç¬¬äºŒéƒ¨åˆ†ï¼‰ - CC98è®ºå›](https://www.cc98.org/topic/5207160)

[å†æ¬¡å…¥é—¨deep learningï¼Œè¿™æ¬¡ç›´æ¥ä¸Šé‡ç‚¹ï¼ˆå®Œç»“ç¯‡ï¼‰ - CC98è®ºå›](https://www.cc98.org/topic/5208795)



å­”é™¢ğŸ•ğŸ¦è¯¾

[äººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹ å¤ä¹ èµ„æ–™ - CC98è®ºå›](https://www.cc98.org/topic/5518130)

[2022-2023ç§‹å†¬äººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹ çº¿ä¸Šè€ƒè¯•å›å¿†å·ï¼ˆå’Œå¤§ä½¬ä»¬å‘é‡äº†ï¼‰ - CC98è®ºå›](https://www.cc98.org/topic/5508899)

[äººå·¥æ™ºèƒ½ä¸æœºå™¨å­¦ä¹ å›å¿†å· - CC98è®ºå›](https://www.cc98.org/topic/5234359)





æœ¬äººå†œå­¦åšå£«ï¼Œç§‘ç ”æ¥è§¦çš„æœºå™¨å­¦ä¹ ï¼Œä¹‹å‰æœ‰è®¡ç®—æœºçš„å¯¼å¸ˆé¢†å…¥é—¨äº†ã€‚ä¸ªäººç›®å‰é‡åˆ°æœ€å¥½çš„æ•™ç¨‹æ˜¯å´æ©è¾¾çš„è§†é¢‘è¯¾ç¨‹ï¼Œå› ä¸ºä»–å……åˆ†è€ƒè™‘åˆ°äº†å­¦ç”Ÿçš„æ°´å¹³ï¼ŒæŠŠéœ€è¦çš„æ•°å­¦çŸ¥è¯†ä¹Ÿè®²äº†ï¼Œå…ˆçœ‹äº†å´æ©è¾¾æ—©æœŸçš„æœºå™¨å­¦ä¹ ï¼ˆåå‘ä¼ æ’­çš„é‚£èŠ‚è®²çš„ä¸æ˜¯å¾ˆå¥½ï¼‰ï¼Œç„¶åè¿‘ä¸¤å¹´çš„æ·±åº¦å­¦ä¹ ï¼Œåœ¨çœ‹äº†bç«™åŒ—å¤§çš„tensorflowç¬”è®°è¯¾ç¨‹ï¼Œè§‰å¾—è‡³å°‘çŸ¥é“è¯¥æ€ä¹ˆåšæœºå™¨å­¦ä¹ ï¼ˆåŒ…æ‹¬æ·±åº¦å­¦ä¹ ï¼‰äº†ã€‚ ä¸è¿‡ä½œä¸ºä¸€ä¸ªéè®¡ç®—æœºä¸“ä¸šçš„å­¦ç”Ÿï¼Œä¸ªäººè§‰å¾—æ‰€æœ‰çš„æ•™ç¨‹éƒ½å¿½è§†äº†ä¸€ä¸ªæœ€åŸºç¡€ä½†æ˜¯ä¹Ÿæ˜¯æœ€é‡è¦çš„ä¸œè¥¿â€”â€”**ç‰¹å¾å·¥ç¨‹**ï¼ŒæŒ‡çš„ä¸æ˜¯ç‰¹å¾é€‰æ‹©ï¼ˆæ— ç›‘ç£å­¦ä¹ çš„é™ç»´ï¼‰ï¼Œè€Œæ˜¯ç‰¹å¾è¡¨å¾ï¼ˆfeature representï¼‰ï¼Œæ·±åº¦å­¦ä¹ é‡Œé¢å«embeddingï¼ˆè‡ªå·±çœ‹äº†åŠŸèƒ½åç†è§£çš„ï¼‰ï¼Œå°±æ˜¯æˆ‘ä»¬åº”è¯¥æ€æ ·å»è¡¨å¾é—®é¢˜ï¼Œå°†é—®é¢˜çš„ä¿¡æ¯è¡¨ç¤ºä¸ºæ•°æ®ç»™è®¡ç®—æœºè¿›è¡Œå­¦ä¹ ã€‚ä¹‹å‰çœ‹äº†ä»€ä¹ˆæœ‰ç›‘ç£å­¦ä¹ å•Šï¼Œæ— ç›‘ç£å­¦ä¹ å•Šï¼Œå¯¹ç‰¹å¾å°±æ˜¯å‘Šè¯‰ä½ æ ·æœ¬æˆ–å‘é‡ç©ºé—´ï¼Œå®Œå…¨ä¸çŸ¥é“æœºå™¨å­¦ä¹ å»åšä»€ä¹ˆã€‚åªåˆ°æœ‰ä¸ªè€å¸ˆè®©æˆ‘åœ¨åšäº†ç‰¹å¾æå–ï¼Œç„¶åé™ç»´ï¼Œç„¶ååˆ†ç±»æˆ–é¢„æµ‹çš„æ—¶å€™æˆ‘æ‰æ˜ç™½æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªä»€ä¹ˆæ ·çš„è¿‡ç¨‹ã€‚



æˆ‘æ˜¯å…¥é—¨çœ‹çš„å’±ä»¬å­¦æ ¡çš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ï¼Œå¯¹æœºå™¨å­¦ä¹ å¤§æ¦‚æœ‰ä¸ªäº†è§£ï¼Œæ²¡å¤ªå…³å¿ƒæ•°å­¦ã€‚ è¯´å®è¯è¿™äº›ç®—æ³•ï¼ˆmlé‡Œä¸åŒ…å«dlçš„é‚£äº›ï¼‰æˆ‘ç§‘ç ”ä¸Šç”¨åˆ°çš„æ¯”è¾ƒå°‘ï¼Œåæ¥éšç€ç§‘ç ”çš„æ·±å…¥ä¼šå»æ€è€ƒè¿™äº›ç®—æ³•åé¢çš„æ•°å­¦åŸç†ï¼Œå°±å»å‚è€ƒæèˆªçš„æœºå™¨å­¦ä¹ ï¼Œè¥¿ç“œä¹¦ã€‚æ›´åŠ é«˜å±‹å»ºç“´ä¸€ç‚¹çš„æ•™æå°±æ˜¯PRMLäº†ã€‚ æˆ‘æ¯”è¾ƒæ¨èUCBçš„CS188ï¼Œä»æ•´ä¸ªäººå·¥æ™ºèƒ½çš„è§’åº¦è®²é—®é¢˜ï¼Œæœºå™¨å­¦ä¹ æ˜¯å…¶ä¸­çš„ä¸€ä¸ªéƒ¨åˆ†ã€‚ç¼–ç¨‹é¡¹ç›®æœ‰è¶£è¿è´¯ã€‚ç”¨çš„æ•™æä¹Ÿæ˜¯ç»å…¸ï¼Œä¸€äº›æ€æƒ³ç°åœ¨ä¹Ÿä¸è¿‡æ—¶ã€‚



pipeline

å®Œæˆå®ç”¨æœºå™¨å­¦ä¹ çš„æ‰€æœ‰ä½œä¸š

å®Œæˆèµµæ´²è€å¸ˆçš„æ‰€æœ‰ä½œä¸š+å¤§ä½œä¸š

