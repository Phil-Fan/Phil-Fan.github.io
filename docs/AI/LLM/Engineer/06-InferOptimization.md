---
status: new
comments: True
--- 
# 06 | Infer Optimization


!!! note "æ­£åœ¨æ–½å·¥ä¸­ğŸ‘·.. "




## Key-value cache
- Key-value cache

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1006179366&bvid=BV1kx4y1x7bu&cid=1616577437&p=1&autoplay=0&high_quality=1&danmaku=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500px"></iframe>

### åŸç†
åœ¨ Decoderé˜¶æ®µï¼Œä½¿ç”¨Auto Regressiveæœºåˆ¶

$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$

ç”±äºæœ‰Maskæœºåˆ¶ï¼Œæ¯æ¬¡æœ‰æ–°çš„tokenåŠ å…¥çš„æ—¶å€™ï¼Œåªéœ€è¦åš$Q_{new}$å’Œ$K_{old}$çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œè€Œä¸ç”¨é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—ã€‚

æ‰€ä»¥ï¼Œæˆ‘ä»¬åªéœ€è¦ä¿å­˜ $K_{old}$ å’Œ $V_{old}$ (å› ä¸ºåªç”¨åˆ°äº†KV)ï¼Œå°±å¯ä»¥å®ç°é«˜æ•ˆçš„å¢é‡ç”Ÿæˆã€‚

![image-20250812230838959](assets/06-InferOptimization.assets/image-20250812230838959.webp)


å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒKV ç¼“å­˜çš„å¤§å°é€šå¸¸å’Œæ¨¡å‹æœ¬èº«å¤§å°æ˜¯åŒä¸€çº§åˆ«ï¼Œä¹Ÿæ˜¯ä¸€ç§ç©ºé—´æ¢æ—¶é—´çš„ç­–ç•¥

```mermaid
pie
    title Memory Usage of 13B LLM on A100-40GB
    "Parameters" : 65
    "KV Cache" : 30
    "Others" : 5
```



## Paged Attention
### ä¸ºä»€ä¹ˆéœ€è¦

!!! note "æ“ä½œç³»ç»Ÿ"
    æ“ä½œç³»ç»Ÿéœ€è¦ç»™è¿›ç¨‹é¢„å…ˆåˆ†é…å†…å­˜å—

    æ¯ä¸ªé¡µ4K
    
    ![image-20250813000031813](assets/06-InferOptimization.assets/image-20250813000031813.webp)



### åŸç†

- ä¸é¢„åˆ†é…ï¼ŒæŒ‰éœ€è°ƒç”¨
  
- æŒ‰å—Blockåˆ†é…å†…å­˜ï¼Œç¢ç‰‡æ›´å°
    ![image-20250813000142399](assets/06-InferOptimization.assets/image-20250813000142399.webp)

- è™šæ‹Ÿå†…å­˜ï¼šé€»è¾‘å†…å­˜æ˜¯è¿ç»­çš„ï¼Œé€šè¿‡æ˜ å°„è¡¨é“¾æ¥åˆ°ç‰©ç†å†…å­˜ï¼ˆå®é™…åˆ†é…ä¸è¿ç»­ï¼‰ï¼›æ–¹ä¾¿è°ƒç”¨

![image-20250813000330631](assets/06-InferOptimization.assets/image-20250813000330631.webp)


## Share KV Cache

**copy on write**æœºåˆ¶ï¼šå¼•ç”¨å¤§äº1çš„æ—¶å€™ï¼Œä¸èƒ½ç›´æ¥å†™å…¥ï¼Œå¿…é¡»æ‹·è´ä¸€ä»½ï¼Œå†å†™å…¥

![image-20250813001125290](assets/06-InferOptimization.assets/image-20250813001125290.webp)

è¿˜å¯ä»¥ä¼˜åŒ– beam-search

![image-20250813001154648](assets/06-InferOptimization.assets/image-20250813001154648.webp)




## Flash Attention
[[2205.14135] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135)

[![citation](https://img.shields.io/badge/dynamic/json?label=citation&style=social&logo=googlescholar&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F87c5b281fa43e6f27191b20a8dd694eda1126336%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/FlashAttention%3A-Fast-and-Memory-Efficient-Exact-Dao-Fu/87c5b281fa43e6f27191b20a8dd694eda1126336)[![GitHub Repo stars](https://img.shields.io/github/stars/Dao-AILab/flash-attention)](https://github.com/Dao-AILab/FlashAttention) 

- Fast
- Memory-Efficient
- Exact 

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1706279420&bvid=BV1UT421k7rA&cid=1625114922&p=1&autoplay=0&high_quality=1&danmaku=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500px"></iframe>


### ä¸ºä»€ä¹ˆéœ€è¦

> Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length.


SRAMè¯»å–å¿«ï¼ŒHBMè¯»å–æ…¢

![image-20250813002617202](assets/06-InferOptimization.assets/image-20250813002617202.webp)


- Compute-bound: ï¼ˆæ•°æ®ç­‰ç®—åŠ›ï¼‰
  - å¤§çš„çŸ©é˜µä¹˜æ³•ï¼Œå¤šchannelå·ç§¯
- IO-bound:ï¼ˆç®—åŠ›ç­‰æ•°æ®ï¼‰
  - æŒ‰ä½æ“ä½œï¼šReluï¼ŒDropout
  - è§„çº¦æ“ä½œï¼šsumã€softmax

ä¸€èˆ¬ä½¿ç”¨fusionèåˆæ“ä½œï¼Œç®—ç»“æœæ—¶å€™åªè¯»å–ä¸€æ¬¡HBM



### åŸå§‹ Attentionçš„å®ç°

<img src="assets/06-InferOptimization.assets/image-20250813143319708.webp" alt="image-20250813143319708" style="zoom: 50%;" />

çŸ©é˜µ $Q$, $K$, $V \in \mathbb{R}^{N\times d}$ å­˜å‚¨åœ¨ HBMã€‚ï¼ˆ$N$ æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$ æ˜¯ç»´åº¦ï¼‰

1. ä» `HBM` åŠ è½½ $Q$, $K$ åˆ° `SRAM`
2. è®¡ç®—å‡º $S = QK^T$
3. å°† $S$ å†™åˆ° `HBM`
4. å°† $S$ åŠ è½½åˆ° `SRAM`
5. è®¡ç®— $P = softmax(S)$
6. å°† $P$ å†™å‡ºåˆ° `HBM`
7. ä» `HBM` åŠ è½½ $P$ å’Œ $V$ åˆ° `SRAM`
8. è®¡ç®— $O = PV$
9. æŠŠ $O$ å†™å‡ºåˆ° `HBM`
10. è¿”å› $O$



<img src="assets/06-InferOptimization.assets/image-20250813142327972.webp" alt="image-20250813142327972" style="zoom: 25%;" />

### tiling **softmax**


å‡å°‘IOé‡

è®©Attentionçš„æ‰€æœ‰è®¡ç®—éƒ½ç¬¦åˆ**åŠ æ³•ç»“åˆå¾‹**

- é€šè¿‡åˆ†å—è®¡ç®—ï¼Œèåˆå¤šä¸ªæ“ä½œï¼Œå‡å°‘ä¸­é—´ç»“æœç¼“å­˜
- åå‘ä¼ æ’­ç­‰æ—¶å€™ï¼Œé‡æ–°è®¡ç®—ç»“æœ

![image-20250813002815635](assets/06-InferOptimization.assets/image-20250813002815635.webp)

!!! note "**softmax**ç²¾åº¦é—®é¢˜"
    $e$çš„æŒ‡æ•°é¡¹å¯èƒ½è¶…è¿‡ç²¾åº¦ï¼Œæ¯”å¦‚65536

    ä½¿ç”¨æŒ‡æ•°é¡¹å¯èƒ½ä¼šçˆ†ç²¾ï¼Œæ‰€ä»¥ä½¿ç”¨safe_softmax


    $$
    \begin{aligned}
    m &= \max(x_1,...,x_N) \\
    \mathrm{softmax}(\{x_1,...,x_N\}) &= \{\frac{e^{x_i}/e^m}{\sum_{j=1}^Ne^{x_j}/e^m}\}_{i=1}^N = \{\frac{e^{x_i-m}}{\sum_{j=1}^Ne^{x_j-m}}\}_{i=1}^N
    \end{aligned}
    $$






å³å¦‚æœè®¡ç®—äº†å·¦ä¾§çš„softmaxï¼Œå³ä¾§çš„softmaxå¦‚ä½•è®¡ç®—æ•´ä½“çš„

KV åœ¨å¤–å¾ªç¯ Qåœ¨å†…å¾ªç¯

$$
\begin{aligned}
& x = \lfloor x_1, \ldots, x_N, \ldots, x_{2N} \rfloor \\
& m(x) := \max(x) \quad \text{å…¶ä¸­} \quad x = [x_1, \ldots, x_N] \\
& p(x) := [e^{x_1 - m(x)}, \ldots, e^{x_N - m(x)}] \\
& l(x) := \sum_i p(x)_i \\
& softmax(x) := \frac{p(x)}{l(x)}
\end{aligned}
$$

å¯¹äºæ•´ä½“æ¥è®²

$$
\begin{aligned}
& x^1 = [x_1, \ldots, x_N] \quad x^2 = [x_{N+1}, \ldots, x_{2N}] \\
& m(x) := \max(m(x^1), m(x^2)) \\
& p(x) := [e^{m(x^1) - m(x)} p(x^1), e^{m(x^2) - m(x)} p(x^2)] \\
& l(x) := e^{m(x^1) - m(x)} l(x^1) + e^{m(x^2) - m(x)} l(x^2) \\
& softmax(x) := \frac{p(x)}{l(x)}
\end{aligned}
$$


```python title="å»æ‰æœ€åä¸€ä¸ª embedding ç»´åº¦ï¼ˆ4ï¼‰"
Q.shape[:-1] = (1, 1, 6)
```


`[..., None]` ä¼šåœ¨æœ€å**å¢åŠ ä¸€ä¸ªç»´åº¦**ï¼Œç›¸å½“äºï¼š

```python
(1, 1, 6) â†’ (1, 1, 6, 1)
```

æ‰€ä»¥ï¼š

* **`l.shape` = `(1, 1, Q_LEN, 1)`**
* **`m.shape` = `(1, 1, Q_LEN, 1)`**


??? note "ä¸ºä»€ä¹ˆæ˜¯ `(1, 1, Q_LEN, 1)` è€Œä¸æ˜¯ `(1, 1, Q_LEN)`ï¼Ÿ"
    
    **ä½œç”¨ï¼šæ–¹ä¾¿å¹¿æ’­è¿ç®—**
    
    åœ¨æ³¨æ„åŠ›è®¡ç®—æ—¶ï¼Œ`l` å’Œ `m` æ˜¯**é’ˆå¯¹æ¯ä¸ª query ä½ç½®**å­˜å‚¨çš„ï¼š
    
    * `m` â†’ è¿™ä¸ªä½ç½®çš„å½“å‰æœ€å¤§ logitï¼ˆæ•°å€¼ç¨³å®š softmax ç”¨ï¼‰
    * `l` â†’ è¿™ä¸ªä½ç½®çš„ softmax åˆ†æ¯ï¼ˆ`sum(exp(...))`ï¼‰
    
    åœ¨åç»­æ›´æ–°ä¸­ï¼Œä¼šç”¨åˆ°åƒï¼š
    
    ```python
    torch.exp(m_block_ij - mi_new)
    ```
    
    è¿™é‡Œçš„ `m_block_ij` å½¢çŠ¶é€šå¸¸æ˜¯ `(1, 1, block_size, 1)`ï¼Œ
    å¦‚æœ `l` å’Œ `m` ä¹Ÿæœ‰æœ€åä¸€ä¸ª `1` ç»´åº¦ï¼Œå°±å¯ä»¥**æ— é¢å¤– reshape ç›´æ¥å¹¿æ’­**ã€‚


    **å¦å¤–ä¸€ä¸ªåŸå› ï¼šä¸ V å¯¹é½**
    
    æ³¨æ„åŠ›è¾“å‡ºæ˜¯ï¼š
    
    ```python
    output = sum(softmax(QK^T) * V)
    ```
    
    `V` çš„å½¢çŠ¶æ˜¯ `(1, 1, KV_LEN, dim)`ï¼Œ
    è€Œ `l`ã€`m` åªå­˜æ¯ä¸ª query çš„ä¸€ä¸ªæ ‡é‡ï¼Œæ‰€ä»¥æœ€åä¸€ç»´æ˜¯ `1`ï¼Œ
    è¿™æ ·åœ¨è®¡ç®—æ—¶æ—¢èƒ½å’Œ `(1, 1, Q_LEN, dim)` å¹¿æ’­ï¼Œä¹Ÿèƒ½å’Œ `(1, 1, Q_LEN, 1)` å¯¹é½ã€‚


éœ€è¦é¢å¤–å­˜å‚¨

![image-20250813003529043](assets/06-InferOptimization.assets/image-20250813003529043.webp)

![image-20250813153517030](assets/06-InferOptimization.assets/image-20250813153517030.webp)

![image-20250813142311190](assets/06-InferOptimization.assets/image-20250813142311190.webp)


### åå‘ä¼ æ’­ recomputation

å‰å‘çš„æ—¶å€™ï¼Œä¼šä¿å­˜softmaxç»Ÿè®¡å€¼ï¼Œ$m$å’Œ$l$

![image-20250813003543657](assets/06-InferOptimization.assets/image-20250813003543657.webp)





## StreamLLM


åœ¨nvidia-smiä¸­å¯ä»¥çœ‹åˆ°æ‰€æœ‰GPUçš„åˆ©ç”¨ç‡ä¼šç›´æ¥å†²åˆ°100%ï¼Œç›´åˆ°è¿™ä¸ªè¶…å¡çš„è¯·æ±‚å…¨éƒ¨ç”Ÿæˆå®Œï¼Œæ‰ä¼šæ¢å¤æ­£å¸¸ã€‚è¿™ä¸å°±æ˜¯å…¸å‹çš„ä¼˜å…ˆprefillæš‚åœdecodeä¹ˆï¼Œè§£å†³åŠæ³•å°±æ˜¯chunked prefill sizeå•Šï¼Œdeepseekéƒ½å‘Šè¯‰ä½ äº†ã€‚
