---
status: new
comments: True
--- 
# BERT

!!! note "ä¸»è¦ä»‹ç»ä¸€ä¸‹encoder-onlyæŠ€æœ¯è·¯çº¿çš„æ¨¡å‹ï¼Œä»¥BERTä¸ºä¸»"


Pre-training of Deep Bidirectional Transformers for Language Understanding

<iframe src="https://arxiv.org/pdf/1810.04805" width="100%" height="500px"></iframe>


## æ ¸å¿ƒæ€æƒ³


åœ¨ä¸€ä¸ªå¾ˆå¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒå¥½ä¸€ä¸ªå¾ˆå®½å¾ˆæ·±çš„æ¨¡å‹ï¼Œåœ¨å¾ˆå¤šå°çš„é—®é¢˜ä¸Šå¯ä»¥é€šè¿‡å¾®è°ƒæ¥å…¨é¢æå‡å°æ•°æ®çš„æ€§èƒ½ï¼ˆåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸç”¨äº†å¾ˆå¤šå¹´ï¼‰ï¼Œæ¨¡å‹è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ï¼ˆå¾ˆç®€å•å¾ˆæš´åŠ›ï¼‰ã€‚

åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œåœ¨'å¤§é‡çš„'æ²¡æœ‰æ ‡ç­¾çš„æ•°æ®é›†ä¸Šåšè®­ç»ƒæ¯”åœ¨'å°‘é‡çš„'æœ‰æ ‡ç­¾çš„æ•°æ®é›†ä¸Šåšè®­ç»ƒæ•ˆæœä¼šæ›´å¥½

## èƒŒæ™¯

BERTçš„åå­—æ¥è‡ªäº"Bidirectional Encoder Representations for Transformer"ã€‚

å½“æ—¶ï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨åœ¨ä¸‹æ¸¸ä»»åŠ¡é€šå¸¸æœ‰ä¸¤ç§åšæ³•

- åŸºäºç‰¹å¾çš„ï¼šELMo åŒå‘çš„ä¿¡æ¯ï¼Œç½‘ç»œæ¶æ„æ¯”è¾ƒè€ï¼Œç”¨çš„æ˜¯RNN
- åŸºäºå¾®è°ƒçš„ï¼šGPTï¼š transformeræƒ³æ³•ï¼Œä½†æ˜¯æ˜¯å•å‘çš„ï¼Œæ²¡æœ‰åŒå‘çš„ä¿¡æ¯

è¿™ä¸¤ä¸ªé€”å¾„éƒ½æ˜¯ä½¿ç”¨ç›¸åŒçš„ç›®æ ‡å‡½æ•°ï¼Œéƒ½æ˜¯ç”¨ä¸€ä¸ªå•å‘çš„è¯­è¨€æ¨¡å‹ï¼ˆè¯´ä¸€å¥è¯ï¼Œé¢„æµ‹ä¸‹ä¸€å¥è¯ï¼‰ã€‚

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/web_pic/AI__LLM__Models__assets__04-BERT.assets__image-20250707172023699.webp)

è€ŒBERTæ˜¯Transformerï¼Œæ‰€ä»¥å¯¹äºä¸‹æ¸¸ä»»åŠ¡ä¸éœ€è¦åšé‚£ä¹ˆå¤šçš„è°ƒæ•´ã€‚

ç°åœ¨çš„æŠ€æœ¯çš„é—®é¢˜æ˜¯è¯­è¨€æ¨¡å‹æ˜¯å•å‘çš„ï¼Œå¦‚æœè¦åšå¥å­å±‚é¢çš„åˆ†æçš„è¯ï¼Œå¦‚æœä»å·¦çœ‹åˆ°å³å¤–ï¼Œè¿˜å¯ä»¥ä»å³çœ‹åˆ°å·¦ï¼Œæ˜¯å¯ä»¥æå‡æ€§èƒ½çš„ã€‚

åœ¨å¤§é‡çš„æ²¡æœ‰æ ‡æ³¨çš„æ•°æ®ä¸Šï¼Œé¢„è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œç„¶åè¿ç§»åˆ°å…¶ä»–ä»»åŠ¡ä¸Šçš„æ•ˆæœï¼Œæœ‰å¯èƒ½æ¯”åœ¨æœ‰æ ‡æ³¨çš„æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹æ•ˆæœè¿˜è¦å¥½ã€‚


**é‚£ä¹ˆå¦‚ä½•è®©è¯­è¨€æ¨¡å‹å˜æˆåŒå‘çš„å‘¢ï¼Ÿ**

å¸¦æ©ç çš„è¯­è¨€æ¨¡å‹éšæœºæŒ–å»ä¸€äº›å­—å…ƒï¼Œç„¶åå…è®¸çœ‹å·¦å³ä¿¡æ¯ï¼Œå¹¶è¿›å»å¡«ç©ºï¼Œä¹Ÿå°±æ˜¯å®Œå½¢å¡«ç©ºã€‚å¦ä¸€ä¸ªæ˜¯ä¸‹ä¸€å¥é¢„æµ‹ï¼Œéšæœºé‡‡æ ·ä¸¤ä¸ªå¥å­ï¼Œåˆ¤æ–­è¿™ä¸¤ä¸ªå¥å­æ˜¯å¦æ˜¯ç›¸é‚»çš„ã€‚è¿™ä¸¤ä¸ªä»»åŠ¡è§£å†³äº†â€œåŒå‘â€çš„é—®é¢˜ã€‚





Bertæœ‰ä¸¤ä¸ªæ­¥éª¤ï¼špre-training + fine-tuning


**Pre-trainingé¢„è®­ç»ƒï¼š**

åœ¨é¢„è®­ç»ƒé˜¶æ®µï¼ŒBERTç”¨å¤§é‡çš„æ— ç›‘ç£æ–‡æœ¬é€šè¿‡è‡ªç›‘ç£è®­ç»ƒçš„æ–¹å¼(é€šè¿‡ä½¿ç”¨å—å®Œå½¢å¡«ç©ºä»»åŠ¡å¯å‘çš„Masked Language Modelé¢„è®­ç»ƒç›®æ ‡)è®­ç»ƒï¼ŒæŠŠæ–‡æœ¬ä¸­åŒ…å«çš„è¯­è¨€çŸ¥è¯†ï¼ˆåŒ…æ‹¬ï¼šè¯æ³•ã€è¯­æ³•ã€è¯­ä¹‰ç­‰ç‰¹å¾ï¼‰ä»¥å‚æ•°çš„å½¢å¼ç¼–ç Transformer-encoder layerä¸­ã€‚é¢„è®­ç»ƒæ¨¡å‹å­¦ä¹ åˆ°çš„æ˜¯æ–‡æœ¬çš„é€šç”¨çŸ¥è¯†ï¼Œä¸ä¾æ‰˜äºæŸä¸€é¡¹NLPä»»åŠ¡ï¼›

å¯¹æ¯”ELMoï¼Œè™½ç„¶éƒ½æ˜¯â€œåŒå‘â€,ä½†ç›®æ ‡å‡½æ•°å…¶å®æ˜¯ä¸åŒçš„ã€‚ELMoæ˜¯åˆ†åˆ«ä»¥$P(w_i|w_1,\ldots w_{i-1})$ å’Œ$P(w_i|w_{i+1},\ldots w_n)$ä½œä¸ºç›®æ ‡å‡½æ•°ï¼Œç‹¬ç«‹è®­ç»ƒå¤„ä¸¤ä¸ªrepresentationç„¶åæ‹¼æ¥ï¼Œè€ŒBERTåˆ™æ˜¯ä»¥$P(w_i|w_1,\ldots,w_{i-1},w_{i+1},\ldots,w_n)$ä½œä¸ºç›®æ ‡å‡½æ•°è®­ç»ƒLMã€‚









**Fine-Tuningå¾®è°ƒï¼š**

NLP é—®é¢˜è¢«è¯æ˜åŒå›¾åƒä¸€æ ·ï¼Œå¯ä»¥é€šè¿‡ finetune åœ¨å‚ç›´é¢†åŸŸå–å¾—æ•ˆæœçš„æå‡ã€‚Bert æ¨¡å‹æœ¬èº«æå…¶ä¾èµ–è®¡ç®—èµ„æºï¼Œä» 0 è®­ç»ƒå¯¹å¤§å¤šæ•°å¼€å‘è€…éƒ½æ˜¯éš¾ä»¥æƒ³è±¡çš„äº‹ã€‚åœ¨èŠ‚çœèµ„æºé¿å…é‡å¤´å¼€å§‹è®­ç»ƒçš„åŒæ—¶ï¼Œä¸ºæ›´å¥½çš„æ‹Ÿåˆå‚ç›´é¢†åŸŸçš„è¯­æ–™ï¼Œæˆ‘ä»¬æœ‰äº† finetune çš„åŠ¨æœºã€‚

## model

åœ¨å¾®è°ƒé˜¶æ®µï¼ŒBERTé¦–å…ˆä½¿ç”¨é¢„è®­ç»ƒçš„å‚æ•°åˆå§‹åŒ–æ¨¡å‹ï¼Œæ‰€æœ‰å‚æ•°éƒ½ä½¿ç”¨ä¸‹æ¸¸ä»»åŠ¡çš„æ ‡ç­¾æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œæ¯ä¸ªä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡éƒ½æœ‰å•ç‹¬çš„å¾®è°ƒæ¨¡å‹

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/web_pic/AI__LLM__Models__assets__04-BERT.assets__image-20250707171751445.webp)

- L: the number of layers
- H: the hidden size
- A: the number of self-attention heads


BERT_{BASE} (L=12, H=768, A=12, Total Parameters=110M) (ä¸GPTè§„æ¨¡ç›¸åŒ).æ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°ä¸»è¦æ¥è‡ªåµŒå…¥å±‚å’ŒTransformerå—ã€‚

BERT_{LARGE} (L=24, H=1024, A=16, Total Parameters=340M).


!!! note "è®¡ç®—å‚æ•°"

    åµŒå…¥å±‚å°±æ˜¯ä¸€ä¸ªçŸ©é˜µï¼ŒåµŒå…¥å±‚çš„è¾“å…¥æ˜¯å­—å…¸çš„å¤§å°ï¼ˆè¿™é‡Œæ˜¯30Kï¼‰ï¼Œè¾“å‡ºæ˜¯éšå±‚å•å…ƒçš„å¤§å°ï¼Œéšå±‚å•å…ƒæ˜¯Transformerçš„è¾“å…¥ã€‚å¤´çš„ä¸ªæ•°$A \times 64 = H$ã€‚

    åœ¨Transformerä¸­:
    - Kã€Qã€Véƒ½æ˜¯$H \times H$çš„çŸ©é˜µ
    - è¾“å‡ºçŸ©é˜µçš„å¤§å°ä¹Ÿæ˜¯$H \times H$
    - åé¢çš„MLPå±‚æ˜¯ä¸¤ä¸ª$H^2 \times 8$çš„çŸ©é˜µ
    - ä¸€å…±æœ‰Lå±‚çš„Transformer

    æ‰€ä»¥ä¸€å…±æœ‰$30K \times H + L \times H^2 \times 12 = 110M$å¤§å°çš„å‚æ•°ã€‚


## ä»»åŠ¡

### Masked Language Model
è¿™ç§ä»»åŠ¡çš„ç›®çš„æ˜¯é¢„æµ‹å¥å­ä¸­éƒ¨åˆ†å•è¯çš„åŸå§‹å½¢å¼ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒBERTæ¨¡å‹ä¼šéšæœºé€‰æ‹©ä¸€äº›å•è¯å¹¶ç”¨`[MASK]`æ ‡è®°æ›¿æ¢å®ƒä»¬ã€‚æ¨¡å‹çš„ä»»åŠ¡æ˜¯é¢„æµ‹è¢«æ›¿æ¢çš„å•è¯çš„åŸå§‹å½¢å¼ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ä½¿æ¨¡å‹åœ¨ç†è§£å¥å­è¯­ä¹‰çš„åŒæ—¶å­¦ä¹ åˆ°è¯è¯­ä¹‹é—´çš„å…³ç³»ã€‚ 

> ä¸å®Œå½¢å¡«ç©ºç±»ä¼¼


```mermaid
pie title BERT çš„æ©ç æ›¿æ¢ç­–ç•¥
    "15%*80%: æ›¿æ¢ä¸º[MASK]" : 12
    "15%*10%: æ›¿æ¢ä¸ºéšæœºè¯" : 1.5
    "15%*10%: ä¿æŒåŸè¯" : 1.5
    "85%: æœªè¢«é€‰ä¸­çš„è¯" : 85
```

- 80% çš„æƒ…å†µï¼šå°†è¯æ›¿æ¢ä¸º `[MASK]` æ ‡è®°
  ä¾‹å¦‚ï¼šmy dog is hairy â†’ my dog is `[MASK]`

- 10% çš„æƒ…å†µï¼šå°†è¯æ›¿æ¢ä¸ºéšæœºè¯ï¼Œä¾‹å¦‚ï¼šmy dog is hairy â†’ my dog is apple
  ä¾‹å¦‚ï¼šmy dog is hairy â†’ my dog is **apple**

- 10% çš„æƒ…å†µï¼šä¿æŒè¯ä¸å˜
  ä¾‹å¦‚ï¼šmy dog is hairy â†’ my dog is **hairy**
  è¿™æ ·åšçš„ç›®çš„æ˜¯ä½¿è¡¨ç¤ºåå‘äºå®é™…è§‚å¯Ÿåˆ°çš„è¯ã€‚

ä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ªæ¯”ä¾‹ï¼Œä½œè€…æ²¡æœ‰è¯´


!!! note "ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨`[MASK]`æ›¿æ¢æ‰€æœ‰è¯ï¼Ÿ"
    å¯¹äºä¸€ä¸ªè¾“å…¥çš„è¯­è¨€åºåˆ—ï¼Œ15%çš„è¯å…ƒä¼šè¢«æ›¿æ¢æˆæ©ç ã€‚ä½†æ˜¯é—®é¢˜åœ¨äºåœ¨é¢„è®­ç»ƒçš„æ—¶å€™ä¼šæœ‰15%çš„è¯ç”¨`[MASK]`æ›¿æ¢ï¼Œä½†æ˜¯å¾®è°ƒçš„æ—¶å€™æ˜¯æ²¡æœ‰`[MASK]`çš„ï¼Œæ‰€ä»¥ä¸¤ä¸ªé˜¶æ®µçœ‹åˆ°çš„æ•°æ®ä¸ä¸€æ ·ã€‚

    å¦‚æœå¥å­ä¸­çš„æŸä¸ªToken100%éƒ½ä¼šè¢«maskæ‰ï¼Œé‚£ä¹ˆåœ¨fine-tuningçš„æ—¶å€™æ¨¡å‹å°±ä¼šæœ‰ä¸€äº›æ²¡æœ‰è§è¿‡çš„å•è¯ã€‚åŠ å…¥éšæœºTokençš„åŸå› æ˜¯å› ä¸ºTransformerè¦ä¿æŒå¯¹æ¯ä¸ªè¾“å…¥tokençš„åˆ†å¸ƒå¼è¡¨å¾ï¼Œå¦åˆ™æ¨¡å‹å°±ä¼šè®°ä½è¿™ä¸ª[mask]æ˜¯token â€™hairyâ€˜ã€‚è‡³äºå•è¯å¸¦æ¥çš„è´Ÿé¢å½±å“ï¼Œå› ä¸ºä¸€ä¸ªå•è¯è¢«éšæœºæ›¿æ¢æ‰çš„æ¦‚ç‡åªæœ‰15%*10% =1.5%ï¼Œè¿™ä¸ªè´Ÿé¢å½±å“å…¶å®æ˜¯å¯ä»¥å¿½ç•¥ä¸è®¡çš„ã€‚


??? note "ä¸ºä»€ä¹ˆæœ‰ä¸‰ç§æ–¹å¼"
    ç¬¬ä¸€ç‚¹ä¸­çš„æ›¿æ¢ï¼Œæ˜¯ Masked LM ä¸­çš„ä¸»è¦éƒ¨åˆ†ï¼Œå¯ä»¥åœ¨ä¸æ³„éœ² label çš„æƒ…å†µä¸‹èåˆçœŸåŒå‘è¯­ä¹‰ä¿¡æ¯ï¼›

    ç¬¬äºŒç‚¹çš„éšæœºæ›¿æ¢ï¼Œå› ä¸ºéœ€è¦åœ¨æœ€åä¸€å±‚éšæœºæ›¿æ¢çš„è¿™ä¸ª token ä½å»é¢„æµ‹å®ƒçœŸå®çš„è¯ï¼Œè€Œæ¨¡å‹å¹¶ä¸çŸ¥é“è¿™ä¸ª token ä½æ˜¯è¢«éšæœºæ›¿æ¢çš„ï¼Œå°±è¿«ä½¿æ¨¡å‹å°½é‡åœ¨æ¯ä¸€ä¸ªè¯ä¸Šéƒ½å­¦ä¹ åˆ°ä¸€ä¸ª å…¨å±€è¯­å¢ƒä¸‹çš„è¡¨å¾ï¼Œå› è€Œä¹Ÿèƒ½å¤Ÿè®© BERT è·å¾—æ›´å¥½çš„è¯­å¢ƒç›¸å…³çš„è¯å‘é‡ï¼ˆè¿™æ­£æ˜¯è§£å†³ä¸€è¯å¤šä¹‰çš„æœ€é‡è¦ç‰¹æ€§ï¼‰ï¼›
    
    ç¬¬ä¸‰ç‚¹çš„ä¿æŒä¸å˜ï¼Œä¹Ÿå°±æ˜¯çœŸçš„æœ‰ 10% çš„æƒ…å†µä¸‹æ˜¯ æ³„å¯†çš„ï¼ˆå æ‰€æœ‰è¯çš„æ¯”ä¾‹ä¸º15% * 10% = 1.5%ï¼‰ï¼Œè¿™æ ·èƒ½å¤Ÿç»™æ¨¡å‹ä¸€å®šçš„ bias ï¼Œç›¸å½“äºæ˜¯é¢å¤–çš„å¥–åŠ±ï¼Œå°†æ¨¡å‹å¯¹äºè¯çš„è¡¨å¾èƒ½å¤Ÿæ‹‰å‘è¯çš„çœŸå®è¡¨å¾ï¼ˆæ­¤æ—¶è¾“å…¥å±‚æ˜¯å¾…é¢„æµ‹è¯çš„çœŸå® embeddingï¼Œåœ¨è¾“å‡ºå±‚ä¸­çš„è¯¥è¯ä½ç½®å¾—åˆ°çš„embeddingï¼Œæ˜¯ç»è¿‡å±‚å±‚ Self-attention åå¾—åˆ°çš„ï¼Œè¿™éƒ¨åˆ† embedding é‡Œå¤šå°‘ä¾ç„¶ä¿ç•™æœ‰éƒ¨åˆ†è¾“å…¥ embedding çš„ä¿¡æ¯ï¼Œè€Œè¿™éƒ¨åˆ†å°±æ˜¯é€šè¿‡è¾“å…¥ä¸€å®šæ¯”ä¾‹çš„çœŸå®è¯æ‰€å¸¦æ¥çš„é¢å¤–å¥–åŠ±ï¼Œæœ€ç»ˆä¼šä½¿å¾—æ¨¡å‹çš„è¾“å‡ºå‘é‡æœè¾“å…¥å±‚çš„çœŸå® embedding æœ‰ä¸€ä¸ªåç§»ï¼‰ã€‚
    
    è€Œå¦‚æœå…¨ç”¨ mask çš„è¯ï¼Œæ¨¡å‹åªéœ€è¦ä¿è¯è¾“å‡ºå±‚çš„åˆ†ç±»å‡†ç¡®ï¼Œå¯¹äºè¾“å‡ºå±‚çš„å‘é‡è¡¨å¾å¹¶ä¸å…³å¿ƒï¼Œå› æ­¤ å¯èƒ½ä¼šå¯¼è‡´æœ€ç»ˆçš„å‘é‡è¾“å‡ºæ•ˆæœå¹¶ä¸å¥½ã€‚


### Next Sentence Prediction(NSP)

è¿™ç§ä»»åŠ¡çš„ç›®çš„æ˜¯é¢„æµ‹ä¸€ä¸ªå¥å­æ˜¯å¦æ˜¯å¦ä¸€ä¸ªå¥å­çš„ä¸‹ä¸€å¥ã€‚å¦‚æœæ˜¯çš„è¯è¾“å‡º`IsNext`ï¼Œå¦åˆ™è¾“å‡º`NotNext`ã€‚

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒBERTæ¨¡å‹ä¼šä»ä¸¤ä¸ªå¥å­ä¸­é€‰æ‹©ä¸€ä¸ªéšæœºçš„å¥å­å¯¹ï¼Œå¹¶æ ¹æ®æ˜¯å¦æ˜¯ä¸‹ä¸€å¥æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ä½¿æ¨¡å‹æ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡ä¹‹é—´çš„å…³ç³»ã€‚



å…¶ä¸­50%ä¿ç•™æŠ½å–çš„ä¸¤å¥è¯ï¼Œå®ƒä»¬ç¬¦åˆIsNextå…³ç³»(æ­£ä¾‹)ï¼Œå¦å¤–50%çš„ç¬¬äºŒå¥è¯æ˜¯éšæœºä»é¢„æ–™ä¸­æå–çš„ï¼Œå®ƒä»¬çš„å…³ç³»æ˜¯NotNext(è´Ÿä¾‹)ã€‚


```
Input= [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]
Label= IsNext

Input= [CLS] the man [MASK] to the store [SEP]penguin [MASK] are flight ##less birds [SEP]
Label= NotNext
```


**æ³¨æ„ï¼šä½œè€…ç‰¹æ„è¯´äº†è¯­æ–™çš„é€‰å–å¾ˆå…³é”®ï¼Œè¦é€‰ç”¨document-levelçš„è€Œä¸æ˜¯sentence-levelçš„ï¼Œè¿™æ ·å¯ä»¥å…·å¤‡æŠ½è±¡è¿ç»­é•¿åºåˆ—ç‰¹å¾çš„èƒ½åŠ›ã€‚**


## æŠ€æœ¯






### è¯è¡¨
ä½¿ç”¨WordPieceè¯åµŒå…¥çš„æƒ³æ³•æ˜¯å¦‚æœä¸€ä¸ªè¯åœ¨æ•´ä¸ªé‡Œé¢å‡ºç°æ¦‚ç‡ä¸å¤§çš„è¯ï¼Œåº”è¯¥åˆ‡å¼€çœ‹å­åºåˆ—ï¼Œè¿™ä¸ªå­åºåˆ—å¯èƒ½æ˜¯è¯æ ¹ï¼Œå‡ºç°æ¬¡æ•°å¾ˆå¤§ã€‚
- word pieceï¼š æŠŠè¯æ‹†åˆ†æˆå­è¯ï¼Œç„¶åè¿›è¡Œè®­ç»ƒ 30000 token çš„è¯è¡¨

å¦‚æœæ˜¯ä¸­æ–‡ï¼Œè¿˜æœ‰åšwordpieceçš„å¿…è¦å—ï¼Ÿ

ä½¿ç”¨jiebaåˆ†è¯ã€‚


```python title="tokenizer"
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
example_text = 'I will watch Memento tonight'
bert_input = tokenizer(example_text,padding='max_length', 
                       max_length = 10, 
                       truncation=True,
                       return_tensors="pt")
# ------- bert_input ------
print(bert_input['input_ids'])
print(bert_input['token_type_ids'])
print(bert_input['attention_mask'])
```
```text title="bert_input"
tensor([[  101,   146,  1209,  2824,  2508, 26173,  3568,   102,     0,     0]])
tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])
```

```python title="decode"
example_text = tokenizer.decode(bert_input.input_ids[0])
print(example_text)
```
```text title="decode"
[CLS] I will watch Memento tonight [SEP] [PAD] [PAD]
```


### embedding

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/web_pic/AI__LLM__Models__assets__04-BERT.assets__image-20250707171808632.webp)

BERTçš„ä¸‰ä¸ªEmbeddingä¸ºä»€ä¹ˆç›´æ¥ç›¸åŠ  

- tokenåµŒå…¥ï¼ˆtoken embeddingï¼‰ï¼šæœ‰ä¸¤ç‰¹æ®Šçš„tokenï¼Œä¸€ä¸ªæ˜¯[CLS]ï¼ˆç”¨äºåˆ†ç±»ï¼‰ï¼Œä¸€ä¸ªæ˜¯[SEP]ï¼ˆç”¨äºåˆ†éš”å¥å­ï¼‰
- ä½ç½®åµŒå…¥ï¼ˆposition embeddingï¼‰ï¼šå› Transformer-encoderlayeræ— æ³•æ•è·æ–‡æœ¬çš„ä½ç½®ä¿¡æ¯ï¼Œï¼ˆâ€œä½ æ¬ æˆ‘500ä¸‡â€ å’Œ â€œæˆ‘æ¬ ä½ 500ä¸‡â€çš„æ„Ÿè§‰è‚¯å®šä¸ä¸€æ ·ï¼‰ï¼Œæ‰€ä»¥éœ€è¦ä½ç½®åµŒå…¥ã€‚**Position Embeddingså’Œä¹‹å‰æ–‡ç« ä¸­çš„Transformerä¸ä¸€æ ·ï¼Œä¸æ˜¯ä¸‰è§’å‡½æ•°è€Œæ˜¯å­¦ä¹ å‡ºæ¥çš„**
- æ®µè½åµŒå…¥ï¼ˆsegment embeddingï¼‰ï¼šåœ¨NSPä»»åŠ¡ä¸­ï¼Œç”¨äºåŒºåˆ†ç¬¬ä¸€ä¸ªå¥å­å’Œç¬¬äºŒä¸ªå¥å­ã€‚ç¬¬ä¸€å¥æ‰€æœ‰çš„tokenï¼ˆåŒ…æ‹¬clså’Œç´§éšç¬¬ä¸€å¥çš„sepï¼‰çš„segment embeddingçš„å€¼ä¸º0ï¼Œç¬¬äºŒå¥æ‰€æœ‰çš„tokenï¼ˆåŒ…æ‹¬ç´§éšç¬¬äºŒå¥çš„sepï¼‰çš„segment embeddingçš„å€¼ä¸º1

BERTæ¨¡å‹éœ€è¦åŒæ—¶è€ƒè™‘è¾“å…¥çš„tokenã€ä½ç½®å’Œæ®µè½ä¿¡æ¯ã€‚è¿™ä¸‰ä¸ªåµŒå…¥åˆ†åˆ«å¯¹åº”äº†è¾“å…¥tokenåœ¨å¥å­ä¸­çš„ä½ç½®å’Œæ‰€å¤„çš„æ®µè½ï¼Œä»¥åŠè¾“å…¥tokenæœ¬èº«çš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œåœ¨å°†å®ƒä»¬ç›¸åŠ ä¹‹åï¼ŒBERTæ¨¡å‹å¯ä»¥åŒæ—¶è·å¾—è¿™äº›ä¿¡æ¯

### mask

åœ¨è®¡ç®—MLMé¢„è®­ç»ƒä»»åŠ¡çš„æŸå¤±å‡½æ•°çš„æ—¶å€™ï¼Œå‚ä¸è®¡ç®—çš„Tokensæœ‰å“ªäº›ï¼Ÿæ˜¯å…¨éƒ¨çš„15%çš„è¯æ±‡è¿˜æ˜¯15%è¯æ±‡ä¸­çœŸæ­£è¢«Maskçš„é‚£äº›tokensï¼Ÿ 

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒBERTæ¨¡å‹ä¼šéšæœºé€‰æ‹©ä¸€äº›tokenså¹¶ç”¨â€œã€MASKã€‘â€æ ‡è®°æ›¿æ¢å®ƒä»¬ã€‚è¿™äº›è¢«æ ‡è®°çš„tokensåªå æ‰€æœ‰tokensçš„ä¸€å°éƒ¨åˆ†ï¼Œé€šå¸¸æ˜¯15%ã€‚ç„¶åï¼Œæ¨¡å‹çš„ä»»åŠ¡æ˜¯é¢„æµ‹è¢«æ›¿æ¢çš„tokensçš„åŸå§‹å½¢å¼ã€‚åœ¨è®¡ç®—æŸå¤±å‡½æ•°æ—¶ï¼Œåªæœ‰çœŸæ­£è¢«æ›¿æ¢æˆäº†â€œã€MASKã€‘â€æ ‡è®°çš„tokensä¼šè¢«ç”¨æ¥è®¡ç®—æŸå¤±å€¼ï¼Œè€Œæ²¡æœ‰è¢«æ ‡è®°çš„tokensåˆ™ä¸ä¼šå‚ä¸æŸå¤±å‡½æ•°çš„è®¡ç®—ã€‚


åœ¨å®ç°æŸå¤±å‡½æ•°çš„æ—¶å€™ï¼Œæ€ä¹ˆç¡®ä¿æ²¡æœ‰è¢« Mask çš„å‡½æ•°ä¸å‚ä¸åˆ°æŸå¤±è®¡ç®—ä¸­å»ï¼› 

åœ¨å®ç°æŸå¤±å‡½æ•°æ—¶ï¼Œéœ€è¦ä½¿ç”¨ä¸€ä¸ªæ©ç ï¼ˆmaskï¼‰å‘é‡æ¥æŒ‡ç¤ºå“ªäº›tokensæ˜¯è¢«Maskçš„ï¼Œå“ªäº›tokensæ˜¯æ²¡æœ‰è¢«Maskçš„ã€‚å…·ä½“æ¥è¯´ï¼Œæ©ç å‘é‡ä¸­è¢«Maskçš„tokensçš„ä½ç½®ä¸º1ï¼Œæ²¡æœ‰è¢«Maskçš„tokensçš„ä½ç½®ä¸º0ã€‚åœ¨è®¡ç®—æŸå¤±å‡½æ•°æ—¶ï¼Œå¯ä»¥å°†æ©ç å‘é‡ä¸é¢„æµ‹çš„tokenså’Œå®é™…çš„tokensç›¸ä¹˜ï¼Œè¿™æ ·å°±å¯ä»¥å°†æ²¡æœ‰è¢«Maskçš„tokensçš„æŸå¤±å€¼ç½®ä¸º0ï¼Œåªè®¡ç®—è¢«Maskçš„tokensçš„æŸå¤±å€¼ã€‚ä»¥PyTorchä¸ºä¾‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æ¥å®ç°æ©ç çš„åŠŸèƒ½ï¼š

```python
loss_mask = torch.tensor(mask, dtype=torch.float32) # maskæ˜¯æ©ç å‘é‡
predictions = model(tokens) # tokensæ˜¯è¾“å…¥çš„tokens
loss = loss_function(predictions, labels)
masked_loss = torch.sum(loss * loss_mask) / torch.sum(loss_mask)
```

### fine-tuning

åˆ†ç±»ï¼šå¯¹äºsequence-levelçš„åˆ†ç±»ä»»åŠ¡ï¼ŒBERTç›´æ¥å–ç¬¬ä¸€ä¸ª[CLS]tokençš„final hidden state
$C\in\mathfrak{R}^H$ ,åŠ ä¸€å±‚æƒé‡$W\in\mathfrak{R}^{K\times H}$åsoftmaxé¢„æµ‹label proba:

$$
P=softmax(CW^T)
$$



![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/web_pic/AI__LLM__Models__assets__04-BERT.assets__v2-859d9bfd39a39eccf9c9eb14c49402bd_1440w.webp)

> å›¾ç‰‡æ¥æº[ä¿å§†çº§æ•™ç¨‹ï¼Œç”¨PyTorchå’ŒBERTè¿›è¡Œæ–‡æœ¬åˆ†ç±» - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/524487313)

![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/web_pic/AI__LLM__Models__assets__04-BERT.assets__image-20250707211059233.webp)


å¯ä»¥è°ƒæ•´çš„å‚æ•°å’Œå–å€¼èŒƒå›´æœ‰ï¼š

- Batch size: 16, 32
- Learning rate (Adam): 5e-5, 3e-5, 2e-5
- Number of epochs: 3, 4


å› ä¸ºå¤§éƒ¨åˆ†å‚æ•°éƒ½å’Œé¢„è®­ç»ƒæ—¶ä¸€æ ·ï¼Œç²¾è°ƒä¼šå¿«ä¸€äº›ï¼Œæ‰€ä»¥ä½œè€…æ¨èå¤šè¯•ä¸€äº›å‚æ•°ã€‚


## ä»£ç 
è°·æ­Œå¼€æºäº†å¦‚ä¸‹è¡¨æ‰€ç¤ºçš„ä¸åŒå¤§å°çš„æ¨¡å‹ï¼Œå¯ä»¥åœ¨[google-research/bert: TensorFlow code and pre-trained models for BERT](https://github.com/google-research/bert)

|          |         H=128         |         H=256         |          H=512          |         H=768          |
| -------- | :-------------------: | :-------------------: | :---------------------: | :--------------------: |
| **L=2**  | **2/128 (BERT-Tiny)** |         2/256         |          2/512          |         2/768          |
| **L=4**  |         4/128         | **4/256 (BERT-Mini)** | **4/512 (BERT-Small)**  |         4/768          |
| **L=6**  |         6/128         |         6/256         |          6/512          |         6/768          |
| **L=8**  |         8/128         |         8/256         | **8/512 (BERT-Medium)** |         8/768          |
| **L=10** |        10/128         |        10/256         |         10/512          |         10/768         |
| **L=12** |        12/128         |        12/256         |         12/512          | **12/768 (BERT-Base)** |



[jina-ai/clip-as-service: ğŸ„ Scalable embedding, reasoning, ranking for images and sentences with CLIP](https://github.com/jina-ai/clip-as-service)

[MuQiuJun-AI/bert4pytorch: è¶…è½»é‡çº§bertçš„pytorchç‰ˆæœ¬ï¼Œå¤§é‡ä¸­æ–‡æ³¨é‡Šï¼Œå®¹æ˜“ä¿®æ”¹ç»“æ„ï¼ŒæŒç»­æ›´æ–°](https://github.com/MuQiuJun-AI/bert4pytorch)



è¿™é‡Œæˆ‘æ”¹ç¼–äº†[è¿™ä¸ªæ•™ç¨‹](https://zhuanlan.zhihu.com/p/524487313)ä¸­çš„ä»£ç ï¼Œåœ¨Kaggleä¸Šçš„[BBC News Classification](https://www.kaggle.com/competitions/learn-ai-bbc/submissions)æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚

ä½¿ç”¨çš„æ˜¯`bert-base-cased`æ¨¡å‹

- epoch=5
- LR = 1e-6
- train_batch_size=2
- eval_batch_size=2

è®­ç»ƒäº†åŠä¸ªå°æ—¶å·¦å³ï¼Œæœ€åç²¾åº¦åœ¨0.97

[:fontawesome-solid-code:   train.py](./assets/04-BERT.assets/train.py){: .md-button .md-button--primary }  

[:fontawesome-solid-code:   data.py](./assets/04-BERT.assets/data.py){: .md-button .md-button--primary }



![](https://philfan-pic.oss-cn-beijing.aliyuncs.com/web_pic/AI__LLM__Models__assets__04-BERT.assets__image-20250707232247160.webp)

```text title="è®­ç»ƒè¿‡ç¨‹"
1192 149 149
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596/596 [06:27<00:00,  1.54it/s]
Epochs: 1 
              | Train Loss:  0.738 
              | Train Accuracy:  0.341 
              | Val Loss:  0.631 
              | Val Accuracy:  0.477
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596/596 [06:26<00:00,  1.54it/s]
Epochs: 2 
              | Train Loss:  0.521 
              | Train Accuracy:  0.640 
              | Val Loss:  0.419 
              | Val Accuracy:  0.785
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596/596 [06:26<00:00,  1.54it/s]
Epochs: 3 
              | Train Loss:  0.323 
              | Train Accuracy:  0.857 
              | Val Loss:  0.270 
              | Val Accuracy:  0.919
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596/596 [06:26<00:00,  1.54it/s]
Epochs: 4 
              | Train Loss:  0.213 
              | Train Accuracy:  0.946 
              | Val Loss:  0.191 
              | Val Accuracy:  0.966
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596/596 [06:26<00:00,  1.54it/s]
Epochs: 5 
              | Train Loss:  0.137 
              | Train Accuracy:  0.978 
              | Val Loss:  0.126 
              | Val Accuracy:  0.973
Test Accuracy:  0.966
Model saved to models/bert_classifier_20250707_230150.pth
```

## å±€é™æ€§

- å¼ºå¤§çš„è¯­è¨€è¡¨ç¤ºèƒ½åŠ›ï¼šBERTæ¨¡å‹ä½¿ç”¨äº†åŒå‘Transformerç»“æ„ï¼Œèƒ½å¤Ÿå­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„è¯­è¨€è¡¨ç¤ºï¼Œå¯ä»¥åº”å¯¹å„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚  
- é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨æ€§ï¼šBERTæ¨¡å‹æ˜¯ä¸€ç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨æœ‰æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥é€‚ç”¨äºå„ç§ä»»åŠ¡å’Œè¯­è¨€ã€‚ 

BERTæ¨¡å‹ä¹Ÿå­˜åœ¨ä¸€äº›ç¼ºç‚¹ï¼š  
- æ¨¡å‹è¿‡äºå¤æ‚ï¼šBERTæ¨¡å‹æ‹¥æœ‰æ•°äº¿ä¸ªå‚æ•°ï¼Œéœ€è¦åœ¨GPUæˆ–TPUç­‰ç¡¬ä»¶å¹³å°ä¸Šè¿›è¡Œè®­ç»ƒå’Œæ¨ç†ï¼Œå¯¹è®¡ç®—èµ„æºçš„è¦æ±‚è¾ƒé«˜ã€‚  
- å­¦ä¹ æ—¶é—´è¾ƒé•¿ï¼šç”±äºBERTæ¨¡å‹éœ€è¦è¿›è¡Œé¢„è®­ç»ƒï¼Œå› æ­¤å…¶è®­ç»ƒæ—¶é—´ç›¸å¯¹è¾ƒé•¿ï¼Œéœ€è¦è€—è´¹å¤§é‡çš„è®¡ç®—èµ„æºã€‚ 
- encoder-only çš„æ¨¡å‹ï¼Œæ²¡æœ‰decoderï¼Œæ‰€ä»¥ä¸èƒ½åšç”Ÿæˆ



> ä¸GPTï¼ˆImproving Language Understanding by Generative Pre-Trainingï¼‰æ¯”ï¼ŒBERTç”¨çš„æ˜¯ç¼–ç å™¨ï¼ŒGPTç”¨çš„æ˜¯è§£ç å™¨ã€‚BERTåšæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬çš„æ‘˜è¦ï¼ˆç”Ÿæˆç±»çš„ä»»åŠ¡ï¼‰ä¸å¥½åšã€‚ ä½†åˆ†ç±»é—®é¢˜åœ¨NLPä¸­æ›´å¸¸è§ã€‚

ä½ çŸ¥é“æœ‰å“ªäº›é’ˆå¯¹BERTçš„ç¼ºç‚¹åšä¼˜åŒ–çš„æ¨¡å‹ï¼Ÿ 

SpanBERT: SpanBERTæ˜¯ä¸€ç§é’ˆå¯¹è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆNLIï¼‰ä»»åŠ¡çš„BERTæ¨¡å‹æ”¹è¿›ï¼Œå®ƒé€šè¿‡å¯¹è¾“å…¥åºåˆ—ä¸­çš„éƒ¨åˆ†å•è¯è¿›è¡Œç‰¹æ®Šæ ‡è®°ï¼Œæ¥ä½¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡ä¸­çš„è¯­ä¹‰å…³ç³»ã€‚

DistilBERT: DistilBERTæ˜¯ä¸€ç§è½»é‡åŒ–çš„BERTæ¨¡å‹ï¼Œé€šè¿‡å‰ªæå’Œè’¸é¦æŠ€æœ¯æ¥å‡å°‘æ¨¡å‹çš„å¤§å°å’Œè®¡ç®—é‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦å’Œæ¨ç†é€Ÿåº¦ã€‚




## Acknowledgement

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=464324279&bvid=BV1PL411M7eQ&cid=444844922&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="500px"></iframe>


- [ææ²è®ºæ–‡ç²¾è¯»_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1PL411M7eQ/?spm_id_from=333.337.search-card.all.click&vd_source=8b7a5460b512357b2cf80ce1cefc69f5)è¯„è®ºåŒº ç”¨æˆ· fdhyhtt çš„ç¬”è®°ï¼Œâ€œDASOUè®²AIâ€ï¼Œâ€œä¸æ˜¯æè€å¸ˆ_â€ç­‰ç”¨æˆ·çš„è¯„è®º
- ærumor - [ã€NLPã€‘Google BERTæ¨¡å‹åŸç†è¯¦è§£ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/46652512)
- VoidOc - [ã€æ·±åº¦å­¦ä¹ ã€‘BERTè¯¦è§£ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/130913995)
- [tomohideshibata/BERT-related-papers: BERT-related papers](https://github.com/tomohideshibata/BERT-related-papers)
- [BERTç›¸å…³é¢è¯•é¢˜ï¼ˆä¸å®šæœŸæ›´æ–°ï¼‰ - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/151412524)