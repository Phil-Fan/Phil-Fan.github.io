# Transformer
!!! info "Transformer"
    ç»§MLPã€CNNã€RNNåçš„ç¬¬å››å¤§ç±»æ¶æ„

<iframe src="https://arxiv.org/pdf/1706.03762" width="100%" height="600px" style="border: none;">
This browser does not support PDFs
</iframe>


åœ¨NLPï¼ŒCVã€mediaç­‰é¢†åŸŸéƒ½æœ‰ç€å¹¿æ³›çš„åº”ç”¨ï¼Œè®©ä¸åŒé¢†åŸŸçš„ä»»åŠ¡éƒ½å¯ä»¥ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹ï¼Œä»»ä½•é¢†åŸŸçš„ç ”ç©¶è€…åšå‡ºçš„çªç ´ï¼Œéƒ½å¯ä»¥æ›´å¿«é€Ÿåº¦åœ°è¢«å…¶ä»–é¢†åŸŸæ‰€ä½¿ç”¨


äººå¯¹ä¸–ç•Œçš„æ„ŸçŸ¥æ˜¯å¤šæ¨¡æ€çš„ï¼Œå¯ä»¥ä½¿ç”¨Transformeræ¥å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯



## å‡è®¾ä¸å½’çº³åç½®

å¯¹ç©ºé—´çš„å‡è®¾å¾ˆå°‘ï¼Œæ¨¡å‹éå¸¸simpleï¼Œå¯ä»¥trainçš„å‚æ•°å¾ˆå°‘ã€‚ä½†æŠ“å–æ•°æ®ä¸­ä¿¡æ¯çš„èƒ½åŠ›å˜å·®äº†ï¼Œæ‰€ä»¥éœ€è¦æ›´å¤šçš„æ•°æ®ï¼Œæ›´å¤§çš„æ¨¡å‹ã€‚

ä¼˜ç‚¹ï¼š

- å¯å¹¶è¡Œ
- ç‹¬ç«‹äºå·ç§¯å’Œå¾ªç¯ï¼Œå®Œå…¨ä¾èµ–äºattentionå¤„ç†å…¨å±€ä¾èµ–ï¼Œè§£å†³é•¿è·ç¦»ä¾èµ–é—®é¢˜
- æ€§èƒ½å¼º


LSTMç›¸æ¯”äºå•çº¯çš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œé¦–å…ˆå…·æœ‰ç†è§£æ–‡æœ¬çš„è¯­åºå…³ç³»çš„èƒ½åŠ›ï¼ˆRNNï¼‰ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œåˆè§£å†³äº†RNNåœ¨å¤„ç†é•¿åºåˆ—æ—¶å‘ç”Ÿçš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ã€‚

Transformerè¿›ä¸€æ­¥è§£å†³äº†RNNã€LSTMç­‰æ¨¡å‹çš„é•¿è·ç¦»ä¾èµ–é—®é¢˜ï¼Œèƒ½å¤Ÿç†è§£æ›´é•¿çš„ä¸Šä¸‹æ–‡è¯­ä¹‰ã€‚å¯ä»¥å¹¶è¡ŒåŒ–ï¼Œæ‰€è¦çš„è®­ç»ƒæ—¶é—´æ›´çŸ­ã€‚


ç¼ºç‚¹ï¼š

- é•¿åº¦å›ºå®š
- å±€éƒ¨ä¿¡æ¯çš„è·å–ä¸å¦‚RNNå’ŒCNNå¼ºï¼šTransformerå…³æ³¨çš„å…¨å±€å…³ç³»ï¼Œè€ŒRNNåœ¨è®¡ç®—è¿‡ç¨‹ä¸­æ›´å…³æ³¨å±€éƒ¨ï¼Œå¯¹è·ç¦»æ›´åŠ æ•æ„Ÿ



![](assets/02-Transformer.assets/202507022301371.png)
> å›¾æº Attention is all you need


ç¼–ç å™¨å°†è¾“å…¥çš„ç¬¦å·åºåˆ— $(x_1, ..., x_n)$ æ˜ å°„ä¸ºè¿ç»­è¡¨ç¤ºåºåˆ— $z = (z_1, ..., z_n)$ã€‚ç»™å®š $z$ï¼Œè§£ç å™¨ä¼šä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªç¬¦å·ï¼Œæœ€ç»ˆç”Ÿæˆè¾“å‡ºåºåˆ— $(y_1, ..., y_m)$ï¼ˆæ³¨æ„ $m$ å’Œ $n$ ä¸ä¸€å®šç›¸ç­‰ï¼‰ã€‚

åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œæ¨¡å‹éƒ½æ˜¯è‡ªå›å½’çš„ï¼Œå³åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ªç¬¦å·æ—¶ï¼Œä¼šå°†ä¹‹å‰ç”Ÿæˆçš„ç¬¦å·ä½œä¸ºé¢å¤–çš„è¾“å…¥ã€‚


## Encoder - Self Attention

éœ€æ±‚ï¼š è¾“å…¥ä¸€ä¸ªåºåˆ—ï¼Œè¾“å‡ºä¸€ä¸ªåºåˆ—

Encoderç”±å…­ä¸ªç›¸åŒå±‚æ„æˆï¼Œæ¯å±‚éƒ½æœ‰ä¸¤ä¸ªå­å±‚ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›å±‚å’Œå…¨è¿æ¥çš„å‰é¦ˆç¥ç»ç½‘ç»œå±‚ï¼ˆLinear+relu+dropout+Linearï¼‰ã€‚ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–è¿æ¥ä¸¤ä¸ªå­å±‚ã€‚

### residual connection

$$
Output = LayerNorm(x + Sublayer(x))
$$

å…¶ä¸­ï¼Œ$x$æ˜¯è¾“å…¥ï¼Œ$\text{Sublayer}(x)$æ˜¯å¯¹$x$åº”ç”¨çš„å­å±‚æ“ä½œï¼Œå¦‚è‡ªæ³¨æ„åŠ›æˆ–å‰é¦ˆç½‘ç»œã€‚æ®‹å·®è¿æ¥æœ‰åŠ©äºè§£å†³æ·±åº¦æ¨¡å‹ä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä½¿å¾—æ›´æ·±å±‚æ¬¡çš„æ¨¡å‹è®­ç»ƒæˆä¸ºå¯èƒ½ã€‚


ä¸ºäº†å®ç°æ®‹å·®è¿æ¥ï¼Œéœ€è¦è®©è¾“å…¥å’Œè¾“å‡ºå…·æœ‰ç›¸åŒçš„ç»´åº¦

ä½œç”¨ï¼šåŒresnetï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ;

é€šè¿‡ç›´æ¥å°†è¾“å…¥åŠ åˆ°å­å±‚çš„è¾“å‡ºä¸Šï¼Œä½¿å¾—æ·±å±‚ç½‘ç»œä¸­çš„ä¿¡å·èƒ½å¤Ÿç›´æ¥ä¼ é€’åˆ°è¾ƒæµ…å±‚ï¼Œæœ‰åŠ©äºç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚





### Layer Norm

BatchNormæ˜¯å¯¹ä¸€ä¸ªbatch-sizeæ ·æœ¬å†…çš„æ¯ä¸ªç‰¹å¾**åˆ†åˆ«**åšå½’ä¸€åŒ–ï¼ŒLayerNormæ˜¯åˆ†åˆ«å¯¹æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾åšå½’ä¸€åŒ–ã€‚

![](assets/02-Transformer.assets/202507030922846.png)

BNæŠ¹æ€äº†ä¸åŒç‰¹å¾ä¹‹é—´çš„å¤§å°å…³ç³»ï¼Œä½†æ˜¯ä¿ç•™äº†ä¸åŒæ ·æœ¬é—´çš„å¤§å°å…³ç³»ï¼›LNæŠ¹æ€äº†ä¸åŒæ ·æœ¬é—´çš„å¤§å°å…³ç³»ï¼Œä½†æ˜¯ä¿ç•™äº†ä¸€ä¸ªæ ·æœ¬å†…ä¸åŒç‰¹å¾ä¹‹é—´çš„å¤§å°å…³ç³»ã€‚

**layer normalization:** å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œå½’ä¸€åŒ–ï¼Œè®¡ç®—$m_x$å’Œ$\sigma_x$ï¼Œç„¶åå½’ä¸€åŒ–$x_i' = \frac{x_i - m_x}{\sigma_x}$


**batch normalization:** å¯¹æ¯ä¸ªfetureè¿›è¡Œå½’ä¸€åŒ–







!!! note "ä¸ºä»€ä¹ˆä»€ä¹ˆä½¿ç”¨layer normè€Œä¸æ˜¯batch norm"
    åœ¨æ—¶åºæ¨¡å‹å½“ä¸­ï¼Œæˆ‘ä»¬çš„æ ·æœ¬é•¿åº¦å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼Œä½¿ç”¨batch normè®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼Œåœ¨å°æ‰¹é‡çš„æƒ…å†µä¸‹ï¼ŒæŠ–åŠ¨ä¼šæ¯”è¾ƒå¤§

    å¦å¤–åœ¨é¢„æµ‹çš„æ—¶å€™ï¼Œå¦‚æœé‡åˆ°äº†æç«¯æ ·æœ¬ï¼Œéœ€è¦è®¡ç®—å…¨å±€çš„å‡å€¼å’Œæ–¹å·®ï¼Œä½¿ç”¨batch normå¯èƒ½æ²¡æœ‰è§è¿‡æç«¯é•¿çš„æ ·æœ¬
    
    layer norm ä¸éœ€è¦è®¡ç®—å…¨å±€çš„å‡å€¼å’Œæ–¹å·®ï¼ŒLNæ˜¯é’ˆå¯¹æ¯ä¸ªæ ·æœ¬åºåˆ—è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ²¡æœ‰æ ·æœ¬é—´ä¾èµ–ï¼Œå¯¹ä¸€ä¸ªåºåˆ—çš„ä¸åŒç‰¹å¾ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–ã€‚
    
    CVä½¿ç”¨BNæ˜¯å› ä¸ºè®¤ä¸ºé€šé“ç»´åº¦çš„ä¿¡æ¯å¯¹cvæ–¹é¢æœ‰é‡è¦æ„ä¹‰ï¼Œå¦‚æœå¯¹é€šé“ç»´åº¦ä¹Ÿå½’ä¸€åŒ–ä¼šé€ æˆä¸åŒé€šé“ä¿¡æ¯ä¸€å®šçš„æŸå¤±ã€‚NLPè®¤ä¸ºå¥å­é•¿çŸ­ä¸ä¸€ï¼Œä¸”å„batchä¹‹é—´çš„ä¿¡æ¯æ²¡æœ‰ä»€ä¹ˆå…³ç³»ï¼Œå› æ­¤åªè€ƒè™‘å¥å­å†…ä¿¡æ¯çš„å½’ä¸€åŒ–


!!! note "BNå’ŒLNçš„ä½¿ç”¨åœºæ™¯"

    > æ¥è‡ª[ä¸€æ–‡ææ‡‚Batch Normalization å’Œ Layer Normalization - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/647813604)
    
    åœ¨BNå’ŒLNéƒ½èƒ½ä½¿ç”¨çš„åœºæ™¯ä¸­ï¼ŒBNçš„æ•ˆæœä¸€èˆ¬ä¼˜äºLNï¼ŒåŸå› æ˜¯åŸºäºä¸åŒæ•°æ®ï¼ŒåŒä¸€ç‰¹å¾å¾—åˆ°çš„å½’ä¸€åŒ–ç‰¹å¾æ›´ä¸å®¹æ˜“æŸå¤±ä¿¡æ¯ã€‚ä½†æ˜¯æœ‰äº›åœºæ™¯æ˜¯ä¸èƒ½ä½¿ç”¨BNçš„ï¼Œä¾‹å¦‚batch sizeè¾ƒå°æˆ–è€…åºåˆ—é—®é¢˜ä¸­å¯ä»¥ä½¿ç”¨LNã€‚è¿™ä¹Ÿå°±è§£ç­”äº†RNN æˆ–Transformerä¸ºä»€ä¹ˆç”¨Layer Normalizationï¼Ÿ
    
    é¦–å…ˆRNNæˆ–Transformerè§£å†³çš„æ˜¯åºåˆ—é—®é¢˜ï¼Œä¸€ä¸ªå­˜åœ¨çš„é—®é¢˜æ˜¯ä¸åŒæ ·æœ¬çš„åºåˆ—é•¿åº¦ä¸ä¸€è‡´ï¼Œè€ŒBatch Normalizationéœ€è¦å¯¹ä¸åŒæ ·æœ¬çš„åŒä¸€ä½ç½®ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†ï¼Œæ‰€ä»¥æ— æ³•åº”ç”¨ï¼›å½“ç„¶ï¼Œè¾“å…¥çš„åºåˆ—éƒ½è¦åšpaddingè¡¥é½æ“ä½œï¼Œä½†æ˜¯è¡¥é½çš„ä½ç½®å¡«å……çš„éƒ½æ˜¯0ï¼Œè¿™äº›ä½ç½®éƒ½æ˜¯æ— æ„ä¹‰çš„ï¼Œæ­¤æ—¶çš„æ ‡å‡†åŒ–ä¹Ÿå°±æ²¡æœ‰æ„ä¹‰äº†ã€‚
    
    å…¶æ¬¡ä¸Šé¢è¯´åˆ°ï¼ŒBNæŠ¹æ€äº†ä¸åŒç‰¹å¾ä¹‹é—´çš„å¤§å°å…³ç³»ï¼›LNæ˜¯ä¿ç•™äº†ä¸€ä¸ªæ ·æœ¬å†…ä¸åŒç‰¹å¾ä¹‹é—´çš„å¤§å°å…³ç³»ï¼Œè¿™å¯¹NLPä»»åŠ¡æ˜¯è‡³å…³é‡è¦çš„ã€‚å¯¹äºNLPæˆ–è€…åºåˆ—ä»»åŠ¡æ¥è¯´ï¼Œä¸€æ¡æ ·æœ¬çš„ä¸åŒç‰¹å¾ï¼Œå…¶å®å°±æ˜¯æ—¶åºä¸Šçš„å˜åŒ–ï¼Œè¿™æ­£æ˜¯éœ€è¦å­¦ä¹ çš„ä¸œè¥¿è‡ªç„¶ä¸èƒ½åšå½’ä¸€åŒ–æŠ¹æ€ï¼Œæ‰€ä»¥è¦ç”¨LNã€‚

!!! note "ä¸ºä»€ä¹ˆlayer norm åœ¨fnå±‚ä¹‹åï¼Ÿ"

    [on layer normalization in the transformer architecture](https://dl.acm.org/doi/pdf/10.5555/3524938.3525913)


Layer Norm çš„æ–°ç ”ç©¶
<iframe src="https://arxiv.org/pdf/1911.07013" width="100%" height="600px" style="border: none;">
This browser does not support PDFs
</iframe>




### position-wise feed-forward network

ç›¸å½“äºä¸€ä¸ªMLP(çº¿æ€§å±‚+ReLu+çº¿æ€§å±‚)

$$
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
$$

![](assets/02-Transformer.assets/202507040928454.png)
- $x$: 512ç»´

- $W_1$: å‡ç»´è‡³2048

- $W_2$: é™ç»´è‡³512



attentionå±‚å·²ç»å«æœ‰äº†æƒ³è¦çš„ä¿¡æ¯ï¼Œè¿›è¡ŒMLPå˜æ¢çš„ç›®çš„æ˜¯å˜æ¢åˆ°æƒ³è¦çš„è¯­ä¹‰ç©ºé—´ä¸Šå».å®ƒåœ¨æ¯ä¸ªä½ç½®ä¸Šç‹¬ç«‹åœ°ä½œç”¨äºå…¶è¾“å…¥ï¼Œæœ‰åŠ©äºå¢åŠ æ¨¡å‹çš„å¤æ‚åº¦å’Œè¡¨è¾¾èƒ½åŠ›ã€‚

å…¶ä¸­ï¼ŒactivationæŒ‡æ¿€æ´»å‡½æ•°ï¼ŒTransformeræœ€å¼€å§‹ç”¨æ˜¯ReLUï¼Œ

ä¹‹åçš„æ¨¡å‹å¯¹è¿™éƒ¨åˆ†æœ‰æ”¹è¿›ï¼Œä¾æ¬¡æ˜¯ï¼š

$$
ReLU \rightarrow GELU \rightarrow Swish(SiLU) \rightarrow SwiGLU
$$

ç°åœ¨ä¸»æµçš„LLMæ¯”å¦‚Llamaã€Qwenå¤§å¤šé‡‡ç”¨SwiGLU







!!! note "å¤§æ¨¡å‹çš„äº‹å®å­˜å‚¨åœ¨MLPå±‚å½“ä¸­"
    è¯¦è§GPTä¸€èŠ‚

### ç»†èŠ‚

self-attentionå±‚åªéœ€è¦å­¦$W_Q,W_K,W_V$ä¸‰ä¸ªçŸ©é˜µï¼Œå‚æ•°æ•°ç›®æ˜¯ $3*d_{model}$

å¯¹äºç›¸ä¼¼çš„çŸ©é˜µè®¡ç®—ï¼Œä½¿ç”¨concatåŒ–ç®€è¡¨è¾¾


- æŠŠVçš„å‚æ•°é‡= Q+V å³æŠŠVåšlow rank transformation
- 






## Decoder - Autoregressive

!!! note "ä»€ä¹ˆå«åšè‡ªå›å½’"
    è¿‡å»æ—¶å€™çš„è¾“å‡ºï¼Œä½œä¸ºç°åœ¨çš„è¾“å…¥

    æœ‰ç‚¹ç±»ä¼¼äºæœ€è¿‘å›¾ä¹¦é¦†é‡Œé¢çš„æ‹¼è´´è¯—æ¥é¾™æ¸¸æˆ


![](assets/02-Transformer.assets/202507030949023.png)
> å›¾æº æå®æ¯…è€å¸ˆppt

éœ€è¦å…ˆç»™ä¸€ä¸ªå¼€å§‹çš„ä¿¡å· â€”â€” begin of sentence tokenï¼Œdecoderä¼šåå‡ºä¸€ä¸ªvocabulary sizeçš„å‘é‡ï¼Œç„¶åé€šè¿‡softmaxå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„tokenä½œä¸ºè¾“å‡º

ç„¶åæŠŠä¹‹å‰è¾“å‡ºçš„tokenä½œä¸ºè¾“å…¥ï¼Œç»§ç»­è¾“å‡ºï¼Œç›´åˆ°é‡åˆ°ç»“æŸçš„ä¿¡å· â€”â€” end of sentence token


> vocabulary sizeï¼š éœ€è¦æå‰æƒ³å¥½ä½ çš„æ•°æ®é‡å¤§å°ï¼Œå–å†³äºä»»åŠ¡ï¼ˆæ¯”å¦‚ç¿»è¯‘ä»»åŠ¡ï¼Œä½ çš„vocabularyå¯ä»¥æ˜¯å¸¸è§çš„3000ä¸ªæ±‰å­—ï¼‰


### Mask

Maskï¼šä¸è¦è®©åé¢çš„tokenå½±å“å‰é¢çš„ï¼Œåœ¨softmaxä¹‹å‰æŠŠå·¦ä¸‹è§’çŸ©é˜µæ”¹æˆè´Ÿæ— ç©·

å¤„ç†ç»“æœï¼Œåœ¨$t$æ—¶åˆ»çš„å€¼ï¼Œåªçœ‹$t-1$åŠä¹‹å‰çš„Qã€K

![](assets/02-Transformer.assets/202507041132597.png){width=50%}

!!! note "why masked"

    è®©è¾“å…¥åºåˆ—åªçœ‹åˆ°è¿‡å»çš„ä¿¡æ¯ï¼Œè€Œçœ‹ä¸åˆ°æœªæ¥çš„ä¿¡æ¯ã€‚
    
    - å¯¹äºencoderæ¥è¯´ï¼Œæ‰€æœ‰tokenéƒ½æ˜¯å¯è§çš„ï¼Œæ˜¯å¹¶è¡Œå¤„ç†çš„
    - ä½†æ˜¯å¯¹äºdecoderæ¥è¯´ï¼Œåªèƒ½çœ‹åˆ°å‰é¢çš„tokenï¼Œæ‰€ä»¥éœ€è¦maskã€‚å…ˆæœ‰a1ï¼Œç„¶åæœ‰a2ï¼Œç„¶åæœ‰a3ï¼Œç„¶åæœ‰a4ï¼Œéœ€è¦æŠŠæœªæ¥çš„tokenéƒ½maskæ‰




### é•¿åº¦


ä½¿ç”¨ END è¡¨ç¤ºç»“æŸï¼Œbeginå’Œend æ˜¯åŒä¸€ä¸ªç¬¦å·

å¦‚ä½•è¾“å‡º ENDå‘¢ï¼Ÿ





## Encoder-Decoder

### Cross-attention

äº¤äº’æ–¹å¼ï¼š
Cross Self-attentionï¼ŒDecoderæä¾›$Q$ï¼ŒEncoderæä¾›$K$ï¼Œ$V$ã€‚

![](assets/02-Transformer.assets/202507030934992.png){width=40% }


Queryæ¥è‡ªä¸‹ä¸€ä¸ªattentionçš„è¾“å…¥ 

å¦‚æœè§£ç å™¨çš„Query ä¸ç¼–ç å™¨è¾“å‡ºçš„keyç›¸ä¼¼åº¦è¾ƒé«˜ï¼Œé‚£ä¹ˆç»™äºˆæ›´é«˜çš„attention

ç›¸å½“äºæ ¹æ®è§£ç å™¨çš„è¾“å…¥ï¼Œå»æŒ‘é€‰ç¼–ç å™¨ä¸­æ„Ÿå…´è¶£çš„ä¸œè¥¿



## è®­ç»ƒ

Transformerçœ‹èµ·æ¥æ¨¡å‹æ¯”è¾ƒå¤æ‚ï¼Œä½†å‡ ä¹æ²¡æœ‰ä»€ä¹ˆå¯ä»¥è°ƒèŠ‚çš„å‚æ•°ï¼Œå¤§éƒ¨åˆ†éƒ½æ˜¯å¯ä»¥æ ¹æ®æ¯”ä¾‹ç®—çš„ã€‚

![](assets/02-Transformer.assets/202507022311529.png)
> å›¾æº Attention is all you need

### è®­ç»ƒæ–¹æ³•

**teacher forcing**

è®©decoder è¾“å‡ºçš„distributionå’Œground truthçš„distributionè¶Šæ¥è¿‘è¶Šå¥½

metricsæ˜¯cross entropyï¼Œå’Œåˆ†ç±»æ¯”è¾ƒç›¸ä¼¼

### Optimizer

å­¦ä¹ ç‡é¢„çƒ­ç­–ç•¥é€šè¿‡é€æ¸å¢åŠ å­¦ä¹ ç‡ï¼Œç›´åˆ°è¾¾åˆ°ä¸€ä¸ªæœ€å¤§å€¼ï¼Œç„¶åå¯èƒ½ä¼šé€æ¸é™ä½

We used the Adam optimizer with $\beta_1=0.9,\beta_2=0.98$ and $\epsilon=10^{-9}.$ We varied the learning rate over the course of training, according to the formula:

$$
lr=d_{\mathrm{model}}^{-0.5}\cdot\min\{step\_num^{-0.5},step\_num\cdot warmup\_steps^{-1.5}\}
$$



This corresponds to increasing the learning rate linearly for the first `warmup_steps` training steps, and decreasing it there after proportionally to the inverse square root of the step number. We used `warmup_steps=4000`



### Regularization


1. residual dropout, $P_{drop} = 0.1$ï¼šåœ¨æ¯ä¸ªå­å±‚ï¼ˆself-attentionå’Œfeed-forward networkï¼‰çš„è¾“å‡ºå¤„æ·»åŠ dropoutï¼Œä¸¢å¼ƒç‡ä¸º0.1ã€‚è¿™ç§dropoutåº”ç”¨åœ¨æ®‹å·®è¿æ¥ä¹‹å‰ï¼Œå¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚å…·ä½“æ¥è¯´ï¼š

   - åœ¨self-attentionå±‚å’Œfeed-forwardå±‚çš„è¾“å‡ºä¸Šåº”ç”¨dropout
   - ç„¶åå†ä¸è¯¥å±‚çš„è¾“å…¥è¿›è¡Œæ®‹å·®è¿æ¥

2. label smoothingï¼šä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œèµ·æºäºInception v3ã€‚ä¼ ç»Ÿçš„one-hotæ ‡ç­¾(å¦‚$y = [0,0,1,0]$)è¦æ±‚æ¨¡å‹è¾“å‡ºå®Œå…¨ç¡®å®šçš„0æˆ–1ï¼Œè¿™å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆã€‚Label smoothingé€šè¿‡å°†æ ‡ç­¾å€¼"è½¯åŒ–"ï¼ˆå¦‚$y' = [0.1,0.1,0.7,0.1]$ï¼‰ï¼Œä½¿æ¨¡å‹å­¦ä¹ åˆ°æ›´å¹³æ»‘çš„åˆ†å¸ƒã€‚å…·ä½“æ¥è¯´:

   - åŸå§‹one-hotæ ‡ç­¾: $y = [0,0,1,0]$ 
   - å¹³æ»‘åçš„æ ‡ç­¾: $y' = (1-\alpha)y + \alpha/K$
   
   å…¶ä¸­$\alpha$æ˜¯å¹³æ»‘å‚æ•°(é€šå¸¸0.1)ï¼Œ$K$æ˜¯ç±»åˆ«æ•°ã€‚è¿™æ ·å¯ä»¥:é˜²æ­¢æ¨¡å‹è¿‡äºè‡ªä¿¡ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ï¼Œå¢å¼ºæ¨¡å‹é²æ£’æ€§









### è®­ç»ƒæŠ€å·§

**å‚æ•°å…±äº«**

åœ¨Transformeræ¨¡å‹ä¸­ï¼Œç‰¹å®šå±‚ï¼ˆå¦‚ç¼–ç å™¨ä¸­çš„å¤šä¸ªç›¸åŒå±‚ï¼‰ä¹‹é—´æˆ–ç‰¹å®šæ“ä½œï¼ˆå¦‚å¤šå¤´æ³¨æ„åŠ›ä¸­çš„å¤´ï¼‰ä¹‹é—´å…±äº«å‚æ•°ï¼Œå¯ä»¥å‡å°‘æ¨¡å‹çš„æ€»å‚æ•°é‡ï¼Œæœ‰åŠ©äºå‡è½»è¿‡æ‹Ÿåˆã€‚


**æ¢¯åº¦è£å‰ª**

æ¢¯åº¦è£å‰ªé€šè¿‡è®¾å®šä¸€ä¸ªé˜ˆå€¼$\theta$ï¼Œå°†æ¢¯åº¦å‘é‡$g$è£å‰ªä¸ºï¼š$g^{\prime}=\min\left(1,\frac\theta{\|g\|}\right)g$è¿™æ ·åš
æ˜¯ä¸ºäº†é˜²æ­¢åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œç¡®ä¿æ¨¡å‹çš„ç¨³å®šè®­ç»ƒã€‚



**copy mechanism**


```
User: hello, I'm John.
Machine: hello, John, nice to meet you.
```

- pointer network


**Guided attention**

> è®­ç»ƒTTSï¼Œè¯»å››éâ€œå‘è´¢â€æ˜¯å¯ä»¥çš„ï¼Œä½†æ˜¯åªè¯»ä¸€éâ€œå‘è´¢â€æ˜¯ä¸è¡Œçš„

è¦æ±‚æœºå™¨æŒ‰ç…§æŸç§ç‰¹å®šçš„æ–¹å¼attentionï¼Œéœ€è¦æœ‰ä¸€äº›

- monotonic attention
- location-aware attention

**Beam search**


greedy decodingä¸ä¸€å®šæ˜¯æœ€å¥½çš„æ–¹æ³•ï¼Œå› ä¸ºå¯èƒ½é”™è¿‡æœ€ä¼˜è§£ 

è€Œbeam search æ˜¯ä¸€ç§å—é™çš„å®½åº¦ä¼˜å…ˆæœç´¢ï¼Œæœ‰ä¸€ä¸ªè¶…å‚æ•°beam widthï¼Œè¡¨ç¤ºæ¯æ¬¡æœç´¢çš„å®½åº¦

æ¯ä¸€ä¸ªæ—¶åˆ»ï¼Œä¿ç•™å¾—åˆ†æœ€é«˜çš„kä¸ªåºåˆ—ï¼Œä¸‹ä¸€ä¸ªæ—¶åˆ»ï¼Œä½¿ç”¨è¿™äº›åºåˆ—ç»§ç»­ç”Ÿæˆ


> æœ‰æ—¶å€™æœ‰ç”¨ï¼Œæœ‰æ—¶å€™æ²¡ç”¨
> å¦‚æœç­”æ¡ˆéå¸¸ç¡®å®šï¼Œé‚£ä¹ˆbeam search å¯èƒ½è¡¨ç°ä¸é”™ã€‚
>
> ä½†æ˜¯ï¼Œå¦‚æœç­”æ¡ˆä¸æ˜ç¡®ï¼Œé‚£ä¹ˆbeam search å¯èƒ½è¡¨ç°ä¸å¥½ã€‚



### BLEU

> æœºå™¨ç¿»è¯‘çš„è¯„ä»·æŒ‡æ ‡

minimize cross entropy is not equivalent to maximize BLEU

and using BLEU score as a metric is not a good idea because it's impossible to calc the derivative of BLEU

so when you don't know how to optimize BLEU, use BLEU as the reward function of reinforcement learning


### exposure bias

æ¨ç†çš„æ—¶å€™ï¼Œdecoderå¯èƒ½çœ‹åˆ°é”™è¯¯çš„ä¸œè¥¿


ä½†æ˜¯åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œ ä¸€ç›´çœ‹åˆ°çš„æ˜¯æ­£ç¡®çš„ç»“æœ



å³å¦‚ä½•è§£å†³ä¸€æ­¥é”™ï¼Œæ­¥æ­¥é”™çš„é—®é¢˜

**scheduled sampling**

åˆç†çš„æ–¹æ³•æ˜¯ç»™decodeä¸€äº›noiseï¼Œè®©decoderçœ‹åˆ°ä¸€äº›é”™è¯¯çš„ä¸œè¥¿

- [Original Scheduled Sampling](https://arxiv.org/abs/1506.03099)

- [Scheduled Sampling for Transformer](https://arxiv.org/abs/1906.07651)

- [Parallel Scheduled Sampling](https://arxiv.org/abs/1906.04331) 

## åº”ç”¨ - æœºå™¨ç¿»è¯‘

### æ•°æ®é›†ä»‹ç»
Multi30K

<iframe src="https://arxiv.org/pdf/1605.00459" width="100%" height="600px" style="border: none;">
This browser does not support PDFs
</iframe>

### Metricsä»‹ç»







## ä»£ç å®æˆ˜

- [hyunwoongko/transformer: Transformer](https://github.com/hyunwoongko/transformer): PyTorch Implementation of "Attention Is All You Need"

- [bentrevett/pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq/)
    Tutorials on implementing a few sequence-to-sequence (seq2seq) models with PyTorch and TorchText.

- [jadore801120/attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch): A PyTorch implementation of the Transformer model in "Attention is All You Need"

- [jayparks/transformer](https://github.com/jayparks/transformer): A Pytorch Implementation of "Attention is All You Need" and "Weighted Transformer Network for Machine Translation"

- [ğŸ¤— Transformersç®€ä»‹](https://huggingface.co/docs/transformers/main/zh/index): åº“
- [awesome-transformers](https://github.com/huggingface/transformers/blob/main/awesome-transformers.md): ç¤ºä¾‹
- [Models â€” MindNLP æ–‡æ¡£](https://mindnlpdocs.readthedocs.io/zh-cn/latest/api/models.html)
## æ‹“å±•
### Decoder - NAT


non-autoregressive model



é•¿åº¦å¦‚ä½•å†³å®šï¼š

- another predictor for output length
- è¾“å‡ºä¸€ä¸ªè¶…çº§é•¿çš„åºåˆ—ï¼Œå¿½ç•¥ENDä¹‹åçš„token


ä¼˜ç‚¹ï¼š

- å¹³è¡Œè®¡ç®—
- è¾“å‡ºé•¿åº¦å¯æ§

ç¼ºç‚¹ï¼š

- æ•ˆæœä¸å¦‚autoregressive model
- multi-modality

### è¶…é•¿æ–‡æœ¬

[åŸºäºBERTçš„è¶…é•¿æ–‡æœ¬åˆ†ç±»æ¨¡å‹_valleriaçš„åšå®¢-CSDNåšå®¢_é•¿æ–‡æœ¬åˆ†ç±»](https://blog.csdn.net/valleria/article/details/105311340)

åŸºæœ¬æ€æƒ³ï¼šå¯¹æ•°æ®è¿›è¡Œæœ‰é‡å çš„åˆ†å‰²ï¼Œè¿™æ ·åˆ†å‰²ä¹‹åçš„æ¯å¥å¥å­ç›´æ¥ä»ä¿ç•™äº†ä¸€å®šçš„å…³è”ä¿¡æ¯ã€‚
æ¨¡å‹ç”±ä¸¤éƒ¨åˆ†æ„æˆï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯fine-tuneåçš„BERTï¼Œç¬¬äºŒéƒ¨åˆ†æ˜¯ç”±LSTM+FCå±‚ç»„æˆçš„æ··åˆæ¨¡å‹ã€‚å³ï¼ŒBERTåªç”¨æ¥æå–å‡ºå¥å­çš„è¡¨ç¤ºï¼Œè€ŒçœŸæ­£åœ¨åšåˆ†ç±»çš„æ˜¯LSTM+FCéƒ¨åˆ†ã€‚

å…·ä½“æµç¨‹ï¼šé¦–å…ˆå°†é•¿å¥å­åˆ†å‰²ä¸ºå¤šä¸ªå°å¥å­ï¼Œå¦‚é•¿200ï¼Œé‡å é•¿åº¦ä¸º50.å°†åˆ†å‰²åçš„æ•°æ®é›†ä¼ å…¥BERTï¼Œåˆ†åˆ«å–æ¯ä¸ªå¥å­çš„[CLS]è¡¨ç¤ºå¥å­çš„embeddingï¼Œå°†æ¥è‡ªç›¸åŒé•¿å¥å­çš„embeddingæ‹¼æ¥ï¼Œä½œä¸ºé•¿å¥å­çš„å‘é‡è¡¨ç¤ºã€‚æœ€åï¼Œå°†é•¿å¥å­çš„å‘é‡è¡¨ç¤ºä¼ å…¥LSTM+FCéƒ¨åˆ†è¿›è¡Œåˆ†ç±»ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œç¬¬äºŒéƒ¨åˆ†è¿˜å¯ä»¥ç”¨Transformerã€‚






## Acknowledgement


<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=577276749&bvid=BV1wB4y1o7is&cid=1303146845&p=3&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height=450px></iframe>

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=577276749&bvid=BV1wB4y1o7is&cid=1303146955&p=4&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height=450px></iframe>

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=506354287&bvid=BV1pu411o7BE&cid=432055065&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height=450px></iframe>


<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=113215035936825&bvid=BV1aTxMehEjK&cid=26046694390&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height=450px></iframe>