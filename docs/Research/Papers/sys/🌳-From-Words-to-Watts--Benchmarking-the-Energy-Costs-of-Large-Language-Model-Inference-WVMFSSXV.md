---
tags: []
parent: 'From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference'
collections:
    - sys
$version: 9341
$libraryID: 1
$itemKey: WVMFSSXV

---
# <span style="color: white"><span style="background-color: rgb(44, 62, 80)">üå≥ From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference</span></span>

| Title     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| --------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Journal   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Authors   | Siddharth Samsi; Dan Zhao; Joseph McDonald; Baolin Li; Adam Michaleas; Michael Jones; William Bergeron; Jeremy Kepner; Devesh Tiwari; Vijay Gadepally                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Pub. date | 2023-10-04                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ÊúüÂàäÊ†áÁ≠æ      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| DOI       | [10.48550/arXiv.2310.03003](https://doi.org/10.48550/arXiv.2310.03003)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Abstract  | Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs -- despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies. In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of the inference performance and inference energy costs of different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta AI on two generations of popular GPUs (NVIDIA V100 & A100) and two datasets (Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in research and practice. We present the results of multi-node, multi-GPU inference using model sharding across up to 32 GPUs. To our knowledge, our work is the one of the first to study LLM inference performance from the perspective of computational and energy resources at this scale. |


## <span style="color: white"><span style="background-color: rgb(127, 42, 42)">üìù Abstract</span></span>

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üéØ Task</span></span>

1231231231323

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">‚ö° Technical Challenge</span></span>

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üí° key insight/motivation</span></span>

*   ‚ú® ‰∏ÄÂè•ËØù‰ªãÁªç insight/motivation
*   üëç ‰∏ÄÂè•ËØù‰ªãÁªç insight ÁöÑÂ•ΩÂ§Ñ

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üõ†Ô∏è technical contribution</span></span>

*   üß© contribution1

    *   üìå ‰∏ÄÂè•ËØù‰ªãÁªç
    *   ‚úÖ Â•ΩÂ§Ñ

*   üß© contribution2

    *   üìå ‰∏ÄÂè•ËØù‰ªãÁªç
    *   ‚úÖ Â•ΩÂ§Ñ

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üìä Experiment</span></span>

***

## <span style="color: white"><span style="background-color: rgb(127, 42, 42)">üìñ Introduction</span></span>

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üéØ Task And application</span></span>

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">‚ö° Technical challenge for previous problem</span></span>

*   üöß challenge 1

    *   üìö previous method
    *   ‚ùå failure cases
    *   üîç technical reason

*   üöß challenge 2

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üõ†Ô∏è our pipeline that fix</span></span>

*   üí° key innovation/insight/contribution

*   üß© contribution 1

    *   üî® specific method
    *   ‚úÖ advantages/insight

*   üß© contribution 2

    *   ü§î ‰∏∫‰∫ÜËß£ÂÜ≥‰ªÄ‰πàÈóÆÈ¢ò
    *   üî® ÂÖ∑‰ΩìÂÅöÊ≥ï
    *   ‚úÖ ËÆ®ËÆ∫ advantage/insight

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üé¨ cool demos/applications</span></span>

***

## <span style="color: white"><span style="background-color: rgb(127, 42, 42)">‚öôÔ∏è Method</span></span>

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üåê Overview</span></span>

*   üéØ ÂÖ∑‰Ωì‰ªªÂä°ÔºöËæìÂÖ• ‚û°Ô∏è ËæìÂá∫
*   ü™ú ÊñπÊ≥ïÔºöÁ¨¨‰∏ÄÊ≠• ‚û°Ô∏è Á¨¨‰∫åÊ≠• ‚û°Ô∏è Á¨¨‰∏âÊ≠•

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üîπ Pipeline module1</span></span>

*   üí° Motivation
*   üî® ÂÅöÊ≥ï
*   üîç why work
*   ‚úÖ technical advantage

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üîπ Pipeline module2</span></span>

***

## <span style="color: white"><span style="background-color: rgb(127, 42, 42)">üß™ Experiments</span></span>

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üìä Comparison experiments</span></span>

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">üî¨ Ablation studies</span></span>

*   üåü core contributions

    *   üß± core components ÂØπ performance ÁöÑÂΩ±Âìç

*   ‚öôÔ∏è ÊØè‰∏Ä‰∏™ pipeline module ‰∏≠ design choices ÂØπ performance ÁöÑÂΩ±Âìç

***

## <span style="color: white"><span style="background-color: rgb(127, 42, 42)">üöß Limitation</span></span>

### <span style="color: white"><span style="background-color: rgb(42, 77, 127)">ü§î ÂêàÁêÜËß£Èáä</span></span>

‰∏∫‰ªÄ‰πàÊñπÊ≥ïÊúâ limitation
